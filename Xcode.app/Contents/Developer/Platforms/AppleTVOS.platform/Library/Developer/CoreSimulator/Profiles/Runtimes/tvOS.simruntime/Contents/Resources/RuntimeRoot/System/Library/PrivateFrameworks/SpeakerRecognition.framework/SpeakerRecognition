%s invalid packets
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s metaInfo passed is nil - Bailing out
%s Unable to read data from file: %@
%s Could not read existing %@ file: err: %@
%s ERR: Failed to create json %{public}@ with %{public}@
%s saveAudioChunk toURL: %{public}@
%s Invalid request: nothing to write to file
%s value = %{public}d
%s Couldn't create SSV log directory at path %{public}@ %{public}@
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Force enabling VoiceTrigger AP mode ? %{public}@
%s Force enabling VoiceTrigger AOP mode ? %{public}@
%s Couldn't create SoS log directory at path %{public}@ %{public}@
%s enableAudioInection: is only available on internal builds
%s setAudioInjectionFilePath: is only available on internal builds
%s kCSAudioInjectionFilePathKey is not array type
%s kCSAudioInjectionFilePathKey array size = %d
%s kCSAudioInjectionFilePathKey doesn't have NSString as an array entry
%s Override iOS barge-in support key to: %{public}@
%s Shouldn't be called on non-iOS platform
%s SpkrId:: ERR: Hybrid endpointer not ready for processing request
%s SpkrId:: VAD processed %f secs of audio
%s SpkrId:: Endpoint already reported. Not scheduling
%s SpkrId:: Found Endpoint at: [%f %f %f %f]
%s SpkrId:: Found startpoint at: [%f %f %f %f]
%s ::: Error reading file %@, err: %d
%s CSAudioFileReader requires prepare recording settings to feed audio
%s CSAudioFileReader only support LinearPCM to feed
%s Setting ExtAudioFileSetProperty failed : %d
%s Starting audio file feed timer, bufferDuration = %f sampleRate = %f, bytesPerFrame = %d, channelsPerFrame = %d
%s ::: Error reading data from audio file : %d
%s Reach to EOF, chunkSize = %d
%s Stopping audio file feed timer
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter with wav file header offset %{public}llu
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s ERR: reading contents of gradingDir: %{public}@ with error %{public}@
%s Deleting orphaned grading file %{public}@
%s ERR: Failed to delete %{public}@ with error %{public}@
%s Removing non-dir at AttentiveSiri AudioLog dir: %@
%s Error removing %@: err: %@
%s Failed to create AudioLogging directory for AttentiveSiri: %@
%s Created AudioLogging dir for AttentiveSiri at: %@
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files number %{public}lu with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s Regular expression is nil!
%s SpkrId:: Failed to create SSRSpeakerRecognizerPSR
%s SpkrId:: %@::uniqueUttTag: %@, extraSamplesAtStart: %lu, _tdEndInSampleCount: %lu(%f ms)
%s SpkrId:: CSSpIdVTSpeakerRecognizer dealloc
%s SpkrId:: Discarded ScoreCard for mismatch session - {%{public}@, %{public}@}
%s channelId %d out of bound %d
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s SpkrId:: Processing ended at: numSamplesProcessed=%lu, totalSampleCountToReach=%lu
%s SSROrch[%{public}@]:: Failed to create PSR Recognizer
%s SSROrch[%{public}@]:: Failed to create SAT Recognizer
%s SSROrch[%{public}@]:: Successfully initialized with {%{public}@, %{public}@}
%s SpkrId:: Released OS transaction for %{public}@
%s SSROrch[%{public}@]:: Ignoring addAudio, endAudio: %d procSamples: %lu maxProcSamples: %lu
%s SSROrch[%{public}@]:: Released OS transaction for %{public}@
%s SSROrch[%{public}@]:: Creating OS transaction for %{public}@
%s SSROrch[%{public}@]:: Scorecard %{public}@ with delay:%{public}ldms, processed:%{public}ldms, await:%{public}ldms
%s ERR: Posting diagnostic report for abnormal score delay - %ldms
%s SSROrch[%{public}@]:: Sync score report with %{public}f delay - with known user scores %{public}@
%s SSROrch[%{public}@]:: ERR: VoiceInfo is nil from recognizer %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session - %{public}@
%s SSROrch[%{public}@]:: EndAudioCalled is false, returning for recognizer %{public}@
%s SSROrch[%{public}@]:: PSR Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: SAT Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: Wait for %{public}@ analyzer to complete the session - %{public}@
%s SSROrch[%{public}@]:: Finished the session with known user scores %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session
%s SSROrch[%{public}@]:: Speech started at - %ldms
%s SSROrch[%{public}@]:: Starting a new segment of speech - %ldms
%s CSVoiceTriggerAsset found: %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %s async called
%s %{public}s async called
%s Waiting for didStop
%s Done waiting for didStop
%s Called before completion called
%s %{public}s Called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s ERR: Failed to save explicit utterance
%s audioSessionDidStopRecording
%s ERR: SpeakerRecognition is not available on this platform
%s Creating OS Transaction for %{public}@
%s Release OS Transaction for %{public}@
%s Creating audioRecorder has failed
%s AudioRecorder creation failed : %@
%s AudioStream Handle ID is invalid : %{public}@
%s audioRecorder %p created
%s Cannot prepare since audio recorder does not exist
%s Failed to activate audio session, error : %{public}@
%s AudioRecorder is already recording, do not prepare anymore
%s Failed to prepareAudioStreamRecord : %{public}@
%s Failed to startAudioStream : %{public}@
%s failed to stopRecording : %{public}@
%s audioInput:[%@]
%s audioOutput:[%@]
%s ERR: voiceProfile is nil - Bailing out
%s ERR: Failed to create TriggerPhraseDetector in %{public}@ with %{public}@
%s ERR: Failed in trigger processing %{public}@ with %{public}@
%s Trigger Score %{public}f not satisfied implicit VT threshold %f
%s Using recognizer scale factor: %f for phrase detector
%s ERR: Failed to create trigger phrase detectors
%s Processing %{public}@ for trigger word detection
%s ERR: Failed to read file: %@
%s ERR: Failed processing %{public}@ with error %{public}@
%s ERR: %{public}@
%s Best trigger score for %{public}@ is %{public}f (%{public}f, %{public}f)
%s Dealloc of CSRemoteControlClient, it should close connection
%s Failed to create AVVC : %{public}@
%s Create new CSAudioRecorder = %{public}p
%s CSAudioRecorder %{public}p deallocated
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Calling AVVC setContext
%s Failed to get handle id : %{public}@
%s setContext elapsed time = %{public}lf
%s Calling AVVC setContextForStream : %{public}@
%s setCurrentContext elapsed time = %{public}lf
%s Calling AVVC prepareRecordForStream(%{public}llu) : %{public}@
%s AVVC prepareRecordForStream failed : %{public}@
%s prepareRecordForStream elapsed time = %{public}lf
%s ::: CSAudioRecord will inject audio file instead of recording
%s Resetting AudioFilePathIndex
%s Increase AudioFilePathIndex = %d
%s AudioFilePathIndex is out-of-boundary _audioFilePathIndex:%d injectAudioFilePaths:%d
%s AudioFilePathIndex:%d accessing:%@
%s Unable to find injectAudioFilePath = %@
%s Calling AVVC startRecordForStream : %{public}@
%s startRecordForStream failed : %{public}@
%s startRecordForStream elapsed time = %{public}lf
%s Calling AVVC stopRecordForStream
%s Failed to stopRecordForStream : %{public}@
%s stopRecordForStream elapsed time = %{public}lf
%s Session State = %d
%s AudioSessionState = YES
%s AudioSessionState = NO
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s isNarrowBand = NO for streamHandleId = %{public}lu
%s isNarrowBand = %{public}@ for streamHandleId = %{public}lu
%s Calling AVVC setSessionActive for Prewarm
%s Prewarm AudioSession has failed : %{public}@
%s Calling AVVC setRecordMode to mode : %{public}d
%s AVVC successfully setRecordMode
%s setRecordMode elapsed time = %{public}lf
%s Calling AVVC setSessionActivate for activation
%s AVVC setSessionActivate has failed : %{public}@
%s AVVC successfully activated audioSession
%s setSessionActivate elapsed time = %{public}lf
%s Calling AVVC setSessionActivate for deactivation : %{public}tu
%s Failed to setIamTheAssistant : %{public}@
%s Creating audio session with allow mixable audio while recording to YES
%s Failed to setAllowMixableAudioWhileRecording : %{public}@
%s Calling AVVC : Enable Smart Routing Consideration
%s Calling AVVC : Disable Smart Routing Consideration
%s enableSmartRoutingConsiderationForStream elapsed time = %{public}lf
%s Fail to setSmartRoutingConsideration : %{public}@
%s Should not call setDuckOthersOptions with NO in B238
%s enableMiniDucking elapsed time = %{public}lf
%s %{public}@
%s configureAlertBehavior elapsed time = %{public}lf
%s VoiceTriggerInfo is nil from AVVC
%s Updated languageCode to: %{public}@ in VTEI received from remote
%s Peak : %f, Avg : %f
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s Cannot handle audio buffer : unexpected format(%{public}u)
%s Compensating %{public}u channel(s), heartbeat = %{public}lld
%s Calling AVVC playAlertSoundForType to play alert
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Received didStartRecording : %{public}p, forStream:%{public}llu, successfully:%{public}d, error:%{public}@
%s Received audio buffer %{public}d, heartbeat = %{public}llu, streamID (%{public}lu)
%s Received didStopRecording : %{public}p forStream:%{public}llu forReason:%{public}ld
%s Received Stream Invalidated : %{public}llu
%s toConfiguration : %{public}d
%s type : %{public}d, error : %{public}@
%s Encoder error : %{public}@
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Unsupported audio format!
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value=%{public}@
%s SpkrId:: Unknown CSSpIdType string: %@
%s ERR: Unknown CSSpIdType: %lu
%s Unknown CSSpIdType: %lu
%s SpkrId:: path is nil - Bailing out
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s SpkrId:: VoiceId not supported in language %{public}@
%s SpkrId:: ERR: filePath passed as nil - Bailing out
%s SpkrId:: ERR: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: %@ is a directory
%s SpkrId:: ERR: file do not exist - %@
%s Unknown Device category for deviceProduceType: %@
%s ERR: Unknown device. returning false: %{public}@
%s ERR: satLanguagePath is nil. returning false
%s Voice Profile Mismatch - CurrentDeviceCategory %@ VoiceProfileCategory %@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s ERR: fetching contents of %{public}@ failed with error %{public}@
%s ERR: Directory is nil - Bailing out
%s ERR: %{public}@ doesnt exist - Bailing out
%s Dump content of %{public}@ - %{public}@
%s Error reading directory at %@: err: %@
%s %@ is empty
%s Fetching homeUserId for siriProfileId %{public}@
%s siriProfileId %{public}@ maps to homeUserId %{public}@
%s ERR: Home User Id erred %{public}@ for Siri Profile Id %{private}@
%s ERR: Seeing more than one voice profiles for Siri App Domain
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Setting payloadstartSample %lu for trigger duration of %fsecs
%s ERR: Setting max payloadstartSample %lu for trigger duration of %fsecs
%s Failed to read file: %@
%s %@
%s Deleted file %{public}@
%s EOF: utteranceLength: %lums, tdlength: %lums tdtiLength: %lums tdtiDiscardedLength: %lums
%s satAudioDirectory is nil - Bailing out
%s metaVersionFile %{public}@ doesnt exist
%s Found %{public}d ambiguous explicit utterances
%s metaVersionFile %{public}@ doesnt exist - Skipping
%s ERR: Fetching contents of %{public}@ failed with error - %{public}@
%s ERR: Scores for profileId %{public}@ not present in %{public}@ - Skipping
%s Siri language is nil, falling back to %@
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s Sending Analytics Event - %{public}@
%s Delta is larger than anchorHostTime: anchorSampleCount = %{public}lld, sampleTime = %{public}lld, anchorHostTime = %{public}lld
%s Delta is larger than anchorSampleCount
%s Not supported on this platform
%s cannot handle nil event 
%s ignore unknown types of message: %{public}@
%s cannot handle nil error
%s Listener connection disconnected
%s connection error: %{public}s
%s Processing onboarded Siri user: %{public}@
%s Detected matching %{public}d users: %{public}@
%s Valid profile not found %{public}@ and %{public}@ - defaulting to %{public}@
%s Adding invalid user for deletion - %{public}@
%s Skipping retaining user %{public}@
%s Detected invalid user: %{public}@
%s File path doesnt exist - %{public}@
%s ERR: Failed reading contents of SAT root %{public}@ with %{public}@
%s App domains in use - %{public}@
%s ERR: Failed determining if file is dir-entry url=%{public}@ with %{public}@
%s Deleting invalid file %{public}@
%s Deleting invalid domain %{public}@ not part of domains %{public}@
%s Processing domain - %{public}@
%s ERR: Failed reading AppDomain %{public}@ at %{public}@ with %{public}@
%s Processing locale - %{public}@
%s Deleting invalid locale %{public}@ not supported in set %{public}@ and current language %{public}@
%s Processing profile - %{public}@
%s Deleting invalid profile %{public}@
%s Removing Implicit utterance cache directory at %{public}@
%s Processing profile %{public}@ with version %{public}d and identity %{public}@
%s Found legacy voice profile - Skipping
%s Deleting invalid SAT entry: %{public}@
%s ERR: Failed to get atrributes of file %{public}@, err %{public}@, size %{public}llu
%s Deleting invalid SAT entry: %{public}@ %{public}@
%s Found non-meta file: %{public}@
%s Deleting invalid SAT entry: %{public}@ : <%{public}@>
%s Processed %{public}@ with %{public}d explicit and %{public}d implicit utterances
%s ObsoleteCutOffDate is nil - Bailing out
%s Checking payload utterances prior to %{public}@ for profile %{public}@ and modelType %d
%s Deleting lifetimeexpired SAT entry %{public}@
%s Deleted lifetimeexpired metafile %{public}@
%s Deleting model file %{public}@ with err %{public}@
%s SpkrId:: ERR: missing arguments to create voice profile - Bailing out
%s Importing %{public}@ of %{public}@ from import Dir %{public}@
%s Successfully imported %{publice}@ %{public}lu(%{public}lu) utterances to profile %{public}@
%s Failed in importing %{public}@ of %{public}@ from import Dir %{public}@
%s Copied %{public}@ to %{public}@ with error %{public}@
%s Copied TD audio files %{public}lu to TDTI which now has %{public}lu(%{public}lu) utterances
%s Error to copy utterance from %{public}@ to %{public}@, error: %{public}@
%s Copied Utterance from %{public}@ to %{public}@
%s Error to copy jsonFile from %{public}@ to %{public}@, error: %{public}@
%s Successfully copied %{public}lu(%{public}lu) utterances to profile %{public}@
%s ERR: date is nil - Bailing out
%s Filtered %@ with enrolled date %@
%s Couldn't delete invalidated speaker model at path %{public}@ %{public}@
%s ERR: Removing %{public}@ as explicit utterances %{public}d from audio dir - %{public}@
%s SAT path doesnt exist - %@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT %{public}@ when there is no audio directory
%s ERR: Unknown device-category for device: %{public}@
%s ERR: error creating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: error removing voice profile version file at: %@, err: %@
%s ERR: Error writing voice profile version file at: %@, err:%@
%s ERR: Profile dict is nil - Bailing out
%s ERR: error updating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: Failed initialization in _EARSyncSpeechRecognizer initWithConfiguration
%s ERR: processAudioChunk failed
%s ERR: endAudio failed
%s Start Recording Host Time = %{public}llu
%s Skipping SAT Model {%{public}@, %{public}@} for %{public}@
%s Skipping model {%{public}@, %{public}@} for %{public}@
%s Added model context {%{public}@, %{public}@} for %{public}@
%s ERR: uttPath is nil -  Bailing out
%s ERR: uttPath is nil - Bailing out
%s ERR: uttMeta is nil - Bailing out
%s ::: Error creating json Metadata: %{public}@
%s ERR: uttMetaURL is nil, defaulting to NO
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s ERR: uttMetaURL is nil - Bailing out
%s ERR: metaData is nil for %{public}@ - Bailing out
%s ERR: read metafile %{public}@ failed with %{public}@ - Bailing out
%s ERR: metaDict from file %{public}@ isnt a dictionary - %{public}@
%s ERR: %{public}@ is not present
%s ERR: Unexpected. metaVersionFileData is empty while the file exists at: %{public}@
%s Json-Err reading metaVersionFile: %{public}@: err: %{public}@
%s ERR: filePath is nil!
%s Failed to create regular expression : %{public}@
%s Testing [%@] against regex.
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s Force endpoint fired
%s Discarding surplus audio of %{public}lu samples, audio sample available %{public}lu (%{public}lu, %{public}lu, %{public}lu)
%s AudioSession Started
%s AudioSession Stopped
%s Begin of speech detected
%s End of speech detected with endpoint type: %{public}ld
%s unknown endpoint type
%s Called with status : %{public}d and success : %{public}d utteranceStored : %{public}d
%s ERR: Failed to store utterance, overiding status
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f (%{public}ld)
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s ERR: Profile not available for %{public}@ & %{public}@ - Bailing out
%s ERR: No configured Siri Profiles
%s ERR: More than one Siri Voice Profiles - %{public}@
%s ERR: Failed to add profile into the store with error %{public}@
%s ERR: Finished implicit training for %@ with error %{public}@
%s Finished implicit training for %@
%s Received implicit utterance for %{public}@ from %{public}@ with context %d
%s ERR: FilePath is nil - Bailing out
%s ERR: Training utterance doesnt exist at %@ - Bailing out
%s ERR: Training utterance is marked as directory at %@ - Bailing out
%s VoiceTriggerEventInfo is nil - Bailing out
%s kVTEILanguageCode is nil - Bailing out
%s ERR: trigger score not found in VTEI - Bailing out
%s ERR: SAT did not trigger!!! - Bailing out
%s Implicit training not enabled for %{public}@
%s sharedSiriId is nil - Bailing out
%s Rejecting barge-in utterance from adding to voice profile
%s Privacy disallowed implicit utterance %{public}@ - skipping
%s ERR: Failed to segment %{public}@ with %{public}@ - Bailing out
%s Processed implicit utterance %{public}@ successfully
%s Voice Profile is full - Ignoring
%s Implicit Policy not satisfied - Ignoring
%s ERR: Failed to process implicit utterance %{public}@ with error %{public}@
%s Enrolling voice profile of %@ 
%s Failed enrolling %@ with error %@
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Skipping language [%{public}@] since we already have enrollment data for this language
%s Skipping language [%{public}@] as file path doesnt exist - %{public}@
%s Skipping language [%{public}@] as voice profile not compatible
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s Adding voiceprofile for %{public}@ in language %{public}@ completed with error %{public}@
%s ERR: Failed migrating Voice profile for language %{public}@ with error %{public}@
%s Successfully added %{public}@ in %.2fms
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Failed to download voice profile with error %{public}@ and category %{public}@ 
%s Successfully enrolled voice profile %@ with %{public}@ profile
%s Triggering profile sync check
%s Skipped enrolling voice profile %@ with %{public}@ profile
%s ERR: Failed in enrolling Voice profile %@ with category %{public}@ profile
%s Failed to enroll siriProfileId %@ with %{public}@
%s Language %{public}@ not supported in %{public}@ - Skipping
%s Skipping profile download for %{public}@ - %{public}@ not matching current %{public}@
%s Skipping profile download for %{public}@ - voiceId not supported in %{public}@
%s ERR: %@
%s ERR: Failed migrating Voice profile with ID %{public}@ for language %{public}@ with error %{public}@
%s Sucessfully enrolled %{public}@ for language %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s Upload of Voice Profile for %@ completed with error %{public}@
%s Upload of Voice Profile for %@ completed successfully
%s ERR: Failed to delete existing SATUpload diretory : %{public}@
%s Upload trigger of voice profile of %@ 
%s Upload not supported on %{public}@
%s Legacy upload API called on Horseman - Bailing out
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create %{public}@ with error %{public}@ - Skipping language
%s Cannot create directory(%{public}@)
%s Cannot copy file: %{public}@ to %{public}@
%s ERR: siriProfileId is nil - Bailing out
%s Unsupported languagecode %{public}@ in %{public}@ - Skipping
%s Skipping uploading %{public}@ legacy version (%lu) of voice profile, current version %lu
%s Skipping uploading %{public}@ voice profile for profileId %@
%s Skipping audiocache file %@
%s Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s ERR: Failed to copy from %{public}@ to %{public}@ with error %{public}@
%s ERR: Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s Successfully copied {%{public}d,%{public}d} utterances from %@ to %@
%s Cannot copy voice profile from %@ to %@ with error %{public}@
%s Triggering upload of voice profile %@
%s Upload of voice profile at %@ with category %{public}@ completed successfully
%s Triggering upload of explicit voice profile %@
%s Upload of explicit voice profile at %@ completed successfully
%s Setting VoiceProfile Training Sync for language: %{public}@
%s ERR: %{public}s: Bailing out as language is nil!
%s VoiceProfile training sync language: %{public}@, VoiceProfile language: %{public}@
%s ERR: Unknown device - returning nil
%s NonAOP device-category - returning nil
%s ERR: Fetching cached devices resulted in error %{public}@
%s Devices with voice profile is nil!
%s Skipping %{public}@ not locally present
%s ERR: error creating profilesJsonData: %@, err: %@
%s Cached devices with VoiceProfile in iCloud: %{public}@
%s CachedVoiceProfileFetch: Done Waiting with timedOut=%ld, waitTimeMs: %fms
%s ERR: metaBlob is nil. Returning false, language: %{public}@
%s ERR: Unknown device. Returning false, language: %{public}@
%s ERR: Unknown device-category for device: %{public}@, languageCode: %{public}@
%s ERR: Failed to deserialize metaBlob with error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in metablob %{public}@
%s currDevice=[%{public}@ : {%{public}@}] ; syncedDevice=[%{public}@ : {%{public}@}]
%s CurrDevice: [%{public}@ : {%{public}@}] DOES NOT have VoiceProfile synced in iCloud
%s Triggering VoiceProfile upload for %{public}@
%s Querying VoiceProfile upload state on %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in devices %{public}@
%s VoiceProfile available locally for %{public}@, not uploaded yet
%s Searching for synced-VoiceProfile for CurrDevice: %{public}@{%{public}@}
%s Will Enable VoiceTrigger after VoiceProfile sync for language: %{public}@
languageCode: %{public}@
%s Devices with VoiceProfile in iCloud for language: %{public}@:%{public}@
%s Timedout waiting for AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage: %{public}ld, waitedFor: %{public}d, Returning nil
%s timeToRet(AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage:): %{public}fms
%s voiceProfileArray is nil!
%s Profiles already migrated, check for enrollment on %{public}@ on profile %{public}@
%s ERR: Failed to delete Voice Profile %{public}@ with error %{public}@
%s Couldn't delete SAT directory at path %@ %@
%s Couldn't delete SAT cache directory at path %@ %@
%s No Audio file exists when enrollment marker is set, remove marker
%s Contents of audio dir - %{public}@
%s Language Code is nil!
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s default to recordContext : %{public}@
%s ERR: Failed to create asset providers - Bailing out
%s parsing provider: %@ name: %@
%s ERR: got nil assets from provider: %@
%s Got asset with version: %@ from provider: %@
%s ERR: Asset not available from provider: %@
%s SpkrId:: Scorecard for {%{public}d, %{public}.2fsec %dms} - %{public}@
%s SpkrId:: %@
%s SpkrId:: Discarded speaker scores for session - %{public}@
%s SpkrId:: Final - %@
%s ::: found %{public}lu installed assets for matching query: %{public}@
%s ERR: Failed to asset for %{public}@
%s Error running query: %{public}@, error: %{public}lu
%s Failed to get assetString for assetType %{public}d
%s ::: found %{public}lu assets matching query: %{public}@
%s Error running asset-query: %{public}@, error: %{public}lu
%s Asset state : %{public}ld
%s Chosen Asset %{public}@
%s SpkrId:: ERR: appDomain passed as nil
%s SpkrId:: ERR: locale passed as nil
%s Profiles already migrated - Bailing out
%s Migration of voice profile is triggered...
%s Sat directory doesnt exist %@
%s Language %{public}@ not supported in %{public}@ - Deleting
%s Migrating voice profiles in languages - %{public}@
%s Voice profile migration for language - %{public}@
%s Skipped migrating non-siri landed profile - %{public}@, %{public}@
%s voice profile created is nil!!! - Skipping %{public}@
%s Moving contents from %{public}@ to %{public}@ failed with error %{public}@
%s Completed migrating voiceprofile for %{public}@ in language %{public}@
%s Triggered cleanup duplicated profiles
%s ERR: Deleted duplicated voiceprofile(%lu): %{public}@
%s Found %{public}d duplicated profiles
%s Triggering voice profiles download
%s ERR: Failed retraining LiveOn onboarded users with error %{public}@
%s Successfully retrained LiveOn onboarded users
%s Cleanup model files with assets %{public}@
%s Synchronize voiceprofiles with Assistant...
%s ERR: Deleted stale voiceprofile(%lu): %@
%s Missing user models - Triggering voice profiles download
%s Needs retraining - Triggering voice profiles download
%s ERR: Failed in adding %{public}@ to %{public}@ with error %{public}@
%s Added Implicit SAT vector from %{public}@ to profile %{public}@
%s Locale is nil - Bail out
%s assetForLocale is nil - Bail out
%s ERR: Failed purging profile %{public}@ with error - %{public}@
%s ERR: Failed to get top scorer in %{public}@ - Bailing out
%s ERR: Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) for profileId %{public}@
%s Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) and thresholds (%{public}f, %{public}f)
%s Utterance scored %{public}f for %{public}@ and thresholds (%{public}f, %{public}f)
%s Skipping retraining for language %{public}@, current %{public}@
%s Detected mean pitch for explicit utterances = %{public}f Hz
%s Deleting VoiceProfile %{public}@
%s Err: %@
%s Needs Retraining Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining storage for Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Skipping Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining %{public}@ model update for profile %{public}@ 
%s Retraining %{public}@ for locale %{public}@
%s Needs retraining %{public}@ - Triggering voice profiles download
%s Retraining successfully finished for %{public}@ in %{public}fms
%s Adding User Voice Profile %@
%s Updating User Voice Profile to %@ from %@
%s Deleting User Voice Profile %@
%s User Voice Profile not found %@ - Bailing out
%s ERR: UserVoiceProfile Action undefined %ld - Bailing out
%s Updating profile %{public}@ with userName %{public}@
%s Retraining for locale %{public}@ with force %d
%s ERR: Retraining failed for %{public}@ with error %{public}@ in %{public}fms
%s Retraining finished for %@ with error %@ in %fms
%s Creating OS Transaction %p for %{public}@
%s No Implicit audio - ignoring filterToVoiceTriggerUtterances
%s ERR: ignoring filtering option as %{public}@ or %{public}@ is nil
%s ERR: ignoring filtering option as VTAssets not found on %{public}@
%s ERR: Failed training %{public}@ of %{public}@ with error %{public}@
%s Failed to create %{public}@ model with error %{public}@ for profile %{public}@
%s Releasing OS Transaction %{public}@
%s Skipping retraining for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed resetting for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed in retraining %{public}@ with error %{public}@
%s ERR: Failed to delete the model at %{public}@
%s Added utterance %{public}@ to {%{public}@, %{public}@, %{public}@, %{public}@} with score %{public}@
%s Rejected utterance %{public}@ with error %{public}@
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s Saving %{public}@ at %{public}@ as %{public}@ training.
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Skipping Model {%{public}@, %{public}@} for %{public}@
%s Skipping Model {%{public}@, %{public}@} as file doesnt exist at %{public}@
%s ERR: triggering profile retrain for asset %{publiic}@
%s Device supporting barge-in ? %{public}@
%s Failed to find matching service to IOPlatformExpertDevice
%s Fetched hardware revision : %{public}@
%s Failed to find property "config-number"
%s ::: SSR logging initialized (%s)
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s unbalanced dispatch_group_enter and leave : ignore we are ignore dispatch_group_leave
-[CSAudioChunkForTV initWithXPCObject:]
T@"NSArray",&,N,V_packets
Tf,N,V_avgPower
Tf,N,V_peakPower
TQ,N,V_timeStamp
TI,N,V_numChannels
TI,N,V_audioFormat
TQ,N,V_streamHandleID
avgPower
peakPower
timeStamp
numChannels
audioFormat
streamHandleID
packets
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
json
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
productType
productVersion
buildVersion
-[CSPlainAudioFileWriter addContextKey:withContext:]
-[CSPlainAudioFileWriter addContextKey:fromMetaFile:]
+[CSPlainAudioFileWriter saveAudioChunck:toURL:]
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
fileURL
T@"NSURL",R,N,V_fileURL
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
kCSPreferencesJarvisTriggerModeDidChangeDarwinNotification
v8@?0
VoiceTrigger Enabled
Phrase Detector Enabled
AttentiveSiri Enabled
AttentiveSiri AudioLogging Enabled
VoiceTrigger CoreSpeech Enabled
-[CSPreferences voiceTriggerInCoreSpeech]_block_invoke
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/smartSiriVolumeContextAwareLogs/
Logs/CrashReporter/Assistant/smartSiriVolumeContextAwareLogs/
-[CSPreferences getSSVLogFilePathWithSessionIdentifier:]
/tmp
%@/SSV_%@.json
VoiceTrigger/TrialAssetData
VoiceTrigger/adBlocker
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger
siriBC
Second Pass Audio Logging Enabled
Speaker Recognition Audio Logging Enabled
Jarvis Audio Logging Enabled
Jarvis Trigger Mode
Enable SoS Audio Logging
Force VoiceTrigger AP Mode
-[CSPreferences forceVoiceTriggerAPMode]_block_invoke
Force VoiceTrigger AOP Mode
-[CSPreferences forceVoiceTriggerAOPMode]_block_invoke
mobile
Logs/CrashReporter/CoreSpeech/sos/
-[CSPreferences getStartOfSpeechAudioLogFilePath]
yyyyMMdd_HHmmss.SSS
%@/%@
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
Audio Injection Enabled
Programmable Audio Injection Enabled
-[CSPreferences enableAudioInjection:withKey:]
-[CSPreferences setAudioInjectionFilePath:]
Audio Injection File Path
-[CSPreferences audioInjectionFilePath]
-[CSPreferences audioInjectionFilePath]_block_invoke
v32@?0@8Q16^B24
SpeakerId Enabled
SpeakerId Score Type
SmartSiriVolume SoftVolume Enabled
Audio Session Activation Delay
Max Number Logging Files
Max Number Grading Files
Enable SiriActivation HomePod
Enable SiriActivation watchOS
IOS Support Barge-in
-[CSPreferences iOSBargeInSupportEnabled]_block_invoke
enabled
disabled
-[CSPreferences iOSBargeInSupportEnabled]
Overwrite Remote VAD Score
Hearst First Pass Model Version
Hearst Second Pass Model Version
Hearst Fake Model Path
VoiceTrigger Companion Sync Enabled
Enable OpportuneSpeakListener Bypass
Bypass Personalized HeySiri
MultiPhraseVTEnabled
MultiChannelAudioLoggingEnabled
Enable AdBlocker Audio Logging
Enable Self Trigger Audio Logging
com.apple.ssr.vad.spg
-[SSRVoiceActivityDetector initWithContext:delegate:]
SearchOrMessaging
-[SSRVoiceActivityDetector processAudioData:numSamples:]
-[SSRVoiceActivityDetector clientSilenceFeaturesAvailable:]
context
T@"SSRSpeakerRecognitionContext",&,N,V_context
delegate
T@"<SSRVoiceActivityDetectorDelegate>",W,N,V_delegate
earSpg
T@"EARCaesuraSilencePosteriorGenerator",&,N,V_earSpg
hybridClassifier
T@"_EAREndpointer",&,N,V_hybridClassifier
defaultServerEpFeatures
T@"CSServerEndpointFeatures",&,N,V_defaultServerEpFeatures
segmentStartPointSampleCount
Tq,N,V_segmentStartPointSampleCount
numSamplesProcessed
TQ,N,V_numSamplesProcessed
endpointReported
TB,N,V_endpointReported
startpointReported
TB,N,V_startpointReported
spgQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_spgQueue
CSAudioFileReader Queue
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader prepareRecording:]
-[CSAudioFileReader startRecording]
-[CSAudioFileReader _readAudioBufferAndFeed]
-[CSAudioFileReader stopRecording]
T@"<CSAudioFileReaderDelegate>",W,N,V_delegate
OPP-
PCM-
OPUS_
Ads-
PHS-
-synced
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:speechId:]_block_invoke
FLLR
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterForPHSTrainingWithLoggingDir:inputFormat:outputFormat:]
^%@*
%@(%@)?.wav$
+[CSAudioFileManager cleanupOrphanedGradingFiles]
+[CSAudioFileManager cleanupOrphanedGradingFiles]_block_invoke
v32@?0@"NSString"8@"NSURL"16^B24
attsiri
+[CSAudioFileManager audioFileWriterForAttentiveSiri]
%@.wav
Dispose Log Queue
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v24@?0@"NSArray"8^@16
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
q24@?0@"NSURL"8@"NSURL"16
B24@?0@"NSURL"8@"NSDictionary"16
+[CSUtils(Directory) _contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:]
SSRSpeakerRecognizerPSR.m
Incorrect ctx for VTSpeakerRecognizer: %@
com.apple.ssr.psrq
-[SSRSpeakerRecognizerPSR initWithContext:delegate:]
extraSamplesAtStart
triggerEndSeconds
-[SSRSpeakerRecognizerPSR dealloc]
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:]
extraAudioAtStartInMs
tdEndInMs
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:]
lastScoreCard
T@"NSDictionary",R,N
spIdCtx
T@"SSRSpeakerRecognitionContext",&,N,V_spIdCtx
sessionId
T@"NSString",&,N,V_sessionId
lastSpeakerInfo
T@"NSDictionary",&,N,V_lastSpeakerInfo
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"<SSRSpeakerRecognizerDelegate>",W,N,V_delegate
invocationStyleStr
T@"NSString",&,N,V_invocationStyleStr
TQ,N,V_extraSamplesAtStart
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
endInSampleCount
TQ,N,V_endInSampleCount
processingEnded
TB,N,V_processingEnded
totalNumSamplesReceived
TQ,N,V_totalNumSamplesReceived
psrAnalyzer
T@"SSRSpeakerAnalyzerPSR",&,N,V_psrAnalyzer
-[CSAudioChunk chunkForChannel:]
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
-[CSAudioChunk splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
T@"NSData",R,N,V_data
TQ,R,N,V_numChannels
TQ,R,N,V_numSamples
TQ,R,N,V_sampleByteDepth
TQ,R,N,V_startSampleCount
TQ,R,N,V_hostTime
remoteVADAvailable
TB,R,N
T@"NSData",&,N,V_remoteVAD
xpcObject
T@"NSObject<OS_xpc_object>",R,N
numSamples
sampleByteDepth
startSampleCount
hostTime
data
remoteVAD
[requestHistoricalAudioDataWithHostTime = %@]
[requestHistoricalAudioDataSampleCount = %@]
[useOpportunisticZLL = %@]
[startRecordingHostTime = %llu]
[startRecordingSampleCount = %llu]
[alertBehavior = %llu %llu %llu]
[skipAlertBehavior = %@]
[requireSingleChannelLookup = %@]
[selectedChannel = %u]
TB,N,V_requestHistoricalAudioDataWithHostTime
TB,N,V_requestHistoricalAudioDataSampleCount
TQ,N,V_startRecordingHostTime
TQ,N,V_startRecordingSampleCount
TB,N,V_useOpportunisticZLL
Tq,N,V_startAlertBehavior
Tq,N,V_stopAlertBehavior
Tq,N,V_errorAlertBehavior
TB,N,V_skipAlertBehavior
localizedDescription
T@"NSString",R,N
TB,N,V_requireSingleChannelLookup
TI,N,V_selectedChannel
requestHistoricalAudioDataWithHostTime
requestHistoricalAudioDataSampleCount
startRecordingHostTime
startRecordingSampleCount
useOpportunisticZLL
startAlertBehavior
stopAlertBehavior
errorAlertBehavior
skipAlertBehavior
requireSingleChannelLookup
selectedChannel
com.apple.speakerrecognition
recognition
-[SSRSpeakerRecognitionOrchestrator initWithContext:withDelegate:error:]
ERR: Failed to init PSR and SAT recognizers - Bailing out
reason
ERR: Failed to init VAD - Bailing out
com.apple.ssr.orchestratorq
SAT+PSR
-[SSRSpeakerRecognitionOrchestrator dealloc]
-[SSRSpeakerRecognitionOrchestrator processAudio:numSamples:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator resetWithContext:]_block_invoke
SSRSpeakerRecognitionOrchestrator-%@
-[SSRSpeakerRecognitionOrchestrator _logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:]
finished
reported
%@_%d
-[SSRSpeakerRecognitionOrchestrator getLatestVoiceRecognitionInfo]
-[SSRSpeakerRecognitionOrchestrator speakerRecognizer:hasSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectStartPointAt:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectEndPointAt:]_block_invoke
T@"<SSRSpeakerRecognitionOrchestratorDelegate>",W,N,V_delegate
ssrUttLogger
T@"<CSAudioFileWriter>",&,N,V_ssrUttLogger
myriadResult
TQ,N,V_myriadResult
psrRecognizer
T@"<SSRSpeakerRecognizer>",&,N,V_psrRecognizer
satRecognizer
T@"<SSRSpeakerRecognizer>",&,N,V_satRecognizer
T@"SSRVoiceActivityDetector",&,N,V_vad
psrLastSpeakerInfo
T@"NSDictionary",&,N,V_psrLastSpeakerInfo
satLastSpeakerInfo
T@"NSDictionary",&,N,V_satLastSpeakerInfo
combinedScores
T@"NSDictionary",&,N,V_combinedScores
psrFinalSpeakerInfo
T@"NSDictionary",&,N,V_psrFinalSpeakerInfo
satFinalSpeakerInfo
T@"NSDictionary",&,N,V_satFinalSpeakerInfo
debugUtteranceAudioFilePath
T@"NSString",&,N,V_debugUtteranceAudioFilePath
debugUtteranceJsonFilePath
T@"NSString",&,N,V_debugUtteranceJsonFilePath
transaction
T@"NSObject<OS_os_transaction>",&,N,V_transaction
transDesc
T@"NSString",&,N,V_transDesc
modelFilePath
T@"NSURL",R,N
implicitTrainingRequired
retrainerType
TQ,R,N
T@"NSURL",R,N,VmodelFilePath
TB,R,N,VimplicitTrainingRequired
TQ,R,N,VretrainerType
+[CSAudioStreamRequest defaultRequestWithContext:]
+[CSAudioStreamRequest requestForLpcmRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForOpusRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForSpeexRecordSettingsWithContext:]
T@"CSAudioRecordContext",&,N,V_recordContext
TB,N,V_requiresHistoricalBuffer
TB,N,V_useCustomizedRecordSettings
Tq,N,V_audioFormat
Td,N,V_sampleRate
TI,N,V_lpcmBitDepth
TB,N,V_lpcmIsFloat
numberOfChannels
TI,N,V_numberOfChannels
TI,N,V_encoderBitRate
TB,N,V_isSiri
requiresHistoricalBuffer
useCustomizedRecordSettings
sampleRate
lpcmBitDepth
lpcmIsFloat
NumberOfChannels
encoderBitRate
isSiri
recordContext
profileID
T@"NSString",R,N,V_profileID
sysConfigRoot
T@"NSString",R,N,V_sysConfigRoot
psrConfigFilePath
T@"NSString",R,N,V_psrConfigFilePath
psrConfigRoot
T@"NSString",R,N,V_psrConfigRoot
satModelAvailable
TB,R,N,V_satModelAvailable
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[SSRVTUITrainingManager setLocaleIdentifier:]
-[SSRVTUITrainingManager createKeywordDetector]
-[SSRVTUITrainingManager prepareWithCompletion:]_block_invoke
-[SSRVTUITrainingManager cleanupWithCompletion:]
-[SSRVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[SSRVTUITrainingManager cancelTrainingForID:]
-[SSRVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[SSRVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[SSRVTUITrainingManager _startAudioSession]
-[SSRVTUITrainingManager setSuspendAudio:]
-[SSRVTUITrainingManager setSuspendAudio:]_block_invoke
-[SSRVTUITrainingManager CSVTUITrainingSessionStopListen]
-[SSRVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:]
-[SSRVTUITrainingManager audioSessionDidStopRecording:]
audioFileWriter
T@"CSPlainAudioFileWriter",&,N,V_audioFileWriter
voiceProfile
T@"SSRVoiceProfile",R
Tf,V_rms
T@"<SSRVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
-[SSRVoiceProfileRetrainerFactory init]
%@-%@
-[CSOSTransaction initWithDescription:]
-[CSOSTransaction dealloc]
route
isRemoteDevice
remoteDeviceUID
remoteDeviceProductIdentifier
%@ {route = %@, isRemoteDevice = %d, remoteDeviceUID = %@, remoteDeviceProductIdentifier = %@}
supportsSecureCoding
TB,R
T@"NSString",R,C,N,V_route
TB,R,N,V_isRemoteDevice
T@"NSUUID",R,C,N,V_remoteDeviceUID
T@"NSString",R,C,N,V_remoteDeviceProductIdentifier
Borealis Input
com.apple.VoiceTriggerUI.RecordSessionQueue
-[CSVTUIAudioSessionRecorder init]
-[CSVTUIAudioSessionRecorder _audioRecorder]
-[CSVTUIAudioSessionRecorder prepareRecord]
-[CSVTUIAudioSessionRecorder startRecording]
-[CSVTUIAudioSessionRecorder stopRecording]
-[CSVTUIAudioSessionRecorder _hasCorrectInputAudioRoute]
-[CSVTUIAudioSessionRecorder _hasCorrectOutputAudioRoute]
powerMeter
T@"CSAudioPowerMeter",&,N,V_powerMeter
audioStreamHandleId
TQ,N,V_audioStreamHandleId
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
-[SSRVoiceProfileMetaContext initWithVoiceProfile:]
[profileId: %@, language: %@, product: %@, version: %@, homeId: %@, name: %@, pitch:%@ Hz]
appDomain
T@"NSString",&,N,V_appDomain
profileId
T@"NSString",&,N,V_profileId
languageCode
T@"NSString",&,N,V_languageCode
productCategory
T@"NSString",&,N,V_productCategory
version
T@"NSNumber",&,N,V_version
dateAdded
T@"NSDate",&,N,V_dateAdded
pitch
T@"NSNumber",&,N,V_pitch
sharedSiriId
T@"NSString",&,N,V_sharedSiriId
homeId
T@"NSString",&,N,V_homeId
userName
T@"NSString",&,N,V_userName
voiceTriggerSecondPass
+[SSRTriggerPhraseDetector filterVTAudioFiles:withLocale:withAsset:]
v20@?0@"NSError"8f16
-[SSRTriggerPhraseDetector initWithLocale:asset:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]_block_invoke
v44@?0{AudioBufferList=I[1{AudioBuffer=II^v}]}8B32@"NSError"36
NDAPI: missing best_score for %@
best_score
Quasar: missing best_score for %@
detectorNDAPI
T@"SSRTriggerPhraseDetectorNDAPI",&,N,V_detectorNDAPI
detectorQuasar
T@"SSRTriggerPhraseDetectorQuasar",&,N,V_detectorQuasar
recognizerScoreScaleFactor
Tf,N,V_recognizerScoreScaleFactor
com.apple.corespeech
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
[streamHandleId = %d]
[startHostTime = %llu]
[startAlert = %d]
[stopAlert = %d]
[stopOnErrorAlert = %d]
[skipAlert = %@]
VoiceControllerCreationQueue
-[CSAudioRecorder initWithQueue:error:]
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder dealloc]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithError:]_block_invoke
-[CSAudioRecorder _voiceControllerWithError:]
-[CSAudioRecorder setContext:error:]
-[CSAudioRecorder setCurrentContext:streamHandleId:error:]
-[CSAudioRecorder prepareAudioStreamRecord:streamHandleId:error:]
-[CSAudioRecorder _startAudioStreamForAudioInjection]
-[CSAudioRecorder startAudioStreamWithOption:streamHandleId:error:]
-[CSAudioRecorder stopAudioStreamWithStreamHandleId:error:]
-[CSAudioRecorder isSessionCurrentlyActivated]
Builtin Microphone
-[CSAudioRecorder recordingSampleRateWithStreamHandleId:]
-[CSAudioRecorder isNarrowBandWithStreamHandleId:]
-[CSAudioRecorder prewarmAudioSessionWithStreamHandleId:error:]
-[CSAudioRecorder setRecordMode:streamHandleId:error:]
-[CSAudioRecorder activateAudioSessionWithReason:streamHandleId:error:]
-[CSAudioRecorder deactivateAudioSession:error:]
+[CSAudioRecorder createSharedAudioSession]
-[CSAudioRecorder enableSmartRoutingConsiderationForStream:enable:]
-[CSAudioRecorder setDuckOthersOption:]
-[CSAudioRecorder enableMiniDucking:]
-[CSAudioRecorder configureAlertBehavior:audioStreamHandleId:]
-[CSAudioRecorder voiceTriggerInfo]
en-US
-[CSAudioRecorder _updateLanguageCodeForRemoteVTEIResult:]
useRemoteBuiltInMic
-[CSAudioRecorder _processAudioBuffer:audioStreamHandleId:]
-[CSAudioRecorder _compensateChannelDataIfNeeded:receivedNumChannels:]
-[CSAudioRecorder playRecordStartingAlertAndResetEndpointerFromStream:]
-[CSAudioRecorder playAlertSoundForType:]
-[CSAudioRecorder voiceControllerDidStartRecording:forStream:successfully:error:]
-[CSAudioRecorder voiceControllerAudioCallback:forStream:buffer:]
-[CSAudioRecorder voiceControllerDidStopRecording:forStream:forReason:]
-[CSAudioRecorder voiceControllerStreamInvalidated:forStream:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerDidFinishAlertPlayback:ofType:error:]
-[CSAudioRecorder voiceControllerEncoderErrorDidOccur:error:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:force:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _getRecordSettingsWithRequest:]
voiceControllerCreationQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_voiceControllerCreationQueue
observers
T@"NSHashTable",&,N,V_observers
crashEventDelegate
T@"<CSAudioServerCrashEventProvidingDelegate>",W,N,V_crashEventDelegate
sessionEventDelegate
T@"<CSAudioSessionEventProvidingDelegate>",W,N,V_sessionEventDelegate
duckOthersOption
TB,N
corespeech.json
assets.json
speakerRecognition.json
adBlockerPayload.bin
hybridendpointer.json
hybridendpointer_marsh.json
CSAsset.m
ERR: Unknown assetType: %lu
/System/Library/PrivateFrameworks/CoreSpeech.framework
+[CSAsset fallBackAssetResourcePath]
defaultFallbackHearst
defaultFallbackAdBlocker
-[CSAsset initWithResourcePath:configFile:configVersion:assetProvderType:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
MobileAssets
Trial
Unknown
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
dictionary
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
assetProvider
TQ,R,N,V_assetProvider
totalSamplesAtEndOfCapture
spIdAudioProcessedDuration
spIdUnknownUserScore
spIdKnownUserScores
spIdUserScoresVersion
spIdScoreThresholdingType
spIdVTInvocationScoreThresholdingType
spIdNonVTInvocationScoreThresholdingType
SpIdScoreThreshold
spIdAssetVersion
spIdDirectionSigsArr
shouldRecordUserAudio
shouldRecordPayload
bestVoiceTriggerScore
segmentStartTime
segmentCounter
myriad
ssrMeta
voiceTriggerRequestUID
numEnrollmentUtt
combinationWeight
numSpeakerVectors
numExplicitUtt
numImplicitUtt
psrContext
spIdKnownUserPSRScores
spIdKnownUserPSRExpScores
satContext
spIdKnownUserSATScores
spIdKnownUserSATExpScores
Unknown InvocationStyle: %lu
tdti
tdtiexplicit
tdexplicit
Unknown CSSpIdType: %lu
+[CSUtils(SSR) spIdTypeForString:]
Unknown SpeakerRecognizerType: %lu
Unknown VoiceProfileRetrainerType: %lu
config.txt
config_td_spid.txt
config_sr_sat.txt
+[CSUtils(SSR) satConfigFileNameForCSSpIdType:]
config_tdti_spid.txt
+[CSUtils(SSR) psrConfigFileNameForCSSpIdType:]
config_ti_spid.txt
+[CSUtils(SSR) satConfigFileNameForCSSpIdType:forModelType:forAssetType:]
spid-imported
+[CSUtils(SSR) createDirectoryIfDoesNotExist:]
Library/Logs/CrashReporter/ssr
self ENDSWITH '.wav'
+[CSUtils(SSR) ssrAudioLogsCountWithinPrivacyLimit]
+[CSUtils(SSR) cleanupOrphanedVoiceIdGradingFiles]
+[CSUtils(SSR) cleanupOrphanedVoiceIdGradingFiles]_block_invoke
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[CSUtils(SSR) isSpeakerRecognitionSupportedInLocale:]
+[CSUtils(SSR) readJsonFileAtPath:]
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
kCSDeviceCategory_audioAccessory
kCSDeviceCategory_iOS_Aop_Explicit
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPod
iPad
iPhone
Accessory
+[CSUtils(SSR) deviceCategoryForDeviceProductType:]
+[CSUtils(SSR) isCurrentDeviceCompatibleWithNewerVoiceProfileAt:]
+[CSUtils(SSR) isCurrentDeviceCompatibleWithVoiceProfileAt:]
audio
pathExtension='json'
Caches/VoiceTrigger/ImplicitUtterences
+[CSUtils(SSR) getNumberOfAudioFilesInDirectory:]
v32@?0@"NSString"8Q16^B24
+[CSUtils(SSR) dumpFilesInDirectory:]
+[CSUtils(SSR) getContentsOfDirectory:]
+[CSUtils(SSR) getHomeUserIdForVoiceProfile:withCompletion:]
+[CSUtils(SSR) getHomeUserIdForVoiceProfile:withCompletion:]_block_invoke
v24@?0@"NSString"8@"NSError"16
homeUserId query for siriProfileId %@ timedout !
+[CSUtils(SSR) getVoiceProfileForSiriProfileId:forLanguageCode:]
+[CSUtils(SSR) logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:]
.wav
ERR: Audio path is nil - Bailing out
+[CSUtils(SSR) segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]
ERR: Failed initializing loggers at %@ and %@
+[CSUtils(SSR) segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]_block_invoke
ERR: Failed to read file: %@
+[CSUtils(SSR) getEnrollmentUtterancesFromDirectory:]
+[CSUtils(SSR) getExplicitEnrollmentUtterancesFromDirectory:]_block_invoke
v32@?0@"NSURL"8Q16^B24
+[CSUtils(SSR) getExplicitEnrollmentUtterancesFromDirectory:]
+[CSUtils(SSR) getExplicitMarkedEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SSR) getImplicitEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SSR) _getUtterancesFromDirectory:]
self.absoluteString ENDSWITH '.wav'
+[CSUtils(SSR) removeItemAtPath:]
Failed to get contents of %@ with error %@
+[CSUtils(SSR) moveContentsOfSrcDirectory:toDestDirectory:]
Failed to move %@ to %@ with error %@
+[CSUtils(SSR) combineScoreFromPSR:fromSAT:withCombinedWt:]
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
triggerStartSampleCount
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
isMaximized
-[CSVTUIKeywordDetector initWithAsset:]
Serial CSPolicy queue
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
VoiceTriggerEventInfo
com.apple.ssr
locale
asset
profileUpdateFailedExplicitUttScore
profileUpdateDiscardImplicitUttScore
profileUpdateExplicitUttScore
profileUpdateImplicitUttScore
profileUpdateNumPrunedUttsPHS
profileUpdateNumDiscardedUttsPHS
profileUpdateNumRetainedUttsPHS
profileUpdateScoreMSE
profileUpdateFailCode
speakerRecognitionWaitTimeMs
speakerRecognitionProcessingStatus
retrainingWaitTimeMs
retrainingStatusCode
TdPsrSATRetrainingTimedOut
TdPsrExtraAudioSamplesProcessed
TdPsrFailedDuringSATDetection
xx_XX
unknown
@"NSDictionary"8@?0
%@.%d
-[SSRLoggingAggregator pushAnalytics]
voiceProfilePruningFailureReasonCode
TQ,N,V_voiceProfilePruningFailureReasonCode
voiceProfileUpdateScoreMSE
Tf,N,V_voiceProfileUpdateScoreMSE
voiceProfileDiscardedUtteranceCount
TQ,N,V_voiceProfileDiscardedUtteranceCount
voiceProfilePrunedUtteranceCount
TQ,N,V_voiceProfilePrunedUtteranceCount
voiceProfileRetainedUtteranceCount
TQ,N,V_voiceProfileRetainedUtteranceCount
voiceProfileRetrainingFailureReasonCode
TQ,N,V_voiceProfileRetrainingFailureReasonCode
retrainingWaitTime
Td,N,V_retrainingWaitTime
TQ,N,V_speakerRecognitionProcessingStatus
speakerRecognitionWaitTime
Td,N,V_speakerRecognitionWaitTime
speakerRecognitionPSRProcessingStatus
TQ,N,V_speakerRecognitionPSRProcessingStatus
speakerRecognitionSATProcessingStatus
TQ,N,V_speakerRecognitionSATProcessingStatus
voic
carplay
hearst
raisetospeak
auto
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) macHostTimeFromBridgeHostTime:]
com.apple.corespeech.corespeechd.voiceid.xpc
v16@?0@"NSObject<OS_xpc_object>"8
-[CSVoiceIdXPCClient _handleListenerEvent:]
-[CSVoiceIdXPCClient _handleListenerError:]
v20@?0B8@"NSError"12
utterencePath
audioDevice
audioRecord
voiceTriggerEventInfo
otherCtxt
type
body
result
resultErrorDomain
resultErrorCode
xpcConnection
T@"NSObject<OS_xpc_object>",&,N,V_xpcConnection
B24@?0@"SSRVoiceProfile"8@"NSDictionary"16
-[SSRVoiceProfileStoreCleaner filterDuplicatedSiriProfilesFrom:]
Primary
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]
kAFAssistantErrorDomain
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]_block_invoke
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]
ERR: Failed to get appdomain for profile %@
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]_block_invoke
v32@?0@"SSRVoiceProfile"8Q16^B24
-[SSRVoiceProfileStoreCleaner _cleanupAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanuplanguageCodePath:forAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanupImplicitUtteranceCacheForProfile:]
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:]
-[SSRVoiceProfileStoreCleaner _cleanupContentsOfSatFolder:]
-[SSRVoiceProfileStoreCleaner _cleanupInvalidAudioFiles:]
Failed reading contents of audioDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesAtURL:]
enrollment_completed
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]
obsoleteCutOffDate is nil - Bailing out
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]_block_invoke
Error reading contents of modelDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupModelFilesAtDir:forAssetArray:]
UserVoiceProfileDateTrained
UserVoiceProfileLocale
UserVoiceProfileAppDomain
UserVoiceProfilePitch
UserVoiceProfilePath
UserVoiceProfileID
UserSharedSiriID
UserVoiceProfileUserName
VoiceProfileIdentifier
VoiceProfileCompatabiltyVersion
VoiceProfileProductType
VoiceProfileSWVersion
VoiceProfileCategoryType
UserVoiceProfileOnboardType
UserVoiceProfileExpSatVecCount
VoiceProfilePruningCookie
-[SSRVoiceProfile initNewVoiceProfileWithLocale:withAppDomain:]
Creating SSRVoiceProfile with no profileId vpDict: %@
-[SSRVoiceProfile importVoiceProfileAtPath:]_block_invoke
Q24@?0@"NSURL"8Q16
-[SSRVoiceProfile importVoiceProfileAtPath:]
ERR: Too less audio files (%ld) for onboarding
utterances passed is nil!
-[SSRVoiceProfile addUtterances:spIdType:]
Failed to copy utterances with error %@
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]_block_invoke
audiocache
td-sr-model
model
-[SSRVoiceProfile deleteModelForSpidType:recognizerType:]
enrollment_migrated
-[SSRVoiceProfile _isSATMarkedWithMarker:]
-[SSRVoiceProfile _markSATEnrollmentWithMarker:]
-[SSRVoiceProfile _updateVoiceProfileVersionFile]
-[SSRVoiceProfile updatePruningCookie:]
profileBasePath
T@"NSString",&,N,V_profileBasePath
voiceProfileBasePath
voiceProfileImplicitCacheDirPath
voiceProfileIdentity
voiceProfileVersion
pruningCookie
profileLocallyAvailable
profilePitch
T@"NSNumber",&,N,V_profilePitch
T@"NSString",R,N,V_locale
T@"NSString",R,N,V_appDomain
T@"NSDate",R,N,V_dateAdded
siriProfileId
T@"NSString",R,N,V_siriProfileId
Dictation
-[SSRTriggerPhraseDetectorQuasar initWithLocale:configPath:resourcePath:]
-[SSRTriggerPhraseDetectorQuasar reset]
Second Pass
-[SSRTriggerPhraseDetectorQuasar analyzeWavData:numSamples:]
-[SSRTriggerPhraseDetectorQuasar endAudio]
-[CSAudioStartStreamOption(AVVC) avvcStartRecordSettingsWithAudioStreamHandleId:]
assetDownloadfailed
assetFetchfailed
VoiceId
satinitfailed
satmodelfilefailed
satvectorfailed
tdsrfailed
tdsrtimeout
retrainsatfailed
explicituttrejected
toolessaudiofiles
unrecognizedmetadata
delayedscores
missinghomeidforclouduser
voiceidstaleprofiledetected
Audio
didStartWatchDogFire
didStopWatchDogFire
streamDeallocDuringStreaming
resourceNotAvailable
recordStoppedBySessionInterruption
InsufficientPriority
secondPassCompleteWatchDogFire
APLeak
launchSlow
Endpointer
endpointerModelVersionIsNil
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: (%@), silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
WordCount
TrailingSilDuration
EOSLikelihood
PauseCounts
SilencePosterior
ProcessedAudioDurationInMilliseconds
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
CSSampleCountHostTimeConverter
anchorSampleCount
TQ,N,V_anchorSampleCount
anchorHostTime
TQ,N,V_anchorHostTime
SSRVoiceRetrainingVoiceProfile
SSRVoiceRetrainingCompareVoiceProfiles
SSRVoiceRetrainingCompareVoiceProfilesSpIdType
SSRVoiceRetrainingAsset
SSRVoiceRetrainingSpIdType
SSRVoiceRetrainingFilterToVoiceTriggerUtterances
SSRVoiceRetrainingForce
SSRVoiceRetrainingPayloadProfile
retraining
ERR: VoiceProfile is invalid - Bailing out
-[SSRVoiceProfileRetrainingContext initWithVoiceRetrainingContext:error:]
ERR: Last known assets are nil - Bailing out
ERR: _modelsContext is nil - Bailing out
[SessionId: %@, Asset: %@, ProfileID: %@]
compareVoiceProfileArray
T@"NSArray",&,N,V_compareVoiceProfileArray
T@"SSRVoiceProfile",&,N,V_voiceProfile
spIdType
TQ,R,N,V_spIdType
resourceFilePath
T@"NSURL",R,N,V_resourceFilePath
filterToVoiceTriggerUtterances
TB,R,N,V_filterToVoiceTriggerUtterances
forceRetrain
TB,R,N,V_forceRetrain
maxAllowedSpeakerVectors
TQ,R,N,V_maxAllowedSpeakerVectors
modelsContext
T@"NSDictionary",R,N,V_modelsContext
Tf,R,N,V_combinationWeight
T@"CSAsset",&,N,V_asset
logAggregator
T@"SSRLoggingAggregator",&,N,V_logAggregator
T@"NSString",R,N,V_sessionId
configFilePath
T@"NSURL",R,N,V_configFilePath
voiceProfileModelFilePath
T@"NSURL",R,N,V_voiceProfileModelFilePath
compareModelFilePaths
T@"NSDictionary",R,N,V_compareModelFilePaths
voiceTriggerInfo
T@"NSDictionary",C,N,V_voiceTriggerInfo
rtsTriggerInfo
T@"NSDictionary",C,N,V_rtsTriggerInfo
triggerNotifiedMachTime
TQ,N,V_triggerNotifiedMachTime
meta_version.json
enrollment_version.json
meta_version
trainingType
explicit
implicit
handheld
near-field
far-field
utteranceWav
triggerSource
audioInputSource
otherSourceProfileMatch
containsPayload
grainedDate
+[SSRVoiceProfileMetadataManager saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:]
+[SSRVoiceProfileMetadataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSRVoiceProfileMetadataManager _writeMetaDict:forUtterancePath:]
.json
+[SSRVoiceProfileMetadataManager isUtteranceImplicitlyTrained:]
+[SSRVoiceProfileMetadataManager getUtteranceEnrollmentType:]
+[SSRVoiceProfileMetadataManager recordedTimeStampOfFile:]
yyyyMMdd
+[SSRVoiceProfileMetadataManager recordedTimeStampFromFileName:]
yyyyMMdd-HHmmss
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
+[CSVTUIRegularExpressionMatcher matchWithString:TrailingStr:LeadingStr:Pattern:]
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
v16@?0@"NSDictionary"8
-[CSVTUITrainingSessionWithPayload handleAudioInput:]_block_invoke
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload closeSessionWithStatus:successfully:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
T@"NSDictionary",&,N,V_voiceTriggerEventInfo
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
triggerFireMachTime
satTriggered
firstPassTriggerSource
deviceHandHeld
ApplicationProcessor
com.apple.voicetrigger.PHSProfileDownloadTrigger
com.apple.voicetrigger.speakermodelUpdated
com.apple.voicetrigger.retrainRequired
com.apple.voicetrigger.voiceprofilesync
VoiceProfileAvailabilityMetaBlobVersion
com.apple.cs.profileManager
VoiceTrigger/SAT
-[SSRVoiceProfileManager discardSiriEnrollmentForProfileId:forLanguageCode:]
-[SSRVoiceProfileManager _getVoiceProfilesForSiriProfileId:withLanguageCode:]
ERR: profile is nil - Bailing out
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]
ERR: Context is nil - Bailing out
ERR: Failed to copy %@ to %@, error: %@
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]_block_invoke
ERR: Failed in marking Enrollment as Successful for profile %@
ImplicitTraining
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]_block_invoke
primary
v16@?0@"NSError"8
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
Failed to get asset for locale %@
%lld
ERR: Voice Profile not found for %@ - Bailing out
ERR: Voice Profile locale %@ not matching with %@ - Bailing out
v32@?0@"NSError"8@"NSURL"16@"NSURL"24
-[SSRVoiceProfileManager notifyUserVoiceProfileDownloadReadyForUser:getData:completion:]_block_invoke
Primary User
Missing downloadTriggerBlock - Bailing out
Unknown device category for device type %@ - Bailing out
-[SSRVoiceProfileManager notifyUserVoiceProfileUpdateReady]_block_invoke
userAddition timedout after %fms
Enable VoiceTrigger Upon VoiceProfile Sync For Language
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke_2
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke
v24@?0@"NSError"8@"NSString"16
@"NSError"16@?0Q8
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]
SAT download path is nil - Bailing out
-[SSRVoiceProfileManager _downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:]
Download for %@ failed with %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]
Skipping profile Update for %@ in %@
ERR: Failed to import profile %@ for %@
ERR: Migrated language %@ for %@ but failed to mark SAT enrollment
ERR: Failed to mark migrated for %@ in language %@
Failed to init retrainCtxt for profileID %@ with error %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]_block_invoke
userAddition timedout for siriProfileId %@ after %fms
Failed to enroll user - %@
SourcePath (%@) or DestinationPath (%@) is nil - Bailing out
-[SSRVoiceProfileManager _enableVoiceTriggerIfLanguageMatches:]
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpdateNewerZone
_%d_%d
-[SSRVoiceProfileManager _getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:]
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:]_block_invoke
-[SSRVoiceProfileManager uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:]_block_invoke
@"NSError"24@?0@"SSRVoiceProfileMetaContext"8@"NSString"16
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]_block_invoke
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadComplete]_block_invoke
-[SSRVoiceProfileManager _getVoiceProfilePathsToBeUploadedForSiriProfileId:]
-[SSRVoiceProfileManager _copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:]
Failed to copy to SATUpload Diretory : %@
v24@?0@"NSError"8Q16
-[SSRVoiceProfileManager _copyVoiceProfileAtPath:toPath:]
ERR: Number of training utterances copied from %@ to %@ is too less %ld
Cannot delete existing SATUpload Diretory : %@
-[SSRVoiceProfileManager _prepareVoiceProfileWithSiriProfileId:withUploadBlock:]
Cannot create SAT Upload Directory : %@
Failed to upload %@ with error %@ - Bailing out
-[SSRVoiceProfileManager _markVoiceProfileTrainingSyncForLanguage:]
Enable VoiceProfile Training Sync For Language
-[SSRVoiceProfileManager _isMarkedForVoiceProfileTrainingSyncForLanguage:]
-[SSRVoiceProfileManager getCachedVoiceProfileAvailabilityMetaBlob]
-[SSRVoiceProfileManager getCachedVoiceProfileAvailabilityMetaBlob]_block_invoke
v24@?0@"NSDictionary"8@"NSError"16
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:]
ERR: Unknown product type. Returning false, language: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]
ERR: Unknown device-category for device: %@, languageCode: %@
ERR: Improper VoiceProfile detected: %@, languageCode: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]_block_invoke
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:]
-[SSRVoiceProfileManager enableVoiceTriggerUponVoiceProfileSyncForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]_block_invoke
v16@?0@"NSArray"8
Caches/VoiceTrigger/SATLegacyUpload
-[SSRVoiceProfileManager provisionedVoiceProfilesForAppDomain:withLocale:]
q24@?0@"SSRVoiceProfile"8@"SSRVoiceProfile"16
-[SSRVoiceProfileManager provisionedVoiceProfilesForLocale:]
-[SSRVoiceProfileManager getVoiceProfileAnalyticsForAppDomain:withLocale:]
ERR: Voice Profile sent as nil - Bailing out
-[SSRVoiceProfileManager triggerRetrainingVoiceProfile:withContext:withCompletion:]
ERR: Voice Profile not found for Id %@ - Bailing out
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]_block_invoke
-[SSRVoiceProfileManager isSATEnrolledForSiriProfileId:forLanguageCode:]
ERR: Voice Profile passed is nil - Bailing out
-[SSRVoiceProfileManager deleteUserVoiceProfile:]
-[SSRVoiceProfileManager deleteAllVoiceProfilesForAppDomain:]
Library/Caches/VoiceTrigger
Caches/VoiceTrigger/SATUpload
td/audio
-[SSRVoiceProfileManager _isLegacyEnrollmentMarkedWith:forLanguageCode:]
currentDeviceCategory
TQ,N,V_currentDeviceCategory
xpcClient
T@"CSVoiceIdXPCClient",&,N,V_xpcClient
SSRVoiceProfileStore
Class getSSRVoiceProfileStoreClass(void)_block_invoke
SSRVoiceProfileManager.m
Unable to find class %s
void *CoreSpeechLibrary(void)
/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
/System/Library/PrivateFrameworks/CoreSpeech.framework/Contents/MacOS/CoreSpeech
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copySamplesFrom:to:channelIdx:]
-[CSAudioCircularBuffer copybufferFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
bufferLength
TQ,N,V_bufferLength
copySamples
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
-[CSNNVADEndpointAnalyzer init]
-[CSNNVADEndpointAnalyzer preheat]
-[CSNNVADEndpointAnalyzer reset]
-[CSNNVADEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSNNVADEndpointAnalyzer recordingStoppedForReason:]
-[CSNNVADEndpointAnalyzer stopEndpointer]
-[CSNNVADEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:recordSettings:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
implDelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N
canProcessCurrentRequest
activeChannel
TQ,N
endpointerModelVersion
elapsedTimeWithNoSpeech
T@"<CSEndpointAnalyzerDelegate>",W,N,Vdelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N,VimplDelegate
TB,R,N,VcanProcessCurrentRequest
TQ,N,VactiveChannel
Tq,N,VendpointStyle
Td,N,Vdelay
Td,N,VstartWaitTime
Td,N,VautomaticEndpointingSuspensionEndTime
Td,N,VminimumDurationForEndpointer
Td,R,N,VlastEndOfVoiceActivityTime
Td,R,N,VlastStartOfVoiceActivityTime
+[CSAudioRecordContext defaultContext]
CSAudioRecordTypeUnspecified
CSAudioRecordTypeHomePress
CSAudioRecordTypeWiredHeadsetButtonPress
CSAudioRecordTypeBluetoothHeadSetButtonPress
CSAudioRecordTypeUIButtonPress
CSAudioRecordTypeServerInvoke
CSAudioRecordTypeVoiceTrigger
CSAudioRecordTypeStark
CSAudioRecordTypeTVRemote
CSAudioRecordTypeRaiseToSpeak
CSAudioRecordTypeHearstDoubleTap
CSAudioRecordTypeHearstVoice
CSAudioRecordTypeJarvis
CSAudioRecordTypePost
CSAudioRecordTypeDictation
CSAudioRecordTypeVoiceTriggerTraining
CSAudioRecordTypeOpportuneSpeaker
CSAudioRecordTypeRemoraVoice
CSAudioRecordTypeUnknown
recordType[%@] deviceId[%@] alwaysUseBuiltInMic[%d]
Tq,N,V_type
T@"NSString",&,N,V_deviceId
TB,N,V_alwaysUseRemoteBuiltInMic
alwaysUseRemoteBuiltInMic
deviceId
Serial SSRAssetManager queue
-[SSRAssetManager init]
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke_2
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke
v32@?0@"<SSRAssetProviding>"8Q16^B24
-[SSRAssetManager _latestVersionedAssetOfType:fromProviders:forLocale:]_block_invoke
assetProviders
T@"NSArray",&,N,V_assetProviders
currentLanguageCode
T@"NSString",&,N,V_currentLanguageCode
T@"<SSRAssetManagerDelegate>",W,N,V_delegate
-[SSRSpeakerRecognitionController voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:]
ERR: Scorecard not available in score dictionary - %@
-[SSRSpeakerRecognitionController voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:]
T@"<SSRSpeakerRecognitionControllerDelegate>",W,N,V_delegate
orchestrator
T@"SSRSpeakerRecognitionOrchestrator",&,N,V_orchestrator
T@"NSDictionary",&,N,V_lastScoreCard
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]
q24@?0@"MAAsset"8@"MAAsset"16
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]_block_invoke_2
v32@?0@"MAAsset"8Q16^B24
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsIPad
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeakerRecognitionAssets
com.apple.MobileAsset.SpeechEndpointAssets
-[SSRMobileAssetProvider _buildAssetQueryForAssetType:]
-[SSRMobileAssetProvider _installedMobileAssetOfType:forLanguage:]
-[SSRMobileAssetProvider _findLatestInstalledAsset:]
com.apple.corespeech.voiceprofilestore
-[SSRVoiceProfileStore userVoiceProfilesForAppDomain:]
-[SSRVoiceProfileStore userVoiceProfilesForLocale:]
-[SSRVoiceProfileStore migrateVoiceProfilesIfNeededWithCompletionBlock:]_block_invoke
Filtered languages is nil - %@
spid
trained_users.json
Could not read existing %@ file: err: %@
-[SSRVoiceProfileStore cleanupDuplicatedProfiles]_block_invoke
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke_2
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke
-[SSRVoiceProfileStore cleanupVoiceProfileModelFilesForLocale:]_block_invoke
-[SSRVoiceProfileStore _synchronizeSiriVoiceProfilesWithAssistant]_block_invoke
com.apple.siri.corespeech.voiceprofilelist.change
-[SSRVoiceProfileStore addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:]_block_invoke
Profile %@ is full - Ignoring
Utterance %@ in profile %@ not satisfied the implicit VT policy
Rejecting Implicit utterance %@ for profile %@
@"NSError"24@?0@"NSURL"8@"NSDictionary"16
Utterance %@ rejected for profile %@
v32@?0@"NSError"8@"NSDictionary"16@"NSDictionary"24
Utterance %@ in profile %@ not satisfied the implicit policy
-[SSRVoiceProfileStore _logVoiceProfileConfusionWithCleanup:]
B16@?0@"NSDictionary"8
-[SSRVoiceProfileStore _logVoiceProfileConfusionWithCleanup:]_block_invoke
v32@?0@"<SSRVoiceProfileRetrainer>"8Q16^B24
-[SSRVoiceProfileStore evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:]
v32@?0@"NSString"8@"NSNumber"16^B24
Profile is nil!
-[SSRVoiceProfileStore addUserVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _deleteUserVoiceProfile:]
Deleting profile data at %@ failed with error %@
Profile path is nil!
-[SSRVoiceProfileStore checkIfVoiceProfile:needsUpdatedWith:withCategory:]
-[SSRVoiceProfileStore _checkIfRetrainingRequiredForProfile:]_block_invoke
Failed to init retrainers for profileID %@ with ctxt %@
B16@?0Q8
-[SSRVoiceProfileStore retrainVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _updateTrainedUsersWithAction:UserVoiceProfile:]
Voice Profile not found for profileId: %@ - Bailing out
-[SSRVoiceProfileStore updateVoiceProfile:withUserName:]
-[SSRVoiceProfileStore _retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:]
VoiceProfile is nil - Bailing out
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:]
context is nil - Bailing out
Invalid spIdType %d - Bailing out
SSRVoiceProfileStore retrainer - %@
Too less (%d) audio files in %@ 
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]_block_invoke
Failed to copy %@ to %@ with error %@
-[SSRVoiceProfileStore copyAudioFiles:toProfile:forModelType:]
voiceProfileArray
T@"NSMutableArray",&,V_voiceProfileArray
storePrefs
T@"SSRVoiceProfileStorePrefs",&,N,V_storePrefs
-[CSSelectiveChannelAudioFileWriter initWithURL:inputFormat:outputFormat:channelBitset:]
v16@?0Q8
-[CSSelectiveChannelAudioFileWriter addSamples:numSamples:]
TI,R,N,V_numberOfChannels
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
T@"<CSAudioDecoderDelegate>",W,V_delegate
+[SSREnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[SSREnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSREnrollmentDataManager writeMetaDict:atMetaPath:]
Known User Voice Profiles
Voice Profile Store Version
SSRSpeakerRecognitionStyle
SSRSpeakerRecognitionAsset
SSRSpeakerRecognitionAssetArray
SSRSpeakerRecognitionVADAssetPath
SSRSpeakerRecognitionLocale
SSRSpeakerRecognitionVTEventInfo
SSRSpeakerRecognitionProfileArray
SSRSpeakerRecognitionUsePayloadProfile
SSRSpeakerRecognitionMaxAudioSecs
SSRSpeakerRecognitionOSTransactionReqd
com.apple.siri
com.apple.siridebug
ERR: SpeakerRecognition not enabled - Bailing out
-[SSRSpeakerRecognitionContext initWithVoiceRecognitionContext:error:]
ERR: Invalid Speaker Recognition style - Bailing out
ERR: Asset not picked - Bailing out
ERR: Endpointer Asset not picked - Bailing out
v24@?0@"NSDictionary"8@"NSDictionary"16
ERR: ModelsContext is nil for locale %@ - Bailing out
%@_%@_%@
[SessionId: %@, RecognitionStyle:(%lu)%@, Asset: %@, vtEventInfo: %@]
-[SSRSpeakerRecognitionContext composeModelContextsForProfiles:forSpIdType:forAsset:completion:]
-[SSRSpeakerRecognitionContext pickAssetForProfiles:forSpIdType:withAssetArray:]
-[SSRSpeakerRecognitionContext dealloc]
T@"NSArray",&,N,V_voiceProfileArray
TQ,N,V_spIdType
T@"NSString",&,N,V_locale
TQ,R,N,V_activeChannel
scoreType
TQ,R,N,V_scoreType
recognitionStyle
TQ,R,N,V_recognitionStyle
vtEventInfo
T@"NSDictionary",R,N,V_vtEventInfo
vadResourcePath
T@"NSURL",R,N,V_vadResourcePath
expModelsContext
T@"NSDictionary",R,N,V_expModelsContext
numEnrollmentUtterances
maxAllowedAudioSamples
TQ,R,N,V_maxAllowedAudioSamples
osTransactionReqd
TB,R,N,V_osTransactionReqd
debugUtteranceAudioFile
T@"NSString",R,N,V_debugUtteranceAudioFile
debugUtteranceMetaFile
T@"NSString",R,N,V_debugUtteranceMetaFile
voiceProfilesModelFilePaths
T@"NSDictionary",R,N,V_voiceProfilesModelFilePaths
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
com.apple.da
liveOnHomePod
U+73bmG4kBGj6kpreQXUTQ
InternalBuild
CSSafeSetOutErrorWithNSError
PTQ+ABwag03BwO/CKvIK/A
+[CSUtils isIOSDeviceSupportingBargeIn]_block_invoke
BuildVersion
IOPlatformExpertDevice
+[CSUtils deviceHwRevision]
config-number
speakerRecognition
satThreshold
implicitProfileThreshold
implicitProfileDeltaThreshold
implicitVTThreshold
pruningExplicitSATThreshold
pruningExplicitPSRThreshold
pruningSATThreshold
pruningPSRThreshold
numPruningRetentionUtt
maxEnrollmentUtterances
configFileRecognizer
configFileNDAPI
voiceTriggerSecondPassAOP
implicit_training_enabled
multiUserHighScoreThreshold
multiUserLowScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
recognizer.json
containsSpeakerRecognitionCategory
satScoreThreshold
Tf,R,N
Tq,R,N
psrCombinationWeight
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
satVTImplicitThreshold
satImplicitTrainingEnabled
pruningExplicitUttThresholdSAT
pruningExplicitUttThresholdPSR
pruningThresholdSAT
pruningThresholdPSR
pruningNumRetentionUtterance
maxAllowedEnrollmentUtterances
voiceProfilePruningCookie
keywordDetectorNDAPIConfigFilePath
keywordDetectorQuasarConfigFilePath
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
::: Initializing VoiceRecognition logging...
CSLogInitIfNeeded_block_invoke
gitrelno_unavailable
_CSGetOrCreateAudioLogDirectory
-[CSDispatchGroup leave]
mcplsupoxeps
@pbhw
pbtb
pbiu
otua
ciov
bhev
eltb
siar
tdtb
cvdh
cvpc
tcid
tsop
rtsh
tvps
cvdh
333333
softlink:r:path:/System/Library/PrivateFrameworks/CoreAnalytics.framework/CoreAnalytics
softlink:r:path:/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
CSAudioChunkForTV
CSPlainAudioFileWriter
CSAudioFileWriter
NSObject
CSPreferences
AVVC
SSRVoiceActivityDetector
EARCaesuraSilencePosteriorGeneratorDelegate
CSAudioFileReader
CSAudioFileManager
SSRSpeakerAnalyzerPSR
Directory
SSRSpeakerRecognizerPSR
SSRSpeakerAnalyzerPSRDelegate
SSRSpeakerRecognizer
CSAudioChunk
CSAudioStartStreamOption
CSAudioPowerMeter
SSRTriggerPhraseDetectorNDAPI
SSRSpeakerRecognitionOrchestrator
SSRSpeakerRecognizerDelegate
SSRVoiceActivityDetectorDelegate
SSRVoiceProfileRetrainerSAT
SSRVoiceProfileRetrainer
CSAudioStreamRequest
SSRDESRecordWriter
AudioStreamBasicDescription
SSRSpeakerRecognitionScorer
SSRVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
SSRVoiceProfileRetrainerFactory
CSOSTransaction
CSAudioRecordDeviceInfo
NSCopying
NSSecureCoding
NSCoding
CSVTUIAudioSessionRecorder
CSAudioRecorderDelegate
CSVTUIAudioSession
SSRVoiceProfileMetaContext
SSRTriggerPhraseDetector
CSRemoteControlClient
CSAsset
debugDescription
remoteVoiceActivityVADBuffer
CSAudioRecorder
AVVoiceControllerRecordDelegate
CSAudioDecoderDelegate
CSAudioFileReaderDelegate
CSAudioServerCrashEventProviding
CSAudioSessionEventProviding
LanguageCode
SSRTrialAssetProvider
SSRAssetProviding
CSVTUIKeywordDetector
CSPolicy
CSEventMonitorDelegate
CSRemoteRecordClient
SSRVoiceProfileComposer
SSRVoiceProfileRetrainerPSR
SSRLoggingAggregator
RecordContext
SSRSpeakerRecognizerSAT
Time
CSVoiceIdXPCClient
SSRVoiceProfileStoreCleaner
SSRVoiceProfile
SSRTriggerPhraseDetectorQuasar
CSPowerAssertionGibraltar
CSDiagnosticReporter
CSServerEndpointFeatures
CSAVVoiceTriggerClientManager
LPCMTypeConversion
CSVTUIEditDistance
CSAudioTimeConverter
SSRVoiceProfilePruner
SSRVoiceProfileRetrainingContext
SSRVoiceProfileModelContext
CSVoiceTriggerEventInfoProvider
SSRVoiceProfileMetadataManager
ResourcePathHash
CSVTUIRegularExpressionMatcher
CSVTUITrainingSessionWithPayload
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
CSVTUITrainingSession
SSRVoiceProfileManager
CSAudioCircularBuffer
CSAudioZeroFilter
CSNNVADEndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
CSAudioRecordContext
SSRAssetManager
SSRSpeakerRecognitionController
SSRSpeakerRecognitionOrchestratorDelegate
SSRMobileAssetProvider
SSRVoiceProfileStore
CSSelectiveChannelAudioFileWriter
CSAudioDecoder
SSRBiometricMatch
SSRSpeakerAnalyzerSAT
SSREnrollmentDataManager
SSRVoiceProfileStorePrefs
SSRSpeakerRecognitionContext
SSRSpeakerRecognitionModelContext
CSVTUIASRGrammars
NSURLSessionDelegate
SSRPitchExtractor
CSUtils
SSRAESKeyManager
CSConfig
SpeakerRecognition
AudioHardware
CSDispatchGroup
init
_cs_initWithXPCObject:
_cs_xpcObject
initWithXPCObject:
xpcObject
packets
setPackets:
avgPower
setAvgPower:
peakPower
setPeakPower:
timeStamp
setTimeStamp:
numChannels
setNumChannels:
audioFormat
setAudioFormat:
streamHandleID
setStreamHandleID:
.cxx_destruct
_avgPower
_peakPower
_numChannels
_audioFormat
_packets
_timeStamp
_streamHandleID
utteranceFileASBD
lpcmInt16ASBD
initWithURL:inputFormat:outputFormat:
fileURLWithPath:
endAudio
dealloc
URLByDeletingPathExtension
URLByAppendingPathExtension:
defaultManager
path
fileExistsAtPath:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
deviceProductType
deviceProductVersion
deviceBuildVersion
numberWithBool:
dictionaryWithObjects:forKeys:count:
dictionaryWithDictionary:
setObject:forKey:
dataWithJSONObject:options:error:
writeToFile:atomically:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
mutableCopy
addContextKey:withContext:
lpcmNonInterleavedASBD
lpcmInterleavedASBD
data
bytes
numSamples
addSamples:numSamples:
saveAudioChunck:toURL:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
initWithURL:
initWithFilepath:
addContextKey:fromMetaFile:
fileURL
isWriting
fFile
inASBD
outASBD
_fileURL
boolValue
_storeModeEnabled
sharedPreferences
runningVoiceTriggerOnMac
setFileLoggingLevel:
fileLoggingLevel
intValue
stringByAppendingPathComponent:
baseDir
fileExistsAtPath:isDirectory:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
localizedDescription
stringWithFormat:
assistantLogDirectory
myriadHashDirectory
numberWithInteger:
integerValue
date
stringByReplacingOccurrencesOfString:withString:
floatValue
interstitialRelativeDirForLevel:
enableAudioInjection:withKey:
audioInjectionEnabledWithKey:
count
enumerateObjectsUsingBlock:
unsignedIntegerValue
smartSiriVolumeContextAwareEnabled
fileLoggingIsEnabled
voiceTriggerEnabled
phraseSpotterEnabled
isAttentiveSiriEnabled
isAttentiveSiriAudioLoggingEnabled
voiceTriggerInCoreSpeech
twoShotNotificationEnabled
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
ssvLogDirectory
getSSVLogFilePathWithSessionIdentifier:
trialBaseAssetDirectory
getCatAdBlockerAssetUpdateDirectory
assistantAudioFileLogDirectory
myriadHashFilePath
secondPassAudioLoggingEnabled
speakerRecognitionAudioLoggingEnabled
jarvisAudioLoggingEnabled
setJarvisTriggerMode:
getJarvisTriggerMode
startOfSpeechAudioLoggingEnabled
forceVoiceTriggerAPMode
forceVoiceTriggerAOPMode
getStartOfSpeechAudioLogFilePath
_isDirectory:
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
enableAudioInjection:
audioInjectionEnabled
enableProgrammableAudioInjection:
programmableAudioInjectionEnabled
setAudioInjectionFilePath:
audioInjectionFilePath
isPHSSupported
_isRemoteVoiceTriggerAvailable
isSpeakerRecognitionAvailable
speakerIdScoreReportingType
smartSiriVolumeSoftVolumeEnabled
smartSiriVolumeContextAwareLoggingEnabled
audioSessionActivationDelay
maxNumLoggingFiles
maxNumGradingFiles
useSiriActivationSPIForHomePod
useSiriActivationSPIForwatchOS
iOSBargeInSupportEnabled
shouldOverwriteRemoteVADScore
overwritingRemoteVADScore
setHearstFirstPassModelVersion:
setHearstSecondPassModelVersion:
fakeHearstModelPath
companionSyncVoiceTriggerUtterancesEnabled
opportuneSpeakListenerBypassEnabled
bypassPersonalizedHeySiri
isMultiPhraseVTEnabled
isMultiChannelAudioLoggingEnabled
isAdBlockerAudioLoggingEnabled
isSelfTriggerFileLoggingEnabled
avvcContext
objectForKeyedSubscript:
initWithMode:deviceUID:
avvcContextSettings
getFixedHighPrioritySerialQueueWithLabel:
_initializeSPGWithContext:
vadResourcePath
initWithConfiguration:modelVersion:
inputRecordingSampleRate
requestSupportedWithSamplingRate:
defaultServerEndpointFeatures
wordCount
trailingSilenceDuration
endOfSentenceLikelihood
silencePosterior
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
addAudio:numSamples:
initWithConfigFile:samplingRate:queue:
setDelegate:
eosLikelihood
pauseCounts
silenceFramesCountMs
silenceProbability
silenceDurationMs
processedAudioMs
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:
didEndpointWithFeatures:audioTimestamp:featuresToLog:endpointPosterior:extraDelayMs:
SSRVoiceActivityDetector:didDetectEndPointAt:
SSRVoiceActivityDetector:didDetectStartPointAt:
clientSilenceFeaturesAvailable:
silenceDurationEstimateAvailable:numEstimates:clientProcessedAudioMs:
initWithContext:delegate:
processAudioData:numSamples:
resetWithContext:
context
setContext:
delegate
earSpg
setEarSpg:
hybridClassifier
setHybridClassifier:
defaultServerEpFeatures
setDefaultServerEpFeatures:
segmentStartPointSampleCount
setSegmentStartPointSampleCount:
numSamplesProcessed
setNumSamplesProcessed:
endpointReported
setEndpointReported:
startpointReported
setStartpointReported:
spgQueue
setSpgQueue:
_numConsecutiveNonSilenceFrames
_endpointReported
_startpointReported
_context
_delegate
_earSpg
_hybridClassifier
_defaultServerEpFeatures
_segmentStartPointSampleCount
_numSamplesProcessed
_spgQueue
unsignedIntValue
_readAudioBufferAndFeed
audioFileReaderDidStartRecording:successfully:error:
dataWithLength:
audioFileReaderBufferAvailable:buffer:atTime:
audioFileReaderDidStopRecording:forReason:
close
setRecordBufferDuration:
prepareRecording:
startRecording
stopRecording
readSamplesFromChannelIdx:
_fFile
_queue
_audioFeedTimer
_bufferDuration
_outASBD
_sharedAudioLoggingQueue
URLByDeletingLastPathComponent
containsString:
removeItemAtURL:error:
inputRecordingNumberOfChannels
inputRecordingSampleByteDepth
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
getBytes:length:
initWithData:encoding:
isEqualToString:
offsetInFile
length
writeData:
_createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterForPHSTrainingWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:
_getDateLabel
getNumElementInBitset:
lpcmNonInterleavedASBDWithSampleRate:numberOfChannels:
lpcmInterleavedASBDWithSampleRate:numberOfChannels:
initWithURL:inputFormat:outputFormat:channelBitset:
_createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:
pruneLogFiles
URLWithString:
removeLogFilesInDirectory:matchingPattern:beforeDays:
pruneNumberOfGradingFilesTo:
pruneNumberOfLogFilesTo:
arrayWithObjects:
countByEnumeratingWithState:objects:count:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
cleanupOrphanedGradingFiles
arrayWithObjects:count:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
dictionary
absoluteString
lastPathComponent
stringByDeletingPathExtension
setObject:forKeyedSubscript:
removeObjectForKey:
removeItemAtPath:error:
enumerateKeysAndObjectsUsingBlock:
generateDeviceAudioLogging:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterForOpportuneSpeakListenerWithInputFormat:outputFormat:
createAudioFileWriterForPHSTrainingWithInputFormat:outputFormat:
createAudioFileWriterForRemoteVADWithInputFormat:outputFormat:withLoggingUUID:
createAudioFileWriterWithInputFormat:outputFormat:withLoggingUUID:
createSelectiveChannelAudioFileWriterWithChannelBitset:
createAudioFileWriterForAdBlockerWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
audioFileWriterForAttentiveSiri
initWithVoiceRecognitionContext:delegate:queue:
processAudioData:
resetForNewRequest
getVoiceRecognizerResults
_sharedDisposeLoggingQueue
dateWithTimeIntervalSinceNow:
distantFuture
getResourceValue:forKey:error:
compare:
URLsInDirectory:matchingPattern:completion:
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
recognitionStyle
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
_initializeWithContext:
stringForInvocationStyle:
sessionId
vtEventInfo
numberWithFloat:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:
voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:
lastScoreCard
spIdCtx
setSpIdCtx:
setSessionId:
lastSpeakerInfo
setLastSpeakerInfo:
queue
setQueue:
invocationStyleStr
setInvocationStyleStr:
extraSamplesAtStart
setExtraSamplesAtStart:
vtEndInSampleCount
setVtEndInSampleCount:
endInSampleCount
setEndInSampleCount:
processingEnded
setProcessingEnded:
totalNumSamplesReceived
setTotalNumSamplesReceived:
psrAnalyzer
setPsrAnalyzer:
_processingEnded
_spIdCtx
_sessionId
_lastSpeakerInfo
_invocationStyleStr
_extraSamplesAtStart
_vtEndInSampleCount
_endInSampleCount
_totalNumSamplesReceived
_psrAnalyzer
initWithBytes:length:
dataForChannel:
copy
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:remoteVAD:
subdataWithRange:
dataWithCapacity:
appendData:
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
apply12dBGain:
subChunkFrom:numSamples:
chunkForChannel:
dataWithRemoteVADWithScaleFactor:numAudioSamplesPerRemoteVAD:
remoteVADAvailable
subChunkFrom:numSamples:forChannel:
remoteVADSubChunkFrom:numSamples:numAudioSamplesPerRemoteVAD:
gainCompensatedChunk
skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
sampleByteDepth
startSampleCount
hostTime
remoteVAD
setRemoteVAD:
_data
_numSamples
_sampleByteDepth
_startSampleCount
_hostTime
_remoteVAD
setSkipAlertBehavior:
appendFormat:
noAlertOption
requestHistoricalAudioDataWithHostTime
setRequestHistoricalAudioDataWithHostTime:
requestHistoricalAudioDataSampleCount
setRequestHistoricalAudioDataSampleCount:
startRecordingHostTime
setStartRecordingHostTime:
startRecordingSampleCount
setStartRecordingSampleCount:
useOpportunisticZLL
setUseOpportunisticZLL:
startAlertBehavior
setStartAlertBehavior:
stopAlertBehavior
setStopAlertBehavior:
errorAlertBehavior
setErrorAlertBehavior:
skipAlertBehavior
requireSingleChannelLookup
setRequireSingleChannelLookup:
selectedChannel
setSelectedChannel:
_requestHistoricalAudioDataWithHostTime
_requestHistoricalAudioDataSampleCount
_useOpportunisticZLL
_skipAlertBehavior
_requireSingleChannelLookup
_selectedChannel
_startRecordingHostTime
_startRecordingSampleCount
_startAlertBehavior
_stopAlertBehavior
_errorAlertBehavior
_reset
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
peakValueSinceLastCall
setPeakValueSinceLastCall:
setSawNotANumber:
setSawInfinity:
_zapgremlins:
_linearToDB:
_ampToDB:
initWithSampleRate:
reset
processShortBuffer:stride:inFrameToProcess:
processFloatBuffer:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_averagePowerF
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_sampleRate
_previousBlockSize
_decay1
_peakDecay1
_clipping
initWithConfigPath:resourcePath:phraseId:
analyzeWavData:numSamples:
getSuperVectorWithEndPoint:
errorWithDomain:code:userInfo:
logAggregator
setSpeakerRecognitionProcessingStatus:
debugUtteranceAudioFile
debugUtteranceMetaFile
updateDebugFilePathsForSegment:
ssrAudioLogsDir
createDirectoryIfDoesNotExist:
ssrAudioLogsCountWithinPrivacyLimit
timeIntervalSinceReferenceDate
maxAllowedAudioSamples
osTransactionReqd
UUID
UUIDString
UTF8String
_resetWithContext:
sharedInstance
submitVoiceIdIssueReport:
combinationWeight
combineScoreFromPSR:fromSAT:withCombinedWt:
numEnrollmentUtterances
scoreType
numberWithUnsignedInteger:
configVersion
numberWithDouble:
logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:
stringByDeletingLastPathComponent
stringByAppendingPathExtension:
orchestratorScoresWithPSRScores:withSATScores:withSegmentStartTime:
timeIntervalSinceDate:
setSpeakerRecognitionWaitTime:
pushAnalytics
voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:
_logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:
voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:
initWithContext:withDelegate:error:
processAudio:numSamples:
getLatestVoiceRecognitionInfo
ssrUttLogger
setSsrUttLogger:
myriadResult
setMyriadResult:
psrRecognizer
setPsrRecognizer:
satRecognizer
setSatRecognizer:
setVad:
psrLastSpeakerInfo
setPsrLastSpeakerInfo:
satLastSpeakerInfo
setSatLastSpeakerInfo:
combinedScores
setCombinedScores:
psrFinalSpeakerInfo
setPsrFinalSpeakerInfo:
satFinalSpeakerInfo
setSatFinalSpeakerInfo:
debugUtteranceAudioFilePath
setDebugUtteranceAudioFilePath:
debugUtteranceJsonFilePath
setDebugUtteranceJsonFilePath:
transaction
setTransaction:
transDesc
setTransDesc:
_lastScoreReportTimeStamp
_lastSegmentStartTime
_segmentCounter
_numSamplesAddedToSpeakerRecognizers
_endAudioCalled
_startPointReported
_ssrUttLogger
_myriadResult
_psrRecognizer
_satRecognizer
_vad
_psrLastSpeakerInfo
_satLastSpeakerInfo
_combinedScores
_psrFinalSpeakerInfo
_satFinalSpeakerInfo
_debugUtteranceAudioFilePath
_debugUtteranceJsonFilePath
_transaction
_transDesc
initWithVoiceRetrainingContext:
resetModelForRetraining
addUtterances:withPolicy:withCompletion:
needsRetrainingWithAudioFiles:
purgeLastSpeakerEmbedding
purgeConfusionInformationWithPolicy:
modelFilePath
implicitTrainingRequired
retrainerType
audioConverterBitrate
setEncoderBitRate:
setSampleRate:
setNumberOfChannels:
inputRecordingSampleBitDepth
setLpcmBitDepth:
inputRecordingIsFloat
setLpcmIsFloat:
setRecordContext:
setUseCustomizedRecordSettings:
defaultRequestWithContext:
requestForLpcmRecordSettingsWithContext:
requestForOpusRecordSettingsWithContext:
requestForSpeexRecordSettingsWithContext:
recordContext
requiresHistoricalBuffer
setRequiresHistoricalBuffer:
useCustomizedRecordSettings
sampleRate
lpcmBitDepth
lpcmIsFloat
numberOfChannels
encoderBitRate
isSiri
setIsSiri:
_requiresHistoricalBuffer
_useCustomizedRecordSettings
_lpcmIsFloat
_isSiri
_lpcmBitDepth
_numberOfChannels
_encoderBitRate
_recordContext
createDESRecordWithSuperVector:withMetaInfo:
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerPacket
inputRecordingFramesPerPacket
inputRecordingBytesPerFrame
lpcmInt16NarrowBandASBD
lpcmFloatASBD
opusASBD
opusNarrowBandASBD
speexASBD
lpcmInterleavedWithRemoteVADASBD
lpcmMonoInterleavedWithRemoteVADASBD
lpcmNonInterleavedWithRemoteVADASBD
lpcmMonoNonInterleavedWithRemoteVADASBD
lpcmASBD
lpcmNarrowBandASBD
aiffFileASBD
createVoiceScorersWithVoiceProfiles:withConfigFile:withResourceFile:withOffsetsType:
initWithProfileID:withModelFile:withConfigFile:withResourceFile:withOffsetsType:
resetScorerWithModelFilePath:
analyzeSpeakerVector:withDimensions:withThresholdType:
scoreSpeakerVector:withDimensions:withThresholdType:
analyzeSuperVector:withDimensions:withThresholdType:
updateSAT
getSATVectorCount
getSpeakerVectorAtIndex:
deleteVectorAtIndex:
profileID
sysConfigRoot
psrConfigFilePath
psrConfigRoot
satModelAvailable
_satModelAvailable
_profileID
_sysConfigRoot
_psrConfigFilePath
_psrConfigRoot
initWithLocaleIdentifier:withAudioSession:withAppDomain:
setLocaleIdentifier:
initNewVoiceProfileWithLocale:withAppDomain:
createKeywordDetector
initWithAsset:
initWithLocale:
prepareRecord
isRecording
releaseAudioSession
enter
_stopAudioSession
leave
destroySpeakerTrainer
waitWithTimeout:
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
addObject:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
audioSource
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
preheat
resetForNewRequestWithSampleRate:recordContext:recordSettings:
hasCorrectAudioRoute
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
sharedTrainer
addUtterance:toProfile:withAsset:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
processAudioSamplesAsynchronously:
audioSessionRecordBufferAvailable:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
didDetectBeginOfSpeech
didDetectEndOfSpeech:
trainingManagerWithLocaleID:withAppDomain:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
voiceProfile
prepareWithCompletion:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
setRms:
speechRecognizerAvailable
audioFileWriter
setAudioFileWriter:
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_currentAsset
_profile
_didStopWaitingGroup
_speechRecognizerAvailable
_rms
_audioFileWriter
voiceRetrainersWithContext:
initWithDescription:
_description
stringWithUTF8String:
initWithUUIDString:
initWithRoute:isRemoteDevice:remoteDeviceUID:remoteDeviceProductIdentifier:
initWithFormat:
encodeObject:forKey:
decodeObjectOfClass:forKey:
recordRoute
isRemoteDevice
remoteDeviceUID
remoteProductIdentifier
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
initWithAVVCRecordDeviceInfo:
route
remoteDeviceProductIdentifier
_isRemoteDevice
_route
_remoteDeviceUID
_remoteDeviceProductIdentifier
_audioRecorder
initWithQueue:error:
contextForVoiceTriggerTraining
setContext:error:
registerObserver:
activateAudioSessionWithReason:streamHandleId:error:
isRecordingWithStreamHandleId:
prepareAudioStreamRecord:streamHandleId:error:
startAudioStreamWithOption:streamHandleId:error:
stopAudioStreamWithStreamHandleId:error:
deactivateAudioSession:error:
unregisterObserver:
_hasCorrectInputAudioRoute
_hasCorrectOutputAudioRoute
recordRouteWithStreamHandleId:
playbackRoute
convertStopReason:
_handleDidStopWithReason:
audioRecorderBufferAvailable:audioStreamHandleId:buffer:remoteVAD:atTime:
audioRecorderBufferAvailable:audioStreamHandleId:buffer:
audioRecorderDidStartRecord:audioStreamHandleId:successfully:error:
audioRecorderDidStopRecord:audioStreamHandleId:reason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderDidFinishAlertPlayback:ofType:error:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
audioRecorder:willSetAudioSessionActive:
audioRecorder:didSetAudioSessionActive:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
audioRecorderWillBeDestroyed:
audioRecorderBuiltInAudioStreamInvalidated:error:
audioRecorderStreamHandleIdInvalidated:
setEndpointerDelegate:
resetEndPointer
hasAudioRoute
updateMeters
averagePower
powerMeter
setPowerMeter:
audioStreamHandleId
setAudioStreamHandleId:
_powerMeter
_audioStreamHandleId
appDomain
siriProfileId
locale
userName
dateAdded
profilePitch
initWithVoiceProfile:
initWithSharedSiriId:languageCode:productCategory:version:
setAppDomain:
profileId
setProfileId:
languageCode
setLanguageCode:
productCategory
setProductCategory:
version
setVersion:
setDateAdded:
pitch
setPitch:
sharedSiriId
setSharedSiriId:
homeId
setHomeId:
setUserName:
_appDomain
_profileId
_languageCode
_productCategory
_version
_dateAdded
_pitch
_sharedSiriId
_homeId
_userName
initWithLocale:asset:
computeTriggerConfidenceForAudio:withCompletion:
removeObject:
satVTImplicitThreshold
resourcePath
keywordDetectorNDAPIConfigFilePath
supportPremiumModel
VTSecondPassConfigPathRecognizerExistFrom:
keywordDetectorQuasarConfigFilePath
initWithLocale:configPath:resourcePath:
VTSecondPassRecognizerScoreScaleFactorFrom:
dataWithBytes:length:
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
bestScore
filterVTAudioFiles:withLocale:withAsset:
detectorNDAPI
setDetectorNDAPI:
detectorQuasar
setDetectorQuasar:
recognizerScoreScaleFactor
setRecognizerScoreScaleFactor:
_recognizerScoreScaleFactor
_detectorNDAPI
_detectorQuasar
waitingForConnection:error:
isConnected
getLocalUrl
string
_compatibilityVersion
stringValue
appendString:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
objectForKey:
state
isPremium
getCSAssetOfType:
isDownloading
isCSAssetInstalled
canBePurged
isLatestCompareTo:
streamID
startHostTime
startAlert
stopAlert
stopOnErrorAlert
skipAlert
remoteVoiceActivityAvailable
remoteVoiceActivityVAD
remoteVoiceActivityVADBuffer
_voiceControllerWithError:
weakObjectsHashTable
_destroyVoiceController
_audioRecorderDidStartRecordingSuccessfully:streamHandleID:error:
setRecordDelegate:
initWithError:
setSynchronousCallbackEnabled:
setContextForStream:forStream:error:
_getRecordSettingsWithRequest:
_createDeInterleaverIfNeeded
inputRecordingBufferDuration
initWithStreamID:settings:bufferDuration:
setMeteringEnabled:
prepareRecordForStream:error:
_logResourceNotAvailableErrorIfNeeded:
_shouldInjectAudio
_needResetAudioInjectionIndex:
lpcmRecordSettings
_startAudioStreamForAudioInjection
avvcStartRecordSettingsWithAudioStreamHandleId:
startRecordForStream:error:
stopRecordForStream:error:
getCurrentSessionState
hasRemoteBuiltInMic
getCurrentStreamState:
getRecordDeviceInfoForStream:
recordSettingsWithStreamHandleId:
getRecordSettingsForStream:
isUpsamplingSourceAudio
activateAudioSessionForStream:isPrewarm:error:
domain
code
_shouldLogResourceNotAvailableError
submitAudioIssueReport:
activateAudioSessionForStream:isPrewarm:recordMode:error:
_convertDeactivateOption:
deactivateAudioSessionWithOptions:
setIAmTheAssistant:error:
setAllowMixableAudioWhileRecording:error:
enableSmartRoutingConsiderationForStream:enable:error:
setDuckOthersOption:
duckOthersOption
voiceTriggerEventInfo
_updateLanguageCodeForRemoteVTEIResult:
voiceTriggerInfo
getSiriLanguageWithFallback:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
updateMeterForStream:
getPeakPowerForStream:forChannel:
getAveragePowerForStream:forChannel:
streamDescription
_deinterleaveBufferIfNeeded:force:
_compensateChannelDataIfNeeded:receivedNumChannels:
_processAudioChain:audioStreamHandleId:remoteVAD:atTime:
opusDecoder
addPackets:audioStreamHandleId:remoteVAD:timestamp:receivedNumChannels:
initWithLength:
replaceBytesInRange:withBytes:
setAlertSoundFromURL:forType:
playAlertSoundForType:overrideMode:
playRecordStartingAlertAndResetEndpointer
alertStartTime
metrics
_processAudioBuffer:audioStreamHandleId:
_audioRecorderDidStopRecordingForReason:streamHandleID:
audioSessionEventProvidingWillSetAudioSessionActive:
audioSessionEventProvidingDidSetAudioSessionActive:
audioServerCrashEventProvidingLostMediaserverd
mutableBytes
shouldDeinterleaveAudioOnCS
numberWithUnsignedInt:
createSharedAudioSession
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerDidFinishAlertPlayback:withSettings:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerWirelessSplitterRouteAvailable:devices:
voiceControllerDidStartRecording:forStream:successfully:error:
voiceControllerDidStopRecording:forStream:forReason:
voiceControllerAudioCallback:forStream:buffer:
voiceControllerStreamInvalidated:forStream:
audioDecoderDidDecodePackets:audioStreamHandleId:buffer:remoteVAD:timestamp:receivedNumChannels:
setAudioServerCrashEventDelegate:
setAudioSessionEventDelegate:
willDestroy
setCurrentContext:streamHandleId:error:
isSessionCurrentlyActivated
recordDeviceInfoWithStreamHandleId:
recordingSampleRateWithStreamHandleId:
isNarrowBandWithStreamHandleId:
prewarmAudioSessionWithStreamHandleId:error:
setRecordMode:streamHandleId:error:
enableSmartRoutingConsiderationForStream:enable:
enableMiniDucking:
configureAlertBehavior:audioStreamHandleId:
_shouldUseRemoteRecordForContext:
_shouldUseRemoteBuiltInMic:
playRecordStartingAlertAndResetEndpointerFromStream:
playAlertSoundForType:
peakPowerForChannel:
averagePowerForChannel:
voiceControllerCreationQueue
setVoiceControllerCreationQueue:
observers
setObservers:
crashEventDelegate
setCrashEventDelegate:
sessionEventDelegate
setSessionEventDelegate:
_voiceController
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_remoteRecordClient
_latestContext
_shouldUseRemoteRecord
_opusDecoders
_audioFileReader
_audioFilePathIndex
_waitingForDidStart
_pendingTwoShotVTToken
_voiceControllerCreationQueue
_observers
_crashEventDelegate
_sessionEventDelegate
assetForAssetType:resourcePath:configVersion:assetProvider:
hybridEndpointerAssetFilename
contentsOfDirectoryAtPath:error:
firstObject
initWithResourcePath:configFile:configVersion:assetProvderType:
fallBackAssetResourcePath
_decodeJson:
getNumberForKey:category:default:
assetHashInResourcePath:
getConfigFileNameForAssetType:
defaultFallBackAssetForSmartSiriVolume
defaultFallBackAssetForHearst
defaultFallBackAssetForAdBlocker
getBoolForKey:category:default:
getStringForKey:category:default:
containsKey:category:
containsCategory:
hashFromResourcePath
isEqualAsset:
stringForCurrentAssetProviderType
assetProvider
_decodedInfo
_path
_resourcePath
_configVersion
_assetProvider
URLsForDirectory:inDomains:
lastObject
predicateWithFormat:
removeItemAtPath:
URLByAppendingPathComponent:
sharedManager
installedAssetOfType:forLanguage:
satConfigFileNameForCSSpIdType:forModelType:forAssetType:
readJsonFileAtPath:
setWithObjects:
containsObject:
deviceCategoryForDeviceProductType:
getVoiceProfileProductCategoryFromVersionFilePath:
deviceCategoryStringRepresentationForCategoryType:
dataWithContentsOfURL:
pathExtension
enumeratorAtPath:
getHomeUserIdForSharedUserId:completion:
userVoiceProfilesForAppDomain:
userVoiceProfilesForAppDomain:forLocale:
objectAtIndexedSubscript:
stringByAppendingString:
getExplicitEnrollmentUtterancesFromDirectory:
getImplicitEnrollmentUtterancesFromDirectory:
arrayByAddingObjectsFromArray:
_getUtterancesFromDirectory:
isUtteranceImplicitlyTrained:
getUtteranceEnrollmentType:
insertObject:atIndex:
addObjectsFromArray:
recordedTimeStampFromFileName:
compare:options:
recordedTimeStampOfFile:
moveItemAtPath:toPath:error:
addEntriesFromDictionary:
stringForCSSpIdType:
explicitSpIdTypeForSpId:
spIdTypeForString:
stringForSpeakerRecognizerType:
stringForVoiceProfileRetrainerType:
satConfigFileNameForCSSpIdType:
psrConfigFileNameForCSSpIdType:
spIdVoiceProfileImportRootDir
cleanupOrphanedVoiceIdGradingFiles
spidAudioTrainUtterancesDir
isSpeakerRecognitionSupportedInLocale:
getVoiceProfileIdentityFromVersionFilePath:
isCurrentDeviceCompatibleWithNewerVoiceProfileAt:
isCurrentDeviceCompatibleWithVoiceProfileAt:
getImplicitUtteranceCacheDirectory
getNumberOfAudioFilesInDirectory:
dumpFilesInDirectory:
getContentsOfDirectory:
getHomeUserIdForVoiceProfile:withCompletion:
getVoiceProfilesForSiriProfileId:
getVoiceProfileForSiriProfileId:forLanguageCode:
segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:
getEnrollmentUtterancesFromDirectory:
getExplicitMarkedEnrollmentUtterancesFromDirectory:
getEnrollmentUtterancesCountFromDirectory:withCountBlock:
moveContentsOfSrcDirectory:toDestDirectory:
encryptFileAt:andSaveTo:error:
getAssetProviderType
installedAssetOfType:forLanguageCode:
allInstalledAssetsOfType:forLanguage:
reloadForLocale:
CVTThreshold
VTSecondPassCategoryForFirstPassSource:
VTSecondPassPreTriggerAudioTimeFrom:
inputRecordingDurationInSecs
initWithNumChannels:recordingDuration:samplingRate:
bestStart
bestEnd
samplesFed
sampleCount
_sampleLengthFrom:To:
copySamplesFrom:to:
channelForProcessedInput
analyze:
triggeredUtterance:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_audioBuffer
array
removeObserver:
addObserver:
_checkAllConditionsEnabled
type
CSEventMonitorDidReceiveEvent:
setCallback:
addConditions:
subscribeEventMonitor:
isEnabled
notifyCallback:option:
notifyCallbackWithOption:
_monitors
_conditions
_callback
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
hasPendingTwoShotBeep
addUtterance:toProfile:
stringByAppendingFormat:
numberWithInt:
initWithEvent:locale:configVersion:
pushAnalyticsWithLazyBlock:
setVoiceProfilePruningFailureReasonCode:
setVoiceProfileUpdateScoreMSE:
setVoiceProfileDiscardedUtteranceCount:
setvoiceProfilePrunedUtteranceCount:
setVoiceProfileRetainedUtteranceCount:
appendVoiceProfileExplicitUtteranceScoreWith:
appendVoiceProfileImplicitUtteranceScoreWith:
appendVoiceProfileDiscardedImplicitUtteranceScoreWith:
appendVoiceProfileFailedExplicitUtteranceScoreWith:
setVoiceProfileRetrainingFailureReasonCode:
setRetrainingWaitTime:
voiceProfilePruningFailureReasonCode
voiceProfileUpdateScoreMSE
voiceProfileDiscardedUtteranceCount
voiceProfilePrunedUtteranceCount
setVoiceProfilePrunedUtteranceCount:
voiceProfileRetainedUtteranceCount
voiceProfileRetrainingFailureReasonCode
retrainingWaitTime
speakerRecognitionProcessingStatus
speakerRecognitionWaitTime
speakerRecognitionPSRProcessingStatus
setSpeakerRecognitionPSRProcessingStatus:
speakerRecognitionSATProcessingStatus
setSpeakerRecognitionSATProcessingStatus:
_eventString
_eventContext
explicitUtteranceIndex
explicitFailedUtteranceIndex
implicitUtteranceIndex
implicitDiscardedUtteranceIndex
_voiceProfileUpdateScoreMSE
_voiceProfilePruningFailureReasonCode
_voiceProfileDiscardedUtteranceCount
_voiceProfilePrunedUtteranceCount
_voiceProfileRetainedUtteranceCount
_voiceProfileRetrainingFailureReasonCode
_retrainingWaitTime
_speakerRecognitionProcessingStatus
_speakerRecognitionWaitTime
_speakerRecognitionPSRProcessingStatus
_speakerRecognitionSATProcessingStatus
isRecordContextVoiceTrigger:
isRecordContextJarvisVoiceTrigger:
isRecordContextHearstDoubleTap:
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextHomeButtonPress:
isRecordContextSpeakerIdTrainingTrigger:
isRecordContextHearstVoiceTrigger:
isRecordContextJarvisButtonPress:
recordContextString:
getHostClockFrequency
secondsToHostTime:
hostTimeToSeconds:
hostTimeToTimeInterval:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
macHostTimeFromBridgeHostTime:
_handleListenerEvent:
disconnect
_handleListenerError:
initWithDictionary:
_sendMessage:connection:completion:
_decodeError:
connect
notifyImplicitUtterance:audioDeviceType:audioRecordType:voiceTriggerEventInfo:otherCtxt:completion:
xpcConnection
setXpcConnection:
_xpcConnection
isMarkedSATEnrolled
getExplicitEnrollmentUtterancesForType:
reverseObjectEnumerator
provisionedVoiceProfilesForAppDomain:withLocale:
SSRSpeakerProfilesBasePath
_cleanupAppDomain:
SSRBasePathForAppDomain:
_cleanuplanguageCodePath:forAppDomain:
voiceProfileForId:
_cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:
_cleanupImplicitUtteranceCacheForProfile:
voiceProfileImplicitCacheDirPath
voiceProfileIdentity
voiceProfileVersion
voiceProfileBasePath
_cleanupContentsOfSatFolder:
voiceProfileAudioDirPathForSpidType:
_cleanupOrphanedMetafilesAtURL:
_cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:
attributesOfItemAtPath:error:
fileSize
_cleanupInvalidAudioFiles:
getImplicitEnrollmentUtterancesPriorTo:forType:
voiceProfileModelDirForSpidType:recognizerType:
_cleanupModelFilesAtDir:forAssetArray:
arrayWithCapacity:
filterDuplicatedSiriProfilesFrom:
filterInvalidSiriProfilesFrom:
cleanupProfileStore
cleanupInvalidModelsForProfile:withAssetArray:
doubleValue
dateWithTimeIntervalSince1970:
timeIntervalSince1970
dictionaryWithObjectsAndKeys:
dictionaryRepresentation
addUtterances:spIdType:
copyItemAtURL:toURL:error:
getEnrollmentUtterancesForModelType:
_voiceProfilePathForSpidType:
_getProfileVersionFilePath
_updateVoiceProfileVersionFile
_markSATEnrollmentWithMarker:
_isSATMarkedWithMarker:
createFileAtPath:contents:attributes:
setValue:forKey:
writeToFile:options:error:
setSharedSiriProfileId:
voiceProfileModelFilePathForRecognizerType:spIdType:
importVoiceProfileAtPath:
getExplicitMarkedEnrollmentUtterancesForType:
getImplicitEnrollmentUtterancesForType:
profileLocallyAvailable
deleteModelForSpidType:recognizerType:
markSATEnrollmentSuccess
markSATEnrollmentMigrated
isMarkedSATMigrated
pruningCookie
updatePruningCookie:
profileBasePath
setProfileBasePath:
setProfilePitch:
_siriProfileId
_profileBasePath
_profilePitch
initWithConfiguration:
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
resultsWithAddedAudio:numberOfSamples:taskName:
tokens
confidence
resultsWithEndedAudio
_syncRecognizer
initWithName:timeout:
invalidate
initWithStreamID:atStartHostTime:
avvcAlertBehavior
setStartAlert:
setStopAlert:
setStopOnErrorAlert:
setSkipAlert:
numberWithUnsignedLongLong:
_alertBehaviorTypeFromAVVCOberrideType:
_avvcAlertOverrideType:
avvcSettings
setAVVCAlertBehavior:
submitVoiceTriggerIssueReport:
submitEndpointerIssueReport:
submitTrialIssueReport:
componentsJoinedByString:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
setWordCount:
setTrailingSilenceDuration:
setEosLikelihood:
setPauseCounts:
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
sharedVoiceTriggerClient
applyGain:toBuffer:
convertToFloatLPCMBufFromShortLPCMBuf:
convertToShortLPCMBufFromFloatLPCMBuf:
applyNegative12dBGain:
lowercaseString
hasPrefix:
substringFromIndex:
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
processSampleCount:hostTime:
hostTimeFromSampleCount:
sampleCountFromHostTime:
anchorSampleCount
setAnchorSampleCount:
anchorHostTime
setAnchorHostTime:
_anchorSampleCount
_anchorHostTime
pruneVoiceProfile:forSpIdType:withAsset:
psrCombinationWeight
initWithConfigFilePath:withModelPath:withCompareModelFilePaths:
maxAllowedEnrollmentUtterances
initWithVoiceRetrainingContext:error:
compareVoiceProfileArray
setCompareVoiceProfileArray:
setVoiceProfile:
spIdType
resourceFilePath
filterToVoiceTriggerUtterances
forceRetrain
maxAllowedSpeakerVectors
modelsContext
asset
setAsset:
setLogAggregator:
_filterToVoiceTriggerUtterances
_forceRetrain
_combinationWeight
_compareVoiceProfileArray
_voiceProfile
_spIdType
_resourceFilePath
_maxAllowedSpeakerVectors
_modelsContext
_asset
_logAggregator
configFilePath
voiceProfileModelFilePath
compareModelFilePaths
_configFilePath
_voiceProfileModelFilePath
_compareModelFilePaths
setVoiceTriggerInfo:
rtsTriggerInfo
setRtsTriggerInfo:
triggerNotifiedMachTime
setTriggerNotifiedMachTime:
_voiceTriggerInfo
_rtsTriggerInfo
_triggerNotifiedMachTime
_getBaseMetaDictionaryForUtterancePath:
timeStampWithSaltGrain
_writeMetaDict:forUtterancePath:
dateFromString:
saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:
matchWithString:TrailingStr:LeadingStr:Pattern:
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
finishSpeechRecognitionTask
closeSessionWithStatus:successfully:
_firedEndPointTimeout
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
feedSpeechRecognitionWithPCMBuffer
_registerVoiceTriggerTimeout
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
handleAudioInput:
_reportStopListening
_registerEndPointTimeout
_registerForceEndPointTimeout
requestTriggeredUtterance:
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
sharedGrammars
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
setVoiceTriggerEventInfo:
_detectBOS
_ASRResultReceived
_reportedStopListening
_utteranceStored
_numSamplesFed
_bestTriggerSampleStart
_voiceTriggerEventInfo
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
removeAllObjects
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
numSamplesInPCMBuffer
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
setCurrentDeviceCategory:
discardSiriEnrollmentForProfileId:forLanguageCode:
_getVoiceProfilesForSiriProfileId:withLanguageCode:
deleteUserVoiceProfile:
updateVoiceProfile:withUserName:
_markVoiceProfileTrainingSyncForLanguage:
addUserVoiceProfile:withContext:withCompletion:
satImplicitTrainingEnabled
isIOSDeviceSupportingBargeIn
fileURLWithPathComponents:
unsignedLongLongValue
getLastBiometricMatchForVoiceTriggerTimeStamp:
defaultCenter
postNotificationName:object:
addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:
_CSSATDownloadPath
_getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:
_downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:
_enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:
_downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:
_getUserVoiceProfileDownloadCacheDirectoryForProfileId:forDeviceCategory:forVoiceProfileVersion:
checkIfVoiceProfile:needsUpdatedWith:withCategory:
_enableVoiceTriggerIfLanguageMatches:
_CSSATUploadPathForSiriProfileId:
_prepareVoiceProfileWithSiriProfileId:withUploadBlock:
_CSSATLegacyUploadPath
_getVoiceProfilePathsToBeUploadedForSiriProfileId:
copyItemAtPath:toPath:error:
_copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:
_copyVoiceProfileAtPath:toPath:
_isMarkedForVoiceProfileTrainingSyncForLanguage:
getDevicesWithAvailablePHSAssetsOnDeviceCheck:
devicesWithVoiceProfileIniCloudForLanguage:
getDevicesWithAvailablePHSAssetsForLanguage:completion:
userVoiceProfilesForLocale:
userVoiceProfileForVoiceProfileID:
migrateVoiceProfilesIfNeededWithCompletionBlock:
cleanupDuplicatedProfiles
cleanupVoiceProfileStore:
cleanupVoiceProfileModelFilesForLocale:
retrainVoiceProfile:withContext:withCompletion:
sharedStorePrefs
getVoiceProfileStoreVersion
_isLegacyEnrollmentMarkedWith:forLanguageCode:
_CSSATCachePath
getSATEnrollmentPath
modelDirectoryPathForProfile:
discardSiriEnrollmentForLanguageCode:
newVoiceProfileWithLocale:withAppDomain:
addUtterances:toProfile:withContext:withCompletion:
notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileDownloadReadyForUser:getData:completion:
notifyUserVoiceProfileUpdateReady
notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:
uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
notifyUserVoiceProfileUploadComplete
getCachedVoiceProfileAvailabilityMetaBlob
hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:
isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:
hasVoiceProfileIniCloudForLanguageCode:
enableVoiceTriggerUponVoiceProfileSyncForLanguage:
provisionedVoiceProfilesForLocale:
getVoiceProfileAnalyticsForAppDomain:withLocale:
triggerVoiceProfileMigrationWithCompletion:
triggerVoiceProfileDuplicatesCleanup
triggerVoiceProfileCleanupWithCompletion:
pruneImplicitUtterancesOfProfile:withAsset:
triggerRetrainingVoiceProfile:withContext:withCompletion:
markSATEnrollmentSuccessForVoiceProfile:
isSATEnrolledForSiriProfileId:forLanguageCode:
isSATEnrollmentMigratedForSiriProfileId:forLanguageCode:
deleteAllVoiceProfilesForAppDomain:
currentDeviceCategory
xpcClient
setXpcClient:
_currentDeviceCategory
_xpcClient
handleFailureInFunction:file:lineNumber:description:
addSamples:numSamples:atHostTime:
createAudioCircularBufferWithDefaultSettings
copySamplesFromHostTime:
copySamplesFrom:to:channelIdx:
copybufferFrom:to:
copyBufferWithNumSamplesCopiedIn:
bufferLength
setBufferLength:
.cxx_construct
_csAudioCircularBufferImpl
_bufferLength
numberWithUnsignedLong:
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastEndOfVoiceActivityTime
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
stopEndpointer
recordingStoppedForReason:
trailingSilenceDurationAtEndpoint
implDelegate
setImplDelegate:
canProcessCurrentRequest
activeChannel
setActiveChannel:
processServerEndpointFeatures:
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
logFeaturesWithEvent:locale:
handleVoiceTriggerWithActivationInfo:
endpointerModelVersion
elapsedTimeWithNoSpeech
initWithRecordType:deviceId:
setAlwaysUseRemoteBuiltInMic:
contextForServerInvoke
recordTypeFromAVVCActivationMode:
setType:
setDeviceId:
_createAVVCContextWithType:deviceId:
avvcActivationMode:
deviceId
isBuiltInVoiceTriggered
isHearstVoiceTriggered
isJarvisVoiceTriggered
isHearstDoubleTapTriggered
recordTypeString:
contextForHearstVoiceTriggerWithDeviceId:
contextForRemoraVoiceTriggerWithDeviceId:
contextForOpportuneSpeakerListener
contextForBuiltInVoiceTrigger
contextForJarvisWithDeviceId:
contextForBTLEWithDeviceId:
contextForHomeButton
defaultContext
initWithAVVCContext:
isVoiceTriggered
isTriggeredFromHearst
isRTSTriggered
isHomePressed
isServerInvoked
isStarkTriggered
isDictation
alwaysUseRemoteBuiltInMic
_alwaysUseRemoteBuiltInMic
_type
_deviceId
_latestVersionedAssetOfType:fromProviders:forLocale:
scannerWithString:
scanFloat:
_convertVersionStringToFloat:
assetProviders
setAssetProviders:
currentLanguageCode
setCurrentLanguageCode:
_assetProviders
_currentLanguageCode
speakerRecognitionController:hasSpeakerInfo:
speakerRecognitionFinishedProcessing:withFinalSpeakerInfo:
processAudio:withNumberOfSamples:
getLatestSpeakerInfo
orchestrator
setOrchestrator:
setLastScoreCard:
_orchestrator
_lastScoreCard
supportsSpeakerRecognitionAssets
_installedMobileAssetOfType:forLanguage:
_buildAssetQueryForAssetType:
returnTypes:
queryMetaDataSync
results
_filteredAssets:forLanguage:
queryParams
_getSSRAssetTypeString
_getSSRAssetCurrentCompatibilityVersion
_getVoiceTriggerAssetTypeString
_getVoiceTriggerAssetCurrentCompatibilityVersion
_getEndpointAssetTypeString
_getEndpointAssetCurrentCompatibilityVersion
initWithType:
addKeyValuePair:with:
_findLatestInstalledAsset:
valueForKey:
initStore
_loadVoiceProfiles
_updateTrainedUsersWithAction:UserVoiceProfile:
_deleteUserVoiceProfile:
_synchronizeSiriVoiceProfilesWithAssistant
_retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:
_logVoiceProfileConfusionWithCleanup:
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:
_getTopScoringProfileIdFromScores:
_retrainVoiceProfile:withContext:
getPitchForUtteranceAudioFiles:
_enrolledVoiceProfiles
loadKnownUserVoiceProfiles
_saveTrainedUsers:
setVoiceProfileStoreVersion:
saveKnownUserVoiceProfiles:
setWithArray:
minusSet:
allObjects
_retrainVoiceProfile:withContext:withUtterances:
logVoiceProfileConfusionWithCleanup:
_checkIfRetrainingRequiredForProfile:
copyAudioFiles:toProfile:forModelType:
voiceProfileArray
setVoiceProfileArray:
storePrefs
setStorePrefs:
_voiceProfileArray
_storePrefs
iterateBitset:block:
selectedChannelList
initWithInASBD:outASBD:
speexDecoder
_decoder
_inASBD
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
pickAssetForProfiles:forSpIdType:withAssetArray:
pickAssetForProfiles:forSpIdType:
composeModelContextsForProfiles:forSpIdType:forAsset:completion:
initWithConfigFilePath:withModelFilePaths:
_checkIfModelsPresentForProfiles:forSpIdType:forAsset:
initWithVoiceRecognitionContext:error:
setSpIdType:
expModelsContext
_osTransactionReqd
_activeChannel
_scoreType
_recognitionStyle
_vtEventInfo
_vadResourcePath
_expModelsContext
_maxAllowedAudioSamples
_debugUtteranceAudioFile
_debugUtteranceMetaFile
voiceProfilesModelFilePaths
_voiceProfilesModelFilePaths
createGrammars
bundleForClass:
bundlePath
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
_processAudioFileURL:
_getVoicingProbFromRawData:
_getPitchHzFromRawData:
supportRaiseToSpeak
supportHearstVoiceTrigger
supportPremiumWatchAssets
shouldRunVTOnCS
channelForOutputReference
supportTTS
supportJarvisVoiceTrigger
supportBluetoothDeviceVoiceTrigger
rootQueueWithFixedPriority:
supportHybridEndpointer
csAudioProcessingQueuePriority
characterSetWithCharactersInString:
componentsSeparatedByCharactersInSet:
supportContinuousVoiceTrigger
supportKeywordDetector
supportPremiumAssets
supportOpportunisticZLL
supportSelfTriggerSuppression
supportCSTwoShotDecision
supportSmartVolume
supportSAT
supportCompactPlus
supportAdBlocker
supportContinuousAudioFingerprint
supportPhatic
shouldDelayPhaticForMyriadDecision
supportSessionActivateDelay
supportLanguageDetector
shouldDownloadVTAssetsOnDaemon
supportLazySessionActivation
hasRemoteCoreSpeech
supportRemoraVoiceTrigger
supportCircularBuffer
supportBeepCanceller
supportZeroFilter
getFixedPrioritySerialQueueWithLabel:fixedPriority:
systemUpTime
deviceUserAssignedName
deviceHwRevision
supportHandsFree
supportsVoiceTriggerFides
getVoiceTriggerProfilesAESKey
generateIfNecessaryVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
hearstNumberOfBytesPerChunk
hearstNumberOfSamplesPerChunk
EncryptionAudioSampleByteDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
zeroFilterWindowSizeInMs
zeroFilterWindowSizeInMsForReport
zeroFilterApproxAbsSpeechThreshold
daysBeforeRemovingLogFiles
remoteVADDuration
serverLoggingChannelBitset
defaultContinousFingerprintBufferDuration
satScoreThreshold
containsSpeakerRecognitionCategory
multiUserLowScoreThreshold
multiUserHighScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
pruningExplicitUttThresholdSAT
pruningExplicitUttThresholdPSR
pruningThresholdSAT
pruningThresholdPSR
pruningNumRetentionUtterance
voiceProfilePruningCookie
_dispatchGroup
_dispatchGroupCounter
@24@0:8@16
@16@0:8
v24@0:8@16
f16@0:8
v20@0:8f16
Q16@0:8
v24@0:8Q16
I16@0:8
v20@0:8I16
v16@0:8
@"NSArray"
v32@0:8@16@24
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8r^v16Q24
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
B32@0:8r^v16q24
^{OpaqueExtAudioFile=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"NSURL"
v20@0:8B16
v24@0:8q16
q16@0:8
d16@0:8
d24@0:8d16
@24@0:8q16
B20@0:8B16
B28@0:8B16^{__CFString=}20
B24@0:8^{__CFString=}16
v36@0:8^f16Q24f32
v24@0:8@"EARClientSilenceFeatures"16
@32@0:8@16@24
v32@0:8@16Q24
@"SSRSpeakerRecognitionContext"
@"<SSRVoiceActivityDetectorDelegate>"
@"EARCaesuraSilencePosteriorGenerator"
@"_EAREndpointer"
@"CSServerEndpointFeatures"
@"NSObject<OS_dispatch_queue>"
B24@0:8d16
@20@0:8I16
@"NSObject<OS_dispatch_source>"
@"<CSAudioFileReaderDelegate>"
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@104@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56@96
@24@0:8Q16
@112@0:8@16@24{AudioStreamBasicDescription=dIIIIIIII}32{AudioStreamBasicDescription=dIIIIIIII}72
@40@0:8@16@24@32
v36@0:8@16@24f32
v40@0:8@16@24Q32
v40@0:8@16@24@?32
@48@0:8@16@24@32^@40
v32@0:8@"SSRSpeakerAnalyzerPSR"16@"NSDictionary"24
@32@0:8@"SSRSpeakerRecognitionContext"16@"<SSRSpeakerRecognizerDelegate>"24
v32@0:8@"NSData"16Q24
v24@0:8@"SSRSpeakerRecognitionContext"16
@"NSDictionary"16@0:8
@"NSString"
@"NSDictionary"
@"<SSRSpeakerRecognizerDelegate>"
@"SSRSpeakerAnalyzerPSR"
@72@0:8@16Q24Q32Q40Q48Q56@64
@28@0:8f16Q20
@40@0:8Q16Q24Q32
@32@0:8Q16Q24
v40@0:8Q16Q24@?32
@"NSData"
@20@0:8f16
v20@0:8i16
v32@0:8r^s16i24i28
v32@0:8r^f16i24i28
v28@0:8i16i20i24
v24@0:8^d16
@"CSAudioUnitMeterClipping"
@40@0:8@16@24Q32
@32@0:8@16Q24
v32@0:8@"<SSRSpeakerRecognizer>"16@"NSDictionary"24
v32@0:8@"SSRVoiceActivityDetector"16Q24
@40@0:8@16@24^@32
v28@0:8@16B24
@40@0:8@16@24d32
@"<SSRSpeakerRecognitionOrchestratorDelegate>"
@"<CSAudioFileWriter>"
@"<SSRSpeakerRecognizer>"
@"SSRVoiceActivityDetector"
@"NSObject<OS_os_transaction>"
v40@0:8@16@?24@?32
@24@0:8@?16
@24@0:8@"SSRVoiceProfileRetrainingContext"16
v40@0:8@"NSArray"16@?<@"NSError"@?@"NSURL"@"NSDictionary">24@?<v@?@"NSError"@"NSDictionary"@"NSDictionary">32
B24@0:8@"NSArray"16
@"NSError"24@0:8@?<B@?@"NSDictionary">16
@"NSURL"16@0:8
v24@0:8d16
@"CSAudioRecordContext"
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
{AudioStreamBasicDescription=dIIIIIIII}24@0:8f16I20
@48@0:8@16@24@32Q40
@56@0:8@16@24@32@40Q48
f40@0:8@16Q24Q32
B44@0:8@16@24@32B40
B44@0:8@"CSVTUITrainingSession"16@"NSData"24@"NSString"32B40
v28@0:8B16@20
v28@0:8B16@"NSError"20
v24@0:8@"NSData"16
v24@0:8@"NSError"16
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
v24@0:8@?16
q36@0:8q16B24@?28
B24@0:8q16
v32@0:8i16B20@?24
@"<CSVTUIAudioSession>"
@"CSNNVADEndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"NSMutableArray"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"CSAsset"
@"SSRVoiceProfile"
@"CSDispatchGroup"
@"<SSRVTUITrainingManagerDelegate>"
@"CSPlainAudioFileWriter"
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@44@0:8@16B24@28@36
@"NSUUID"
v56@0:8@16Q24@32@40Q48
v40@0:8@16Q24@32
v44@0:8@16Q24B32@36
v40@0:8@16Q24q32
v32@0:8@16q24
v40@0:8@16q24@32
v56@0:8@"CSAudioRecorder"16Q24@"NSData"32@"NSData"40Q48
v40@0:8@"CSAudioRecorder"16Q24@"CSAudioChunkForTV"32
v44@0:8@"CSAudioRecorder"16Q24B32@"NSError"36
v40@0:8@"CSAudioRecorder"16Q24q32
v32@0:8@"CSAudioRecorder"16q24
v40@0:8@"CSAudioRecorder"16q24@"NSError"32
v24@0:8@"CSAudioRecorder"16
v32@0:8@"CSAudioRecorder"16@"NSDictionary"24
v28@0:8@"CSAudioRecorder"16B24
v24@0:8@"NSDictionary"16
v32@0:8@"CSAudioRecorder"16@"NSError"24
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
q24@0:8q16
@"CSAudioRecorder"
@"<CSVTUIAudioSessionDelegate>"
@"CSAudioPowerMeter"
@48@0:8@16@24@32@40
@"NSNumber"
@"NSDate"
v32@0:8@16@?24
@"SSRTriggerPhraseDetectorNDAPI"
@"SSRTriggerPhraseDetectorQuasar"
B32@0:8d16^@24
@"<CSRemoteControlClientDelegate>"
Q24@0:8Q16
v36@0:8@16B24@28
v28@0:8@16i24
v36@0:8@16i24d28
v36@0:8@16i24@28
v40@0:8@16@24@32
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v36@0:8@"AVVoiceController"16i24@"NSError"28
v40@0:8@"AVVoiceController"16@"AVVCAlertInformation"24@"NSError"32
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v28@0:8B16@"NSArray"20
v44@0:8@"AVVoiceController"16Q24B32@"NSError"36
v40@0:8@"AVVoiceController"16Q24q32
v40@0:8@"AVVoiceController"16Q24@"AVVCAudioBuffer"32
v32@0:8@"AVVoiceController"16Q24
v60@0:8@16Q24@32@40Q48I56
v60@0:8@"CSAudioDecoder"16Q24@"NSData"32@"NSData"40Q48I56
v40@0:8@"CSAudioFileReader"16@"NSData"24Q32
v36@0:8@"CSAudioFileReader"16B24@"NSError"28
v32@0:8@"CSAudioFileReader"16q24
v24@0:8@"<CSAudioServerCrashEventProvidingDelegate>"16
v24@0:8@"<CSAudioSessionEventProvidingDelegate>"16
@32@0:8@16^@24
@24@0:8^@16
Q32@0:8@16^@24
B40@0:8@16Q24^@32
B32@0:8Q16^@24
B24@0:8Q16
f24@0:8Q16
B40@0:8q16Q24^@32
B40@0:8Q16Q24^@32
v28@0:8Q16B24
v48@0:8@16Q24@32Q40
@28@0:8@16I24
B32@0:8@16q24
v36@0:8B16Q20@28
v32@0:8q16Q24
@28@0:8@16B24
@"AVVoiceController"
^{OpaqueAudioConverter=}
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"NSMutableDictionary"
@"CSAudioFileReader"
@"NSHashTable"
@"<CSAudioServerCrashEventProvidingDelegate>"
@"<CSAudioSessionEventProvidingDelegate>"
@40@0:8Q16@24@32
@48@0:8Q16@24@32Q40
B36@0:8@16@24B32
B32@0:8@16@24
Q24@0:8@16
v80@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24Q64@?72
q24@0:8@16
v44@0:8@16@24B32@?36
B40@0:8@16@24^@32
@36@0:8@16@24f32
@32@0:8Q16@24
@"CSAsset"32@0:8Q16@"NSString"24
@"NSArray"32@0:8Q16@"NSString"24
Q24@0:8I16I20
@"CSAudioCircularBuffer"
v28@0:8B16Q20
B32@0:8@16^@24
B24@0:8^@16
@"<CSRemoteRecordClientDelegate>"
B40@0:8@16@24@32
Q20@0:8f16
d24@0:8Q16
Q40@0:8Q16Q24Q32
v64@0:8@16@24@32@40@48@?56
@"NSObject<OS_xpc_object>"
@32@0:8@16q24
@40@0:8@16Q24q32
B32@0:8Q16Q24
@"_EARSyncSpeechRecognizer"
@32@0:8@16d24
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v28@0:8f16@20
v32@0:8Q16Q24
@"SSRLoggingAggregator"
v64@0:8@16@24B32@36@44Q52B60
q48@0:8@16@24@32@40
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
v24@0:8i16B20
@96@0:8q16q24@32@40@48@56@64@72@80@?88
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
v48@0:8@16@24@32@?40
v72@0:8@16@24@32@40@48@56@?64
@32@0:8@16@?24
v48@0:8@16Q24@?32@?40
@40@0:8@16Q24Q32
@"CSVoiceIdXPCClient"
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@24@0:8^Q16
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
v40@0:8Q16@24@32
v32@0:8d16@?24
v40@0:8Q16@"NSDictionary"24@"NSDictionary"32
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
@"<CSEndpointAnalyzerImplDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerImplDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
v32@0:8@"NSString"16@"NSString"24
@"<CSEndpointAnalyzerDelegate>"
@"<CSEndpointAnalyzerImplDelegate>"
@32@0:8q16@24
f24@0:8@16
@"<SSRAssetManagerDelegate>"
v32@0:8@"SSRSpeakerRecognitionOrchestrator"16@"NSDictionary"24
@"<SSRSpeakerRecognitionControllerDelegate>"
@"SSRSpeakerRecognitionOrchestrator"
B40@0:8@16@24f32f36
B40@0:8@16@24Q32
v32@0:8Q16@24
v36@0:8@16B24@?28
@"SSRVoiceProfileStorePrefs"
@112@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64Q104
v52@0:8@16Q24@32Q40I48
@"<CSAudioDecoderDelegate>"
v36@0:8@16@24B32
B44@0:8@16@24Q32B40
B28@0:8@16B24
v48@0:8@16Q24@32@?40
B40@0:8@16Q24@32
@40@0:8@16Q24@32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@40@0:8@16q24@32
f20@0:8f16
@20@0:8i16
@28@0:8@16i24
@44@0:8Q16@24@32B40
S16@0:8
i16@0:8
q24@0:8Q16
@"NSObject<OS_dispatch_group>"
333333
333333
