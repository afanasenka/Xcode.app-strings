?F]k
 @333333
VUUUUU
?YAH
?ffffff
@<,_
@ffffff
Ww'&l
MbP?{
333333
?ffffff
?ffffff
?333333
333333
ffffff
333333
333333
@UUUUUU
?UUUUUU
z>UUUUUU
?333333
I@ffffff
?333333
0@hfffff
VUUUUU
?VUUUUU
?YAH
?YAH
pCh?
UUUUUU
UUUUUU
UUUUUU
?UUUUUU
zt?{
p?33
UUUUUU
?UUUUUU
4@333333
?333333
?ffffff
Mbp?
?333333
sU?gDi?
z?-C
Mb@?
UUU?
?~0d>
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-310.2.160
33333cs@
N6VRF_V111SessionImplE
N6VRF_V17SessionE
NSt3__114default_deleteIN6VRF_V111SessionImplEEE
NSt3__120__shared_ptr_pointerIPN6VRF_V111SessionImplENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
0Xr
?g|_\
12VRFTrackImpl
8VRFTrack
NSt3__114default_deleteI8VRFTrackEE
NSt3__120__shared_ptr_pointerIP8VRFTrackNS_14default_deleteIS1_EENS_9allocatorIS1_EEEE
N6VRF_V211SessionImplE
N6VRF_V27SessionE
NSt3__114default_deleteIN6VRF_V211SessionImplEEE
NSt3__120__shared_ptr_pointerIPN6VRF_V211SessionImplENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
#?6t>=
333333
?333333
?ffffff
14VRFSessionBase
10VRFSession
Fail: %{public}@
Trace: %{public}@
Setting face bounding boxes based on orientation (%@) and observations: %@
Setting bounding boxes based on orientation (%@) and observations: %@
Tried to set sceneLabel to unsupported value: %ld
class %@ is not the correct type, its superclass should be %@
Unable to serialize finalized composition: %{public}@
Continue: %{public}@
facerect yiq = %.5f, %.5f, %.5f
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
aperture=%@, shutterSpeed=%@, iso=%@
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
Choosing gray world instead of gray edge
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
NUVideoRotationExportRequest failed. Error:%@
[PICompositionExporter shouldTryVideoRotationFastPath] failed. Error:%@
failed to render auxiliary image data: %@
Unexpected Live Photo export format pairing. Video codec (%@) and image export format (%@)
Failed to prepare video metadata: %@
Failed to export image to data: %@
Failed to export image to %@: %@
invalid format version: %@
Requesting forced cleanup of Vision caches
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
isAVAssetEditable failed. error:%@
Writing long-exposure motion mask to %{public}@
Writing long-exposure image to %{public}@
waitUntilReadyForMoreData: waited for %0.1fms
High-resolution image registration failure : %@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Evaluating pipeline as HDR input
Using modern pipeline
Using JavaScript pipeline
Received error while finalizing: %@
Starting reframe auto calculator
Starting horizon auto calculator
Starting perspective auto calculator
Skipping perspective auto calculator due to reframing occurring
Finished perspective auto calculator: %@
Finished horizon auto calculator: %@
Finished reframe auto calculator: %@
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains keyframes: %@
Finalizer result contains reframed rect: (%f, %f, %f, %f) roll angle degrees: %f
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator comitted
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
Starting still reframe
Dominant subject: %@
Removing peripheral subject: %@
Adding wanted face: %@
Correcting for subject %@
Requesting forced clean up of Vision caches
Resetting reframed bounds (%@) to view bounds due to low confidence
Final reframe bounds are %@
Still reframe subjects: %@
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%i)
Depth effects not supported: %@
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
Invalid focus rect: {%g,%g,%g,%g}
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
PIVideoFrameRequest: not generating keyframes due to low reframing confidence: %f
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
s_hdrOffsetPos kernel is nil
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
inputImage
T@"CIImage",&,V_inputImage
-[PIPhotoEffectHDR outputImage]
inputBackgroundImage
CIAdditionCompositing
inputImage cannot be nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
s_hdrOffsetPosBlackAndWhite kernel is nil
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
inputDepthMap
T@"CIImage",&,V_inputDepthMap
inputThreshold
Tf,V_inputThreshold
-[PIPhotoEffect3DHDR outputImage]
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
isCandidateForReframe
TB,R
isCandidateForPerspective
isCandidateForHorizon
facts.candidateForVideo == true
facts.candidateForStill == true
state.hasAdjustments == true
state.reframingAllowed == false
state.pano == true
state.video == true AND state.livePhoto == true
state.deviceIsStationary == true
v16@?0@"NURuleSystem"8
state.backFacing == false
state.largeSubject == true
facts.isStitched == true AND facts.hasHorizonLine == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.validSubjects == true
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoDuration > $VideoDurationUpperBound
state.videoDuration < $VideoDurationLowerBound
state.video == true
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
hasHorizonLine
state.horizonLinePresent == true
state.largestSubjectArea <= $MinimumSubjectSize
state.largeSubjectFace == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.allSubjectsInsidePrimaryBounds == true
validSubjects
state.subjectCount >= $SubjectCountMinThreshold
state.buildingConfidence > $BuildingConfidenceThreshold
hasBuilding
state.buildingCount > 0
state.stitchConfidence < $StitchConfidenceThreshold
isStitched
state.stitched == true OR state.perfectlyStitched == true
addRule exception : %@
v24@?0@"NSString"8@"NSString"16
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
candidateForHorizon
candidateForPerspective
candidateForReframe
candidateForVideo
candidateForStill
intensity
Td,N
scene
Tq,R,N
sceneConfidence
Td,R,N
boundingBoxes
T@"NSArray",R,C,N
platedFood
genericLandscape
sunriseSunset
faceBoundingBoxes
sceneLabel
inputScale
CIEdgePreserveUpsampleFilter
inputLumaSigma
inputSpatialSigma
inputSmallImage
CIColorMatrix
inputBiasVector
inputAVector
inputBVector
inputGVector
inputRVector
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; float max_comp = max(im.r,max(im.g,im.b)); if (max_comp < 0.75 * hlg_scale) { float lerp_t = max_comp * 1.333333333 * hlg_scale; im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = retHDR; } return im; }
mutablePoints
T@"NSMutableSet",R,C,N,V_mutablePoints
saliencyScale
Tf,R,N,V_saliencyScale
salientObject
TB,R,N,GisSalientObject,V_salientObject
body
T@"PIReframeSubject",R,N,V_body
direction
TQ,R,N,V_direction
centerPoint
T{CGPoint=dd},R,N,V_centerPoint
boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
points
T@"NSSet",R,C,N
expandedSubject
T@"PIReframeSubject",R,N
<%@ %p centerPoint=(%f, %f) points=%@>
smartTone
smartColor
smartBlackAndWhite
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
videoStabilize
videoCrossfadeLoop
semanticEnhance
composition
T@"NUComposition",R,C,N
changeDelegate
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
mediaType
Tq,N,V_mediaType
imageOrientation
Tq,N,V_imageOrientation
sourceSelection
Tq,N
v16@?0@"PISmartToneAdjustmentController"8
PICompositionController(0x%X): %@
primary
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
schemaRevision
appVersion
buildNumber
CFBundleVersion
Invalid parameter not satisfying: %@
PICompositionSerializer.m
+[PICompositionSerializer _sanitizeComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Adjustment for %@ is identity
PICompositionSerializer
exception
dictionary
v32@?0@"NSString"8#16^B24
Value for key %@ has type %@; expected type %@
PICompositionSerializerDomain
Missing required key: %@
masterWidth
masterHeight
adjustments
metadata
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
data
T@"NSData",&,N,V_data
formatIdentifier
T@"NSString",&,N,V_formatIdentifier
formatVersion
T@"NSString",&,N,V_formatVersion
versionInfo
Serialization map does not contain identifierName
auto
current
omitIfDisabled
Serialization map has no entry for %@
Missing identifierName
PICompositionSerializer-geometry
width
Tq,N,V_width
height
Tq,N,V_height
orientation
Tq,N,V_orientation
Orientation
nil identifierName
Configuration does not support CTM adjustments.
Configuration does not support Aperature adjustments.
Unsupported adjustment: 
Missing definition in conversion map for the adjustment key: 
dummy
file://dummy.jpg
denistyThreshold
TQ,N,V_denistyThreshold
neighborhoodOffset
Tq,N,V_neighborhoodOffset
saliencyThreshold
Td,N,V_saliencyThreshold
saliencyDeltaThreshold
Td,N,V_saliencyDeltaThreshold
initialSaliencyValue
Td,N,V_initialSaliencyValue
useRegressedSaliencyBoxes
TB,N,V_useRegressedSaliencyBoxes
unionIOUThreshold
Td,N,V_unionIOUThreshold
clusterToRegressedRatio
Td,N,V_clusterToRegressedRatio
regressedConfidenceThreshold
Td,N,V_regressedConfidenceThreshold
highSaliencyThreshold
Td,N,V_highSaliencyThreshold
interSalientObjectDistance
Tq,N,V_interSalientObjectDistance
maxSalientObjectsCount
TQ,N,V_maxSalientObjectsCount
salientClusterConvergenceMaxDistance
Td,N,V_salientClusterConvergenceMaxDistance
defaultConfiguration
T@"PIExpandedSubjectCalculatorConfiguration",R,N
saliencyRevisionOneConfiguration
regressedSalientSubjects
T@"NSArray",R,C,N,V_regressedSalientSubjects
TQ,R,N
configuration
T@"PIExpandedSubjectCalculatorConfiguration",R,N,V_configuration
detectedSubjects
T@"NSArray",R,C,N,V_detectedSubjects
saliencyData
T@"NSArray",R,C,N,V_saliencyData
expandedSubjects
T@"NSArray",R,C,N,V_expandedSubjects
subjectDirection
TQ,N,V_subjectDirection
saliencyImageObservation
T@"VNSaliencyImageObservation",R,N,V_saliencyImageObservation
/var/tmp/saliency-%f.jpeg
saveSaliencyHeatmap
inputWhiteDst
inputWhiteSrc
inputHilightDst
inputHilightSrc
inputMidDst
inputMidSrc
inputShadowDst
inputShadowSrc
inputBlackDst
inputBlackSrc
T@"CIImage",&,N,V_inputImage
inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackSrcRGB
inputBlackDstRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
inputShadowDstRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
inputMidDstRGB
T@"NSNumber",&,N,V_inputMidDstRGB
inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
inputHilightDstRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
inputWhiteDstRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackSrcRed
inputBlackDstRed
T@"NSNumber",&,N,V_inputBlackDstRed
inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowSrcRed
inputShadowDstRed
T@"NSNumber",&,N,V_inputShadowDstRed
inputMidSrcRed
T@"NSNumber",&,N,V_inputMidSrcRed
inputMidDstRed
T@"NSNumber",&,N,V_inputMidDstRed
inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightSrcRed
inputHilightDstRed
T@"NSNumber",&,N,V_inputHilightDstRed
inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
inputWhiteDstRed
T@"NSNumber",&,N,V_inputWhiteDstRed
inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackSrcGreen
inputBlackDstGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
inputShadowDstGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
inputMidDstGreen
T@"NSNumber",&,N,V_inputMidDstGreen
inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
inputHilightDstGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
inputWhiteDstGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackSrcBlue
inputBlackDstBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
inputShadowDstBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
inputMidDstBlue
T@"NSNumber",&,N,V_inputMidDstBlue
inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
inputHilightDstBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
inputWhiteDstBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
inputColorSpace
T@"NSString",&,N,V_inputColorSpace
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
Failed converting data to RGBAh: %ld
Blue
Green
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
getTrackDominance
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Reframer/Internal/VRFSessionV1.mm
getViewRect
reframeSegment
getCamerawork
reframeKeyframe
sceneSegment
expandSubjectsInKeyframe
subjectExpander
Keyframes - Postprocessed
keyframe: time: %.4f compositionScore: %.4f outOfViewScore: %.4f
Global Keyframes - NLMS
Global Keyframes - Consolidated
Global Keyframes - Deadzone
getCompositionAnchor
getCompositionScore
Reframe Segments - Final
segment [ %.4f : %.4f ] reframe = %s {
  subjects: { 
%ld 
Reframe Segments - Coalesced
propagateReframeSegments
Reframe Segments - Propogated
Reframe Segments - Subdivided
createSceneSegments
cameraMotionMarkers.size()
subjectGroupMarkers.size()
Scene Segments
segment [ %.4f : %.4f ] {
  avgCameraMotion: <%.4f, %.4f>
camera motion segment marker: %.4f avgCameraMotion: <%.4f, %.4f>
subject group segment marker: %.4f 
subjects: { 
/tmp/CameraMotionSamples.txt
%.4f, %.4f, %.4f
/tmp/CameraMotionSamplesSmoothed.txt
/tmp/CameraMotionBins.txt
%.4f, %u
/tmp/CameraMotionBinsSmoothed.txt
scalePolicy
T@"<NUScalePolicy>",&,N,V_scalePolicy
stillReframeRequest
T@"PIStillReframeRequest",R,N
reframeResult
T@"PIReframeResult",&,N,V_reframeResult
bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
confidence
Td,N,V_confidence
saliencyObservation
T@"VNSaliencyImageObservation",&,N,V_saliencyObservation
ANODSubjects
T@"NSArray",C,N,V_ANODSubjects
statistics
T@"<NURenderStatistics>",R
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
T@"VNSaliencyImageObservation",R,N
-[PIStillReframeJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Reframer/PIStillReframeRequest.m
ReframeEvaluation.DBG
forceStillReframeResult
PIStillReframeRequest-imageProperties
Invalid parameter not satisfying: %s
-[PIStillReframeJob initWithRequest:]
Initializer not available: -[%@ %@], use designated initializer instead.
 Scale Policy: %@
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
s_falseColorHDRDebugKernelSource kernel is nil
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
inputCutoff
Td,V_inputCutoff
-[PIFalseColorHDRDebug outputImage]
destinationUTI
T@"NSString",R,V_destinationUTI
destinationLongExposureURL
T@"NSURL",R,V_destinationLongExposureURL
destinationMaskURL
T@"NSURL",R,V_destinationMaskURL
outputColorSpace
T@"NUColorSpace",R
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
-[PIAutoLoopExportRequest initWithRequest:]
flavor
Tq,N,V_flavor
recipe
T@"NSDictionary",C,N,V_recipe
cleanAperture
T{?={?=qq}{?=qq}},N,V_cleanAperture
PIZlibErrorDomain
compressionLevel
Ti,N,V_compressionLevel
strategy
Ti,N,V_strategy
windowBits
Ti,N,V_windowBits
memoryLevel
Ti,N,V_memoryLevel
chunkSize
TQ,N,V_chunkSize
-[PIZlibDataCompressionOptions setCompressionStrategy:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Util/PIZLibDataCompression.m
unknown strategy %d
-[PIZlibDataCompressionOptions setCompressionLevel:]
unknown level %d
createBuffer
T@?,C,N,V_createBuffer
growData
T@?,C,N,V_growData
decompressAllAtOnce
TB,N,V_decompressAllAtOnce
growData != nil
-[PIZlibDataDecompressionOptions setGrowData:]
createBuffer != nil
-[PIZlibDataDecompressionOptions setCreateBuffer:]
v24@?0@"NSData"8@"NSMutableData"16
@"NSMutableData"16@?0@"NSData"8
+[PIZlibDataCompression decompressData:options:error:]
1.2.11
English Error String - Not Localized
%@ %@
zlib-error: 
%@ %@ %s
Z_ERRNO
Z_STREAM_ERROR
Z_DATA_ERROR
Z_MEM_ERROR
Z_BUF_ERROR
Z_VERSION_ERROR
unknown error
should be < 4GB, so casts from NSUInteger to uInt below will not be invalid
+[PIZlibDataCompression compressData:options:error:]
non-instantiable class, use the class methods!
T@"NSDictionary",C,N
T@"NSString",C,N
stabilizedCropRect
Height
Width
NormStabilizeInstructions
startTime
T{?=qiIq},R,N,V_startTime
loopTimeRange
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
crossfadeDuration
T{?=qiIq},R,N,V_crossfadeDuration
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
secondary
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
No input video track found
[[AVMutableComposition alloc] init] failed.
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
CIDissolveTransition
Missing secondary video frame
video
Missing primary video frame
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
keyFrameTime
T{?=qiIq},N
scale
radius
falloff
intensityKey
T@"NSString",R,N
radiusKey
falloffKey
settings
T@"NSDictionary",R,N
enabled
TB,N
identifier
T@"NUIdentifier",&,N,V_identifier
displayName
adjustment
T@"NUAdjustment",R,N,V_adjustment
inputKeys
T@"NSArray",R,N
displayInputKeys
canBeEnabled
TB,R,N
PIAdjustmentController(0x%X): %@
-[PIAdjustmentController setIsAuto:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController inputKeys]
Adjustment empty
-[PIAdjustmentController displayName]
PIReframeAutoCalculatorConfidenceKey
PIReframeAutoCalculatorXOriginKey
PIReframeAutoCalculatorYOriginKey
PIReframeAutoCalculatorWidthKey
PIReframeAutoCalculatorHeightKey
PIReframeAutoCalculatorSaliencyObservationKey
PIReframeAutoCalculatorANODSubjectsKey
PIReframeAutoCalculatorKeyframesKey
PIReframeAutoCalculatorStabCropRectKey
Unknown media type
PIFaceObservationCache
PIFaceObservationCache-newRequest
spatialOvercaptureFused
spatialOvercapture
time
T{?=qiIq},R,N,V_time
homography
T{?=[3]},R,N,V_homography
dictionaryRepresentation
T@"NSDictionary",R,C,N
timeScale
timeValue
<%@:%p time:%@>
count
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
pre-AutoLoop
PILongExposureFusionAutoCalculator-videoProperties
kind
version
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
temperature
Td,N,V_temperature
tint
Td,N,V_tint
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
all(minsize <= maxr.wh)
CRect CRectClamp(CRect, Float2, const CRect &)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Reframer/Internal/CRect.mm
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
videoSource
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
-[PIAutoLoopAnalysisJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
AutoLoop is not supported
-[PIAutoLoopAnalysisJob prepare:]
unable to find video source node
progressHandler
T@?,C,N,V_progressHandler
rangeMin
Td,N,V_rangeMin
rangeMax
Td,N,V_rangeMax
shouldCancelHandler
T@?,C,N,V_shouldCancelHandler
keyframes
T@"NSArray",R,C,N,V_keyframes
stabCropRect
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
analysisType
TQ,R,N,V_analysisType
rawHomographies
T@"NSDictionary",R,N,V_rawHomographies
T{?={?=qq}{?=qq}},R,N
allowedAnalysisTypes
TQ,N,V_allowedAnalysisTypes
allowedCropFraction
Td,N,V_allowedCropFraction
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
unexpected homography
PIVideoStabilizeRequest.m
frameInstructions
Failure in ICCalcCinematicL1Corrections
Stabilize request was cancelled
No available analysis types were allowed
Failure in ICAnalyzeInputMotion
Failure in ICSynthesizeAnalysis
neutralGray
faceBalance
tempTint
rawState
Tq,R,V_rawState
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIRAWFaceBalanceAutoCalculator.responseQueue
WarmFace
WarmTint
WarmTemp
Strength
Warmth
OrigQ
OrigI
Failure in rendering image
PIWhiteBalanceAutoCalculator-face-balance
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
/Master/Source
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
PIFaceBalanceAutoCalculator-calculate
CIFaceBalance
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
PIWhiteColorCalculator-grayEdges
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-computeGreenPercentage
CIAreaAverage
v16@?0@"<NUBuffer>"8
color
PIWhiteBalanceAutoCalculator
-[PIWhiteBalanceAutoCalculator submit:]
PIFaceBalanceAutoCalculator.responseQueue
grayColor
PIWhiteBalanceAutoCalculator-imageProperties
pi_grayColorResultValue
T{?={?=[4d]}{?=[4d]}d},R
RGBResultValue
T{?=[4d]},R
{?=[4d]}
{?={?=[4d]}{?=[4d]}d}
Create
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/Internal/VRFSession.mm
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
inputBlurImage
T@"CIImage",&,V_inputBlurImage
inputIntensity
T@"NSNumber",&,V_inputIntensity
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_bounds
Td,R,N,V_confidence
strengthKey
neutralKey
toneKey
grainKey
hueKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
strength
neutral
tone
grain
request
T@"NUImageExportRequest",&,V_request
inputSize
T{?=qq},V_inputSize
geometry
T@"NUImageGeometry",&,V_geometry
auxiliaryImages
T@"NSDictionary",&,V_auxiliaryImages
properties
T@"NSDictionary",C,V_properties
T{?=qq},D
companionImageData
T@"NSData",&,V_companionImageData
companionVideoURL
T@"NSURL",&,V_companionVideoURL
T@"NSData",&,V_data
priority
T@"NUPriority",&,V_priority
colorSpace
T@"NUColorSpace",&,V_colorSpace
pairingIdentifier
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
<%@:%p - priority: %@, color space: %@, scale policy: %@>
metadataProcessor
T@?,C,V_metadataProcessor
increaseBitRateIfNecessary
TB,N,V_increaseBitRateIfNecessary
videoCodecType
T@"NSString",C,N,V_videoCodecType
preserveSourceColorSpace
TB,N,V_preserveSourceColorSpace
bypassOutputSettingsIfNoComposition
TB,N,V_bypassOutputSettingsIfNoComposition
applyVideoOrientationAsMetadata
TB,N,V_applyVideoOrientationAsMetadata
requireHardwareEncoder
TB,N,V_requireHardwareEncoder
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
imageExportFormat
T@"NUImageExportFormat",C,V_imageExportFormat
JPEGCompressionQuality
Td,V_JPEGCompressionQuality
optimizeForSharing
TB,V_optimizeForSharing
applyImageOrientationAsMetadata
TB,V_applyImageOrientationAsMetadata
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
Export URL UTI (%@) does not match expected export format (%@)
primaryURL
T@"NSURL",&,V_primaryURL
videoComplementURL
T@"NSURL",&,V_videoComplementURL
videoPosterFrameURL
T@"NSURL",&,V_videoPosterFrameURL
renderCompanionResources
TB,V_renderCompanionResources
reframeCropAdjustment
T@"NUAdjustment",&,V_reframeCropAdjustment
reframeVideoAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
PICompositionExporter-video
yOrigin
xOrigin
roll
PICompositionExporter-shouldTryVideoRotationFastPath-geometry
reference
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
PICompositionExporter.imageProperties.transaction
PICompositionExporter-%@
PICompositionExporter-imageProperties
v32@?0@"NSString"8@"<NUAuxiliaryImage>"16^B24
PICompositionExporter-image
unable to prepare image properties
_companion
Unexpected image export format when attempting to export Live Photo
Unexpected video codec when attempting to export Live Photo
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
PICompositionExporter-videoProperties
-[PICompositionExporter init]
metadataConverter must be set
metadataConverter
T@"<PICompositionExporterMetadataConverter>",&
Regions
convertFromLabToRGB
convertFromRGBToLab
bilateralFinalize
bilateralAdd_9
bilateralAdd_8
bilateralAdd_7
bilateralAdd_6
bilateralAdd_5
bilateralAdd_4
bilateralAdd_3
bilateralAdd_2
bilateralAdd_1
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
inputPoints
T@"NSArray",&,V_inputPoints
inputWeights
T@"NSArray",&,V_inputWeights
inputEdgeDetail
T@"NSNumber",&,V_inputEdgeDetail
inputVersion
T@"NSNumber",&,V_inputVersion
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop11
bilateralLoop5
bilateralLoop2
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
inputBorder
T@"NSNumber",&,V_inputBorder
inputRadius
T@"NSNumber",&,V_inputRadius
-[PIBilateralFilter outputImage]
unable to allocate convolution table in bilateral filter
ridiculously large radius for bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
string
T@"NSString",R,W,N
majorVersion
TQ,R,N,V_majorVersion
minorVersion
TQ,R,N,V_minorVersion
subMinorVersion
TQ,R,N,V_subMinorVersion
platform
T@"NSString",R,C,N,V_platform
%lu.%lu%@%@
.%lu
amountKey
T@"CIImage",&,N,VinputImage
inputVibrancy
T@"NSNumber",&,N,VinputVibrancy
inputContrast
T@"NSNumber",&,N,VinputContrast
inputCast
T@"NSNumber",&,N,VinputCast
CIVibrance
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
satAutoValue
satPercentileG98
satPercentile98
satPercentile75
PISmartColorHDR-sat-histogram
T{?={?=qiIq}{?=qiIq}},N
crossfadeDurationValueKey
crossfadeDurationTimescaleKey
startTimeValueKey
startTimeTimescaleKey
loopTimeRangeStartValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeDurationTimescaleKey
timeRangeDurationTimescale
timeRangeDurationValue
timeRangeStartTimescale
timeRangeStartValue
startTimeTimescale
startTimeValue
crossfadeDurationTimescale
crossfadeDurationValue
portraitInfo
Tq,N,V_version
spillMatteAllowed
T@"NSNumber",C,N
@"NUAdjustment"16@?0@"NUAdjustment"8
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
v48@?0{CGRect={CGPoint=dd}{CGSize=dd}}8^{CGContext=}40
type
Tq,R,N,V_type
Tq,R,N,V_identifier
source
Tq,R,N,V_source
expandedBounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
edgeBleed
Tq,N,V_edgeBleed
isHuman
isAnimal
supportsSecureCoding
(%.4f, %.4f, %.4f, %.4f)
  edgeBleed=%@
maxY 
maxX 
minY 
minX 
  expandedBounds=%@
  bounds=%@
<%@ %p %@ %@ id=%lu conf=%.2f>
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
inputStrength
T@"NSNumber",&,N,VinputStrength
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
SalientAreaTooSmall
SalientHumansAndAnimalsTooLarge
SalientBoundingBoxMissing
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
debugLineDetectionImage
T@"CIImage",&,N,V_debugLineDetectionImage
maxAutoYaw
T@"NSNumber",C,V_maxAutoYaw
maxAutoPitch
T@"NSNumber",C,V_maxAutoPitch
maxAutoAngle
T@"NSNumber",C,V_maxAutoAngle
minimumPitchCorrection
Td,V_minimumPitchCorrection
minimumYawCorrection
Td,V_minimumYawCorrection
minimumAngleCorrection
Td,V_minimumAngleCorrection
minimumConfidence
Td,V_minimumConfidence
maxFaceSize
Td,V_maxFaceSize
minimumPitchCorrectionArea
Td,V_minimumPitchCorrectionArea
minimumYawCorrectionArea
Td,V_minimumYawCorrectionArea
disableOnPanos
TB,V_disableOnPanos
disableOnFrontFacingCameraImages
TB,V_disableOnFrontFacingCameraImages
shouldRunDetectorsIfNecessary
TB,V_shouldRunDetectorsIfNecessary
shouldRunBuildingCheck
TB,V_shouldRunBuildingCheck
minSalientArea
Td,N,V_minSalientArea
maxSalientSubjectArea
Td,N,V_maxSalientSubjectArea
angleSeedDegreesCCW
Td,V_angleSeedDegreesCCW
debugFilesEnabled
TB,V_debugFilesEnabled
debugFilesPrefix
T@"NSString",C,V_debugFilesPrefix
debugDiagnostics
T@"NSMutableDictionary",R,V_debugDiagnostics
faceObservationCache
T@"PIFaceObservationCache",&,N,V_faceObservationCache
T@"PIFaceObservationCache",&,N
-[PIPerspectiveAutoCalculator submitVerified:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
NUPixelSize NUPixelSizeMake(NSInteger, NSInteger)
PIPerspectiveAutoCalculator-getPrimaryOrientation
yawError
ciYawError
pitchError
ciPitchError
yawCorrectionAreaCoverage
pitchCorrectionAreaCoverage
debugImage
rollAngleInDegreesCW
yawExpandLeftDegrees
pitchExpandTopDegrees
passesMinimumCorrectionCheck
passesConfidenceCheck
submitVerified
CIAutoPerspective
saliencyHeatMap
(width >= 0) && (height >= 0)
NUGeometryPrimitives.h
rollLimit
yawLimit
pitchLimit
focalLength
canGenerateNewCropRect
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
minConfidence
Not supported by Core Image. Default: YES
supported
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
setToPrimary
preseedRoll
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
PIPerspectiveAutoCalculator-overcaptureImageProperties
No overcapture
-[PIPerspectiveAutoCalculator submit:]
passesBuildingCheck
passesFaceCheck
passesSaliencyCheck
passesImagePropertiesCheck
PIPerspectiveAutoCalculator-subjects
submit
PIPerspectiveAutoCalculator-saliency
-[PIPerspectiveAutoCalculator passesSaliencyCheck:]
faceArea
subjectArea
salientArea
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
isSelfieCam
aspectRatio
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
Apple
front camera
-[PIPerspectiveAutoCalculator passesFaceCheck:]
faceSize
PIPerspectiveAutoCalculator-faceCheck
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
PIPerspectiveAutoCalculator-debug
Edit
%@PerspectiveLineDetection-png.DBG
%@PerspectiveEvaluation-txt.DBG
yawError.underlyingError
pitchError.underlyingError
%@.underlyingError
%@.error
%@.%@
-[PIPerspectiveAutoCalculator initWithComposition:]
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
luminanceKey
luminance
Flash
ApertureValue
FNumber
ExposureTime
ShutterSpeedValue
ISOSpeedRatings
cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},N
constraintWidth
constraintHeight
angle
angleRadians
pitch
pitchRadians
yawRadians
autoCropped
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
cannot set empty crop rect
inputDecoderVersion
inputLight
offsetBlack
offsetBrightness
offsetContrast
offsetExposure
offsetHighlights
offsetLocalLight
offsetShadows
overcaptureStatistics
localAutoValue
inputLightKey
offsetBlackKey
offsetBrightnessKey
offsetContrastKey
offsetExposureKey
offsetHighlightsKey
offsetLocalLightKey
offsetShadowsKey
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
overcaptureStatisticsKey
sourceSelectionKey
inputLocalLight
inputLightMapHeight
inputLightMapWidth
inputLightMap
fuse_image_compute
blur_image_compute_3x3
blur_image_compute_5x5
blur_image_compute_7x7
ncc_coarse_compute
ncc_compute
warp_homography
rgba_to_luma
jointbilateralfilter
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
SkipShaderPrewarm
endScale
startScale
start
PIPhotoEditHelper.m
PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
PIGrainAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
PISharpenAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
PILevelsAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
PICropAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
PITrimAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
PISlomoAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
PIMuteAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
PIDepthAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
PISourceAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
allAdjustmentTypes
nonVisualAdjustmentTypes
/post-Geometry
/pre-Geometry
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
/ShowOriginalSource
VideoReframe
/pre-VideoReframe
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
v32@?0@"NSString"8@"NSString"16^B24
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
name != nil
+[PIPhotoEditHelper newImageRenderClientWithName:]
identifier != nil
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newAdjustmentWithName:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
+[PIPhotoEditHelper videoSourceWithURL:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
%@ %a
%@ (preview) %a
StudioV2
Light
ContourV2
Contour
Commercial
CIPortraitEffectStudioV2
CIPortraitEffectLight
CIPortraitEffectContourV2
CIPortraitEffectContour
CIPortraitEffectCommercial
StageWhite
StageV2
StageMonoV2
BlackoutMono
Black
CIPortraitEffectStageWhite
CIPortraitEffectStageV2
CIPortraitEffectStageMonoV2
CIPortraitEffectBlackoutMono
CIPortraitEffectBlack
3DVividWarm
3DVividCool
3DVivid
3DSilverplate
3DNoir
3DDramaticWarm
3DDramaticCool
3DDramatic
CIPhotoEffect3DVividWarm
CIPhotoEffect3DVividCool
CIPhotoEffect3DVivid
CIPhotoEffect3DSilverplate
CIPhotoEffect3DNoir
CIPhotoEffect3DDramaticWarm
CIPhotoEffect3DDramaticCool
CIPhotoEffect3DDramatic
Transfer
Tonal
Process
Noir
Mono
Instant
Fade
Chrome
CIPhotoEffectTransfer
CIPhotoEffectTonal
CIPhotoEffectProcess
CIPhotoEffectNoir
CIPhotoEffectMono
CIPhotoEffectInstant
CIPhotoEffectFade
CIPhotoEffectChrome
+[PIForwardFakeBoost kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
boost kernel is nil
kernel vec4 forwardBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
inputBoost
Td,V_inputBoost
-[PIForwardFakeBoost outputImage]
+[PIInverseFakeBoost kernel]_block_invoke
inverse boost kernel is nil
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, k);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > 0.8 ? k + 2.126286*(pos.r-.91803) : im.r;
im.g = pos.g > 0.8 ? k + 2.126286*(pos.g-.91803) : im.g;
im.b = pos.b > 0.8 ? k + 2.126286*(pos.b-.91803) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
-[PIInverseFakeBoost outputImage]
/tmp/VRFTrack_%ld_raw.txt
/tmp/VRFTrack_%ld.txt
%.4f, %.4f, %.4f, %.4f, %.4f 
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/Internal/BSpline.inl
extend
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/Internal/VRFTrack.mm
time < head || time > tail
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
autoValue
requiresEnabled
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
timeRange
portraitEffectFilterName
timescale
regions
redEyeCorrections
sensitivity
center
pointY
pointX
inputSpots
RKRedEyeOperation
detectedFaces
inputDetectedFaces
pressure
yLocation
xLocation
hasSource
sourceOffset
repairEdges
opacity
softness
mode
brushStroke
inputStrokes
RKRetouchOperation
corrections
RKSelectiveColorOperation
redCurvePoints
RGBCurvePoints
greenCurvePoints
blueCurvePoints
pointsR
pointsL
pointsG
pointsB
RKCurvesOperation
whiteSrcRed
whiteSrcRGB
whiteSrcGreen
whiteSrcBlue
whiteDstRed
whiteDstRGB
whiteDstGreen
whiteDstBlue
shadowSrcRed
shadowSrcRGB
shadowSrcGreen
shadowSrcBlue
shadowDstRed
shadowDstRGB
shadowDstGreen
shadowDstBlue
midSrcRed
midSrcRGB
midSrcGreen
midSrcBlue
midDstRed
midDstRGB
midDstGreen
midDstBlue
hilightSrcRed
hilightSrcRGB
hilightSrcGreen
hilightSrcBlue
hilightDstRed
hilightDstRGB
hilightDstGreen
hilightDstBlue
blackSrcRed
blackSrcRGB
blackSrcGreen
blackSrcBlue
blackDstRed
blackDstRGB
blackDstGreen
blackDstBlue
RKLevelsOperation
warm
face
inputMethodVersion
RKRawDecodeOperation
inputLNRAmount
inputCNRAmount
inputDetailAmount
detail
DGRAWReduceNoiseOperation
straightenAngle
effectIntensity
effectVersion
effectName
inputSharpness
inputFalloff
inputEdgeScale
edges
RKProSharpenOperation
edgeDetail
RKNoiseReductionOperation
DGDefinition2Operation
DGVignetteEffectOperation
seed
amount
offsetNeutralGamma
inputBlackAndWhite
offsetTone
offsetGrain
offsetStrength
lightMapAvg
lightMap
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
autoRedEyeCorrections
allCorrections
RedEye
ApertureRedEye
B16@?0@"NSDictionary"8
Retouch
Curves
Display P3
sRGB
Adobe RGB
Generic P3
Levels
RawNoiseReduction
CropStraighten
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
@"NSString"16@?0@"NSDictionary"8
Sharpen
NoiseReduction
Definition
Vignette
v32@?0@8@16^B24
inputTableImage
T@"CIImage",&,V_inputTableImage
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
_accumError
T@"NSError",&,V__accumError
isReadyForMoreData
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
destinationURL != nil
Failed to create CGImageRef
Failed to create CGImageDestinationRef
Failed to finalize image destination
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
v16@?0@"CIImage"8
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator _accumulate:error:]
B16@?0@"CIRenderDestination"8
failed to render maximum image
PILongExposureAccumulator-maximum
failed to render minimum image
PILongExposureAccumulator-minimum
CIBoxBlur
failed to render average image
PILongExposureAccumulator-average
frame != nil
-[PILongExposureAccumulator accumulate:error:]
failed to init accumulator
PILongExposureAccumulator-main
Accumulation was cancelled
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
observation
T@"VNImageHomographicAlignmentObservation",R,C
extent
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
guideExtent
T{?={?=qq}{?=qq}},N,V_guideExtent
stillImage
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
-[PILongExposureRegistrationJob render:]
Image registration failure (expected 1 observation)
Failed to render luma
PILongExposureRegistrationJob-reference
PILongExposureRegistrationJob-render
Failed to allocate intermediate pixel buffer
-[PILongExposureRegistrationJob prepare:]
Malformed AutoLoop recipe : crop
return Source(composition.source, {'skipOrientation':true});
/AutoLoop/LongExposure
/RAW/SushiLevel1
/Master%@
inputDestinationImage
T@"CIImage",&,N,V_inputDestinationImage
inputCorrectionInfo
T@"NSArray",&,N,V_inputCorrectionInfo
inputCameraModel
T@"NSString",&,N,V_inputCameraModel
CILanczosScaleTransform
CIRedEyeCorrection
posterFrameTime
dynamic
@"NSString"16@?0@"NUAdjustment"8
1.5.1
convexHull
overcaptureComputed
computed
T{?=qiIq},R,V_time
subjects
T@"NSArray",R,V_subjects
estimatedCenterMotion
T{CGVector=dd},R,V_estimatedCenterMotion
estimatedMotionBlur
T{CGVector=dd},R,V_estimatedMotionBlur
trajectoryHomography
T{?=[3]},R,V_trajectoryHomography
timedMetadataArray
T@"NSArray",R,N,VtimedMetadataArray
-[PIVideoReframeMetadataExtractor extractMetadata]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/PIVideoReframeMetadataExtractor.mm
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
type >= 0
Confidence
Source
DogBody
CatBody
HumanFace
HumanBody
Type
Subjects
Time
Invalid plist at path: %@
init is not a valid initializer
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
endTime
rate
inputExposureKey
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
inputRAWGamutMapMax
outputExposure
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PIApertureRedEye.mm
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
repairBuffer != nil
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
reddesty out of bounds
reddestx out of bounds
Buffer must be RGBA16 type for red eye repairs
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
inputOrigI
Td,N,V_inputOrigI
inputOrigQ
Td,N,V_inputOrigQ
Td,N,V_inputStrength
inputWarmth
Td,N,V_inputWarmth
facebalance
inputPointsR
T@"NSArray",&,V_inputPointsR
inputPointsG
T@"NSArray",&,V_inputPointsG
inputPointsB
T@"NSArray",&,V_inputPointsB
inputPointsL
T@"NSArray",&,V_inputPointsL
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
colorBalance
PPtogHDR
gHDRtoPP
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTemp
inputWarmTint
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
inputHasFace
T@"NSNumber",&,N,V_inputHasFace
inputIsRaw
T@"NSNumber",&,N,V_inputIsRaw
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
PILevelsFilter
PISelectiveColorFilter
PICurvesFilter
PICurvesLUTFilter
PIBilateralFilter
PINeutralGrayWhiteBalanceFilter
PIRAWFaceBalance
PITempTintFilter
PIColorBalanceFilter
PIFalseColorHDRDebug
CISmartTone
CISmartColor
CILocalLight
CIVignetteEffect
@"NSString"16@?0@"NSString"8
CISmartToneFilter
CISmartColorFilter
CISmartBlackAndWhite
CIPhotoGrain
CILocalLightMapPrepare
CILocalLightFilter
CILocalContrast
CIHighKey
CIProSharpenEdges
CIExposureAdjust
CIGammaAdjust
CIMix
alignment
forceGlassesMatteOff
TB,N,V_forceGlassesMatteOff
forceSpillMatteOff
TB,N,V_forceSpillMatteOff
allowSpillMatteOnOlderPortraitV2Captures
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
debugDecoratorFiltersEnabled
PIGlobalSettings
falseColorHDR
photoEditingSettings
PXSettingsArchiveKey
PURootSettings
editSettings
IPXRootSettings
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
B8@?0
PI_AUTOLOOP_EXPORT_USE_METAL
OvercaptureRectForAutoCrop
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PIPhotosPipeline.m
Unable to unwrap the input node!
input to VideoCrossfadeLoop cannot be nil
Missing duration for crossfade
Invalid data type for adjustmentValue
/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
/System/Library/Frameworks/JavaScriptCore.framework/Contents/MacOS/JavaScriptCore
void *JavaScriptCoreLibrary(void)
JSContext
Class getJSContextClass(void)_block_invoke
Unable to find class %s
JavaScriptCoreSoftLinking.h
input to VideoReframe cannot be nil
Invalid data type for stabCropRect
Invalid data type for keyframes
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
Couldn't find the specified Pipeline
PhotosPipeline
Couldn't find bundle for class %@
point
T{CGPoint=dd},R,N,V_point
value
Td,R,N,V_value
<%@ %p point=(%f, %f) value=%f>
redEyeSpots
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
pre-Adjustments
PIApertureRedEyeAutoCalculator-faceDetection
NSNumber
CIEdgePreserveUpsampleRGFilter
_lightMapImageFromData_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILocalLightHDR.m
x == 0
y == 0
width == lmWidth
height == lmHeight
PILocalLightHDR
inputSmartShadows
-[PILocalLightFilterHDR outputImage]
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
lightMapHeight
lightMapWidth
PILocalLightHDR-stats
T@"NSMutableData",&,Vdata
elementByteSize
TQ,R,VelementByteSize
rowElements
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
format
Ti,R,Vformat
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
v24@?0@"<NUBufferTile>"8^B16
v16@?0@"<NUMutableBuffer>"8
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
Produced invalid BlackAndWhite settings, using defaults
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
T@"NSNumber",C,N,VinputStrength
inputNeutralGamma
T@"NSNumber",C,N,VinputNeutralGamma
inputTone
T@"NSNumber",C,N,VinputTone
inputHue
T@"NSNumber",C,N,VinputHue
inputGrain
T@"NSNumber",C,N,VinputGrain
inputSeed
T@"NSNumber",C,N,VinputSeed
inputScaleFactor
T@"NSNumber",C,N,VinputScaleFactor
PIPhotoGrainHDR
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
There is no need to call smartBlackAndWhiteStatistics.
T@"NSArray",C,N
whiteDst
whiteSrc
hilightDst
hilightSrc
midDst
midSrc
shadowDst
shadowSrc
blackDst
blackSrc
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
This is an abstract method! Subclass '%@' should provide concrete implementation
-[PILevelsAutoCalculator submit:]
pre-Levels
PILevelsAutoCalculator-histogram
adjustmentConstants
T@"PIAdjustmentConstants",R,N
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
Adjustment controller for key %@ is of class: %@, but was expected to be %@
Td,R,N,V_x
Td,R,N,V_y
TB,R,N,GisEditable,V_editable
not editable
editable
<%@:%p> [(%.3f, %.3f), %s]
inputExposure
T@"NSNumber",&,N,VinputExposure
inputBrightness
T@"NSNumber",&,N,VinputBrightness
inputShadows
T@"NSNumber",&,N,VinputShadows
inputHighlights
T@"NSNumber",&,N,VinputHighlights
inputBlack
T@"NSNumber",&,N,VinputBlack
inputRawHighlights
T@"NSNumber",&,N,VinputRawHighlights
CIHighlightShadowAdjust
inputHighlightAmount
inputShadowAmount
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
whitePoint
blackPoint
highKey
tonalRange
PISmartToneFilterHDR-histogram
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
inputStillImage
T@"CIImage",&,N,V_inputStillImage
inputMaskImage
T@"CIImage",&,N,V_inputMaskImage
inputRenderScale
T@"NSNumber",&,N,V_inputRenderScale
inputVideoScale
T@"NSNumber",&,N,V_inputVideoScale
inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentExtent
inputAlignmentTransform
T@"CIVector",&,N,V_inputAlignmentTransform
long-exp-fusion-image.tiff
long-exp-refined-mask-image.tiff
long-exp-ncc-map-image.tiff
long-exp-guide-image.tiff
long-exp-still-image.tiff
long-exp-mask-image.tiff
long-exp-input-image.tiff
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
kBlendMaskThreshold1
kBlendMaskThreshold0
kNCCEdge1
kNCCEdge0
kNCCBlurHalfSize
@"NSString"8@?0
PI_LONG_EXPOSURE_FUSION_PARAMS
{?={?=qq}{?=qq}}
{?=qq}
Image
Master/RAW/Linear
PIInverseFakeBoost
post-Adjustments
inputCenter
post-Geometry
pre-Orientation
projectUsingEstimatedCleanAperture
com.apple.quicktime.live-photo.vitality-disabled
projectUsingOriginalSize
projectUsingCleanAperture
resetCleanAperture
perspectiveStraighten
pre-Crop
pre-Geometry
pre-VideoCrossfadeLoop
pre-VideoStabilize
pre-VideoReframe
filterVersion
CIPhotoEffect%@
saturation
hueShift
spread
blue
green
pre-Curves
inputSaturation
inputLightMapImage
inputGuideImage
inputTime
inputTargetImage
CIDynamicRender
CIDynamicFood
Unknown sceneLabel when rendering semantic enhance adjustment
sunsetOrSunrise
inputBoundingBoxArray
inputFaceBoxArray
inputConfidence
SloMo
pre-SloMo
pre-Trim
pre-LivePhotoKeyFrame
pre-Mute
pre-WB
masterSpace
inputGainMap
CIDepthEffectApplyBlurMap
inputShape
shape
inputLumaNoiseScale
lumaNoiseScale
PortraitV2
PortraitV2-zeroStrength
CIPortraitEffect%@
inputFullSizeImage
inputGenerateSpillMatte
CIPortraitEffectV2
inputTeethMask
inputHairMask
inputFaceMask
inputBlurMap
inputMatte
inputDisparity
inputFaceLandmarkArray
faceLandmarks
inputEV
inputPower
inputGlassesImage
inputHairImage
inputMatteImage
CIDepthEffectMakeBlurMap
inputOriginalSize
inputAuxDataMetadata
inputCalibrationData
inputShiftmapImage
__dominantInputSettingsKey
Missing required depth settings
chinY
chinX
noseY
noseX
rightEyeY
rightEyeX
leftEyeY
leftEyeX
inputChinPosition
inputFaceMidPoint
inputRightEyePosition
inputLeftEyePosition
inputFocusRect
inputAperture
faces
focusRect
MeteorPlusGainMap
GlassesSegmentationMatte
TeethSegmentationMatte
SkinSegmentationMatte
HairSegmentationMatte
PortraitEffectsMatte
Source does not contain depth
Disparity
Intermediate
keepCacheWhenAtOneToOne
PIForwardFakeBoost
ShowOriginalSource
RAW/Linear
unsupported sourceSelect adjustment
Master
skipOrientation
LongExposure
defaultFrameTime
hardCropCleanAperture
mediaComponentType
Video
inputNeutralTint
inputNeutralTemperature
inputNoiseReductionContrastAmount
contrast
inputNoiseReductionSharpnessAmount
sharpness
inputUILuminanceNoiseReductionAmount
inputUIColorNoiseReductionAmount
inputNoiseReductionDetailAmount
inputGamutMapMax
kCGImageSourceShouldExtendRaw
inputSushiLevel
composition != nil
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PIModernPhotosPipeline.m
pipelineState != nil
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
Mirror
T@"NUIdentifier",R
PhotosComposition
+[PISchema registerPhotosSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PISchema.m
failed to construct photos pipeline %@
enableModernPipeline
failed to register %@: %@
+[PISchema photosCompositionSchema]
Failed to register schema %@: %@
Composition
WhiteBalance~1.0
Vignette~1.0
VideoStabilize~1.0
VideoReframe~1.0
VideoPosterFrame~1.0
VideoCrossfadeLoop~1.0
Trim~1.0
SourceSelect~1.0
SmartTone~1.0
SmartColor~1.0
SmartBlackAndWhite~1.0
SlowMotion~1.0
Sharpen~1.0
SemanticEnhance~1.0
SelectiveColor~1.0
Retouch~1.0
com.apple.photo:Source~1.0
RedEye~1.0
RawNoiseReduction~1.0
RAW~1.0
PortraitEffect~1.0
Orientation~1.0
NoiseReduction~1.0
Mute~1.0
LivePhotoKeyFrame~1.0
Levels~1.0
HighResolutionFusion~1.0
Grain~1.0
Effect3D~1.0
Effect~1.0
DepthEffect~1.0
Definition~1.0
Debug~1.0
Curves~1.0
CropStraighten~1.0
AutoLoop~1.0
ApertureRedEye~1.0
required
contents
+[PISchema semanticEnhance]
number
minimum
maximum
default
compound
content
array
enum
values
bool
Adjustment
+[PISchema debugSchema]
ui_minimum
ui_maximum
Debug
+[PISchema videoCrossfadeLoopSchema]
+[PISchema videoStabilizeSchema]
+[PISchema videoReframeSchema]
+[PISchema selectiveColorSchema]
+[PISchema curvesSchema]
+[PISchema levelsSchema]
+[PISchema whiteBalanceSchema]
identity
+[PISchema noiseReductionSchema]
+[PISchema definitionSchema]
+[PISchema orientationSchema]
+[PISchema vignetteSchema]
+[PISchema retouchSchema]
+[PISchema apertureRedEyeSchema]
+[PISchema redEyeSchema]
opaque
iPhone
+[PISchema effectSchema]
+[PISchema portraitEffectSchema]
+[PISchema effect3DSchema]
+[PISchema depthEffectSchema]
+[PISchema highResFusionSchema]
+[PISchema autoLoopSchema]
+[PISchema videoPosterFrameSchema]
+[PISchema muteSchema]
+[PISchema livePhotoKeyFrameSchema]
+[PISchema slomoSchema]
+[PISchema trimSchema]
+[PISchema cropSchema]
+[PISchema sharpenSchema]
+[PISchema grainSchema]
+[PISchema smartBlackAndWhiteSchema]
+[PISchema smartColorSchema]
+[PISchema smartToneSchema]
+[PISchema rawNoiseReductionSchema]
+[PISchema rawSchema]
+[PISchema sourceSelectSchema]
finalizerError
T@"NSError",&,N,V_finalizerError
performedActions
TQ,N,V_performedActions
reframeRect
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_reframeRect
rollAngleDegrees
Td,N,V_rollAngleDegrees
pitchAngleDegrees
Td,N,V_pitchAngleDegrees
yawAngleDegrees
Td,N,V_yawAngleDegrees
T@"NSArray",C,N,V_keyframes
T{?={?=qq}{?=qq}},N,V_stabCropRect
shouldAllowPerspectiveCorrection
candidacy
TQ,N,V_candidacy
enable_perspective_correction
Capture
minRotateCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraVerticalPerspectiveCorrection
maxCameraAutoStraightenCorrection
com.apple.mobileslideshow.reframe.type.photo.perspective
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.type.photo.subject
com.apple.mobileslideshow.reframe.type.video.subject
disposition
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
v16@?0@"PIAdjustmentController"8
PICompositionFinalizer-overcaptureImageProperties
PICompositionFinalizer-primaryImageProperties
horizon
perpsective
reframe
shouldPerformAutoCrop
TB,V_shouldPerformAutoCrop
shouldPerformAutoStraighten
TB,V_shouldPerformAutoStraighten
shouldUseAutoStraightenVerticalDetector
TB,V_shouldUseAutoStraightenVerticalDetector
autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
autoStraightenDominantAngleDiffThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
maxAutoStraighten
Td,V_maxAutoStraighten
minAutoStraighten
Td,V_minAutoStraighten
-[PICropAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
%@AutoCropEvaluation-txt.DBG
error
autoCrop
belowMinimum
limitExceeded
straightenAngleInDegreesCCW
CIAutoStraighten
filterOptions
PICropAutoCalculator-faceDetection
kCIImageAutoAdjustLevel_DominantAngleDiffThreshold
kCIImageAutoAdjustLevel_VerticalAngleThreshold
kCIImageAutoAdjustLevel_UseVerticalDetector
-[PICropAutoCalculator undoExifOrientation:error:]
Source geometry has 0 size
PICropAutoCalculator-imageProperties
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
v16@?0@"PICropAdjustmentController"8
PICropAutoCalculator-getAfterGeometry
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-stitchedOvercaptureRect
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
getSpan
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/Internal/CatmullRom.inl
vrfPositionKernelSize
vrfPhaseShift
%f, 
Gaussian1D
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/Internal/VRFFilters.h
ABS(weightSum - 1) < 0.01
VRFLoggingEnabled
vrfSmoothingMethod
vrfAreaLookBehind
vrfAreaLookAhead
inputColor
offsetSaturation
offsetCast
inputColorKey
offsetSaturationKey
offsetCastKey
attributeVibrancyKey
attributeCastKey
inputCorrections
T@"NSArray",&,N,V_inputCorrections
iptLumHueSatTable
add_gaussian
CIConstantColorGenerator
iptToSRGB
srgbToIPT
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
T{?=qiIq},N,V_time
rawTime
T{?=qiIq},N,V_rawTime
sampleMode
Tq,N,V_sampleMode
keyframeSequence
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
inputVideoProperties
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
frameDuration
T{?=qiIq},N,V_frameDuration
shouldApplyWatermark
TB,N,V_shouldApplyWatermark
CIPerspectiveTransform
inputBottomRight
inputBottomLeft
inputTopRight
inputTopLeft
pipelineState
Could not get the input image properties
Could not get the input geometry
Could not get the input image
-[PIVideoReframeNode _evaluateImageGeometry:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/PIVideoReframeNode.m
Failed to get input geometry
-[PIVideoReframeNode _evaluateVideoProperties:]
-[PIVideoReframeNode initWithSettings:inputs:]
showStabilizationWatermark
invalid crop rect
PIVideoReframeNode.m
convertFromYIQToRGB
convertFromRGBToYIQ
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
T@"NSNumber",&,N,V_strength
warmth
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
PISmartToneAutoCalculator
-[PISmartColorAutoCalculator submit:]
force
TB,V_force
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
touchDiameter
locationY
locationX
/masterSpace
inputISO
T@"NSNumber",C,N,VinputISO
inputAmount
T@"NSNumber",C,N,VinputAmount
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
%@-%d-%ld.tiff
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:options:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
fileURL != nil
Failed to write image to file %@
Successfully wrote image to file %@
+[PIImageIO writeCGImage:fileURL:]
GU %@ %@
Unhandled bit depth: %ld
image != nil
+[PIImageIO writeImage:fileURL:]
unwantedSubjectInclusionThreshold
Td,N,V_unwantedSubjectInclusionThreshold
dominantToPeripheralSubjectRatioThreshold
Td,N,V_dominantToPeripheralSubjectRatioThreshold
minimumCorrectionThreshold
Td,N,V_minimumCorrectionThreshold
humanFaceBoundsContainmentThreshold
Td,N,V_humanFaceBoundsContainmentThreshold
humanBodyBoundsContainmentThreshold
Td,N,V_humanBodyBoundsContainmentThreshold
humanBodyExpandedBoundsContainmentThreshold
Td,N,V_humanBodyExpandedBoundsContainmentThreshold
humanBodyBoundsContainmentCoefficient
Td,N,V_humanBodyBoundsContainmentCoefficient
petBoundsContainmentThreshold
Td,N,V_petBoundsContainmentThreshold
petExpandedBoundsContainmentThreshold
Td,N,V_petExpandedBoundsContainmentThreshold
petBoundsContainmentCoefficient
Td,N,V_petBoundsContainmentCoefficient
facePaddingFactor
Td,N,V_facePaddingFactor
bodyPaddingAmount
Td,N,V_bodyPaddingAmount
overscanPercentageAllowed
Td,N,V_overscanPercentageAllowed
unwantedSubjectStartingThreshold
Td,N,V_unwantedSubjectStartingThreshold
shouldAttemptReframe
TB,R,N,V_shouldAttemptReframe
mutableSubjects
T@"NSMutableArray",R,C,N,V_mutableSubjects
ruleSystem
T@"NURuleSystem",R,N,V_ruleSystem
sceneContainsPet
TB,N,V_sceneContainsPet
sceneContainsHuman
TB,N,V_sceneContainsHuman
sceneContainsMultipleSubjects
TB,N,V_sceneContainsMultipleSubjects
T@"PIStillReframerConfiguration",R,N,V_configuration
overscanBounds
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_overscanBounds
viewBounds
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_viewBounds
T@"NSArray",R,C,N,V_ANODSubjects
T@"VNSaliencyImageObservation",R,N,V_saliencyObservation
result
T@"PIReframeResult",R,N,V_result
evaluationData
T@"NSData",R,N
postGateInsignificantCorrection
postGateEqualViewBounds
reframedBounds
i12@?0i8
state.%K.type == %@
shouldIncludeSubject
state.%K == TRUE && facts.%K == TRUE && state.%K == TRUE
state.%K < constants.overscanPercentageAllowed
canIncludeSubject
candidateRectAllowed
amountOfOverscanUsed
object
candidateReframedBounds
expandedBoundsPercentageInside
boundsPercentageInside
v24@?0@"NSPredicate"8@"NSString"16
@"NSString"32@?0@"NSPredicate"8Q16@"NSString"24
@"NSString"32@?0Q8@"NSString"16@24
peripheralSubject-%ld
dominantSubject
q24@?0@"PIReframeSubject"8@"PIReframeSubject"16
v24@?0@"PIReframeSubject"8@?<i@?i>16
(%f, %f, %f, %f)
PIStillReframer
T@"PIStillReframerConfiguration",R,N
v24@?0@"VNRequest"8@"NSError"16
subject%ld-%@
edgesKey
frameProvider
T@?,C,N,V_frameProvider
revision
Tq,R,N,V_revision
v8@?0
Invalid viewRect
Invalid cleanAperture
Invalid encodedPixelSize
PIVideoReframer is an abstract base class
defaultRevision
Tq,R
{CGRect={CGPoint=dd}{CGSize=dd}}
{CGPoint=dd}
viewRect
blobCentroid
blobRect
blobSubjects
subjectsReframed
tail
head
subjectsTracked
avgCameraMotionY
avgCameraMotionX
frameDominance
Td,V_frameDominance
cameraCorrelation
Td,V_cameraCorrelation
usedInReframing
TB,V_usedInReframing
  cameraCorrelation=%.2f
  frameDominance=%.2f
<%@ %p %@ id=%lu conf=%.2f>
velocity
T{CGVector=dd},V_velocity
acceleration
T{CGVector=dd},V_acceleration
isVirtualHead
TB,V_isVirtualHead
isVirtualTail
TB,V_isVirtualTail
Invalid revision
+[PIVideoReframer defaultConfigForRevision:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/PIVideoReframer.mm
0 && "unreachable"
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Adjustments/PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
Sensitivity
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
Bad float to fixed 16 conversion
inputSensitivity
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
aperture
Tf,N,V_aperture
portraitStrength
Tf,N,V_portraitStrength
minimumAperture
T@"NSNumber",&,N,V_minimumAperture
maximumAperture
T@"NSNumber",&,N,V_maximumAperture
portraitMajorVersion
TQ,N,V_portraitMajorVersion
portraitMinorVersion
TQ,N,V_portraitMinorVersion
depthVersionInfo
T{?=ii},N,V_depthVersionInfo
<%@:%p aperture:%f minimumAperture: %@ maximumAperture: %@ portraitStrength: %f portraitMajorVersion:%lu portraitMinorVersion:%lu>
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
We only know up to sDOF version 4. Found:
Support for sDOF version 4 is turned off. defaults read -g disablePhotoEditSupportForAdjustmentVersion_1_8 == 1
disablePhotoEditSupportForAdjustmentVersion_1_8
Depth data version mismatch, asset has %@ but we can only handle %@
depthData:DepthDataVersion
Missing auxiliary metadata
Failed to load auxiliary data
Missing camera calibration data
Low quality depth data is not supported
Unfiltered depth data is not supported
Failed to load depth data
Portrait was previously applied.
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
-[PIPortraitAutoCalculator submit:]
capturedPortraitStrength
capturedAperture
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
metadata != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
SDOFRenderingVersion
faceObservations != nil
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
NUOrientationIsValid(orientation)
rightPupil
leftPupil
innerLips
outerLips
medianLine
noseCrest
nose
rightEyebrow
leftEyebrow
rightEye
leftEye
faceContour
allPoints
faceOrientationIndex
faceJunkinessIndex
faceBoundingBox
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
Insufficient number of landmark points
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Adjustments/PIPipelineFilters.m
Could not construct oneShotPortraitV2ExportFilter filter from inline source
/PortraitV2
/PortraitV2-zeroStrength
return input;
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
Could not construct overcaptureSourceFilter filter from inline source
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters primarySourceFilter]
Could not construct primarySourceFilter filter from inline source
return Source(composition.source, { 'skipOrientation' : true })
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
+[PIPipelineFilters autoloopStabilizedVideoFilter]
Could not construct autoloopStabilizedVideoFilter filter from inline source
/AutoLoop/Output
/AutoLoop/StabilizedVideo
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
+[PIPipelineFilters applyOrientationFilter]
Could not construct pipeline filter from source: %@
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
Could not construct stopAtTagIncludeOrientationFilter from inline source
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/Orientation
/pre-Orientation
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
Could not construct stopAtTagIncludeGeometryFilter from inline source
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
/perspectiveStraighten
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
/Crop
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
+[PIPipelineFilters noOrientationFilter]
Could not construct noOrientationFilter filter from inline source
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
+[PIPipelineFilters noGeometryFilter]
Could not construct noGeometry filter from inline source
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
/pre-Trim
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
+[PIPipelineFilters iosCropToolFilter]_block_invoke
/post-Adjustments
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
+[PIPipelineFilters noCropFilter]
Could not construct noCropFilter filter from inline source
/pre-Crop
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
+[PIPipelineFilters noMuteFilter]
Could not construct noMuteFilter filter from inline source
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
+[PIPipelineFilters noTrimFilter]
Could not construct noTrimFilter filter from inline source
/SloMo
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
+[PIPipelineFilters noRedEyeFilter]
Could not construct noRedEye filter from inline source
/post-RedEye
/pre-RedEye
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
expected RAW in rawSourceFilterIncludingOrientation
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
Raw/Linear
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
colorType
faceStrength
faceWarmth
faceI
faceQ
grayStrength
grayWarmth
grayY
grayI
grayQ
warmTemp
warmTint
warmFace
warmTempKey
warmTintKey
warmFaceKey
none
%f,%f,%f,%f
-[PIVideoReframeRenderJob _createStabilizedKeyframesFromReframer:videoTrack:viewRect:timedMetadata:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.1.190/workspaces/neutrino/PhotoImaging/Reframer/PIVideoReframeRequest.m
ABS(sx - sy) < 1e-4
invalid padded rect
PIVideoReframeRequest.m
ReframeRects.DBG
ReframeSubjects.DBG
confidenceThreshold
maxStabilizeCrop
vrfPostgateRecoveryImprovement
error != nil
-[PIVideoReframeRenderJob prepare:]
^{CGImage=}32@?0{?=qiIq}8
Invalid metadata in asset
Failed to initialize metadata extractor
PIRedEye
PIApertureRedEyeFilter
PILongExposureFusion
AutoLoop/LongExposureMotion
transform
error != NULL
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Adjustments/PIPhotosPipelineHelper.m
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
depthInfo
glassesMatteAllowed
T{?={?=qq}{?=qq}},N
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_y
origin_x
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
q24@?0@"NSDictionary"8@"NSDictionary"16
loopFrameData_presTime
loopRecipe_frameInstructions
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-310.2.160/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
completion != nil
-[PICurvesAutoCalculator submit:]
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIReframeRules
PISemanticEnhanceAdjustmentController
PILocalContrastHDR
PISubjectCluster
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PIExpandedSubjectCalculatorConfiguration
PIExpandedSubjectCalculator
PILevelsFilterHDR
PIStillReframeRequest
PIStillReframeJob
_PIStillReframeResult
PIStillReframeResult
NURenderResult
NSObject
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
PIZlibDataCompressionOptions
PIZlibDataDecompressionOptions
PIZlibDataCompression
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIReframeAutoCalculator
PIFaceObservationCache
PISourceSelectAdjustmentController
PIReframeKeyframe
PIReframeKeyframeSequence
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PIAutoLoopAnalysisResult
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
ICFlowControl
_PIVideoStabilizeFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
NUTimeBased
_PIWhiteColorCalculator
PIWhiteBalanceAutoCalculator
GrayColorResult
RGBResult
PIDefinitionFilter
PIReframeResult
PISmartBlackAndWhiteAdjustmentController
PICompositionExportImagePrepareResult
PICompositionExportResult
PICompositionExportAuxiliaryResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PISmartColorFilterHDR
PrivateSmartColorHDR
PIVideoCrossfadeLoopAdjustmentController
PIPortraitAdjustmentController
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIDefinitionAdjustmentController
PIReframeSubject
NSSecureCoding
NSCoding
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
PILongExposureAccumulator
PILongExposureRegistrationResult
_PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PICaptureDebugUtilities
PISlomoAdjustmentController
PIDebugAdjustmentController
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIClusterPoint
PIApertureRedEyeAutoCalculator
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PrivateLocalLightHDR
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PISmartToneFilterHDR
PrivateSmartToneHDR
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframePipelineStateSetting
PIVideoReframeNode
PIVideoReframe
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISourceSampler
PIPhotoGrainHDR
PIImageIO
PIStillReframerConfiguration
PIStillReframer
PISharpenAdjustmentController
PIVideoReframer
PIVideoReframerV1
PIVideoReframeDebugSubjectV1
PIVideoReframeDebugSubject
PIVideoReframerV2
PIVideoReframeDebugSubjectV2
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIVideoReframeRenderJob
_PIVideoReframeResult
PIVideoReframeResult
PIVideoReframeRequest
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
kernel
componentsJoinedByString:
callStackSymbols
stringWithFormat:
kernelWithString:
_inputImage
photoEffectName
outputImage
inputImage
setInputImage:
.cxx_destruct
setValue:forKey:
filterWithName:
applyWithExtent:arguments:
arrayWithObjects:count:
stringByReplacingOccurrencesOfString:withString:
kernelBlackAndWhite
_inputThreshold
_inputDepthMap
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
numberWithFloat:
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
gradeForFact:
factCandidateForHorizon
factCandidateForPerspective
factCandidateForReframe
factCandidateForStill
factCandidateForVideo
sharedPregateRules
pregateRulesSystemWithConstants:
addRulesFromArray:
setConstants:
setEnableLogging:
initWithArray:
retractFact:
addObject:
ruleWithPredicate:retractingFact:grade:
predicateWithFormat:
ruleWithPredicate:assertingFact:grade:
ruleWithPredicate:action:
initWithCapacity:
isSettingEqual:forKey:
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
faceBoundingBoxesKey
copy
dictionaryWithObjects:forKeys:count:
numberWithDouble:
countByEnumeratingWithState:objects:count:
arrayWithCapacity:
boundingBoxesKey
sceneConfidenceKey
sceneLabelKey
floatValue
genericLandscapeSceneLabel
sunriseSunsetSceneLabel
isEqualToString:
platedFoodSceneLabel
intensityKey
isEqualToArray:
doubleValue
currentHandler
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
customAttributes
inputStrength
inputScale
_kernelLocalContrast
HLGOpticalScale
imageByCroppingToRect:
imageByApplyingGaussianBlurWithSigma:
imageByApplyingTransform:
imageByClampingToExtent
imageByApplyingFilter:withInputParameters:
vectorWithX:Y:Z:W:
_salientObject
_saliencyScale
_body
_direction
_mutablePoints
_centerPoint
initWithBody:face:saliencyScale:
initSalientClusterWithCenterPoint:saliencyScale:
initWithBody:direction:saliencyScale:
description
shouldAllowPoint:
expandedSubject
points
addPoint:
addPointsFromCluster:
boundingBox
body
direction
centerPoint
mutablePoints
saliencyScale
isSalientObject
subjectForExpansionWithSaliencyScale:startingSubject:
directionForBody:face:
_composition
_delegateFlags
_identifierMap
_changeDelegate
_mediaType
_imageOrientation
copyWithZone:
initWithComposition:
composition
setChangeDelegate:
compositionKeys
adjustmentKeys
availableKeys
addAdjustmentWithKey:
replaceAdjustment:withKey:
removeAdjustmentWithKey:
adjustmentControllerForKey:
modifyAdjustmentWithKey:modificationBlock:
applyChangesFromCompositionController:
isEqual:visualChangesOnly:
isEqual:forKeys:visualChangesOnly:
isEqual:forKeys:comparisonBlock:
debugDescription
differingAdjustmentsWithComposition:
userOrientation
sourceSelection
setSourceSelection:
setOvercaptureSource:
overcaptureSource
_adjustmentControllerClassForKey:
setSource:mediaType:
source
setMediaType:
changeDelegate
mediaType
imageOrientation
setImageOrientation:
isSubclassOfClass:
compositionController:adjustmentControllerClassForKey:
adjustmentControllerClassForKey:
boolValue
_keyToIdentifierMap
integerValue
containsObject:
allKeys
schema
compositionController:didUpdateAdjustments:
compositionController:didUpdateAdjustment:
reset
initWithIdentifier:
compositionController:didRemoveAdjustment:
compositionController:didAddAdjustment:
addObjectsFromArray:
contents
schemaForKey:
settingForAdjustmentKey:settingKey:
photosSchema
schemaWithIdentifier:
sharedRegistry
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
validateCompositionWithMissingSource:error:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
serializeComposition:versionInfo:error:
serializeComposition:versionInfo:serializerMetadata:error:
validateAdjustmentsEnvelope:error:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
serializeDictionary:error:
deserializeDictionaryFromData:error:
_sanitizeComposition:
adjustmentInformationForComposition:error:
formatVersion
formatIdentifier
data
dictionary
objectForKey:
infoDictionary
mainBundle
handleFailureInMethod:object:file:lineNumber:description:
errorWithDomain:code:userInfo:
JSONObjectWithData:options:error:
dataWithJSONObject:options:error:
enumerateKeysAndObjectsUsingBlock:
initWithDomain:code:userInfo:
setWithArray:
setData:
setFormatVersion:
setFormatIdentifier:
_data
_formatIdentifier
_formatVersion
setWithObjects:
array
orientation
height
numberWithInteger:
width
setOrientation:
setHeight:
setWidth:
size
setName:
_width
_height
_orientation
initWithName:
stringByAppendingString:
validateComposition:error:
URLWithString:
_useRegressedSaliencyBoxes
_denistyThreshold
_neighborhoodOffset
_saliencyThreshold
_saliencyDeltaThreshold
_initialSaliencyValue
_unionIOUThreshold
_clusterToRegressedRatio
_regressedConfidenceThreshold
_highSaliencyThreshold
_interSalientObjectDistance
_maxSalientObjectsCount
_salientClusterConvergenceMaxDistance
denistyThreshold
setDenistyThreshold:
neighborhoodOffset
setNeighborhoodOffset:
saliencyThreshold
setSaliencyThreshold:
saliencyDeltaThreshold
setSaliencyDeltaThreshold:
initialSaliencyValue
setInitialSaliencyValue:
useRegressedSaliencyBoxes
setUseRegressedSaliencyBoxes:
unionIOUThreshold
setUnionIOUThreshold:
clusterToRegressedRatio
setClusterToRegressedRatio:
regressedConfidenceThreshold
setRegressedConfidenceThreshold:
highSaliencyThreshold
setHighSaliencyThreshold:
interSalientObjectDistance
setInterSalientObjectDistance:
maxSalientObjectsCount
setMaxSalientObjectsCount:
salientClusterConvergenceMaxDistance
setSalientClusterConvergenceMaxDistance:
defaultConfiguration
saliencyRevisionOneConfiguration
_expandedSubjects
_configuration
_detectedSubjects
_saliencyData
_subjectDirection
_saliencyImageObservation
_regressedSalientSubjects
initWithDetectedSubjects:saliencyData:configuration:
initWithDetectedSubjects:cgImage:
initWithDetectedSubjects:ciImage:ciContext:configuration:
neighborsForPoint:
closestClusterForPoint:fromClusters:
clusterPoints:intoClusters:
findBestStartingPointForStartingPoint:
findSalientPointsWithSaliencyScale:outsideOfSubjectsRect:
expandedSubjects
configuration
detectedSubjects
saliencyData
subjectDirection
setSubjectDirection:
saliencyImageObservation
regressedSalientSubjects
consolidateCandidateSalientClusters:maxDistance:
removeAllObjects
arrayWithArray:
removeObject:
objectAtIndexedSubscript:
unionSet:
setWithSet:
anyObject
salientSubjectsWithImageRequestHandler:
saliencyDataForSaliencyObservation:
saliencyObservationWithImageRequestHandler:
initWithCIImage:options:
initWithCGImage:options:
performRequests:error:
initWithCompletionHandler:
salientObjects
firstObject
results
setPrivateRevision:error:
writeJPEGRepresentationOfImage:toURL:colorSpace:options:error:
fileURLWithPath:
context
initWithCVPixelBuffer:
timeIntervalSince1970
boolForKey:
standardUserDefaults
pixelBuffer
defaultValueForKey:
_customAttributesForKey:
dictionaryWithObjectsAndKeys:
containsString:
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
P3KernelHDR
setDefaults
floatValueForKey:defaultValue:clearIfNotDefault:
_LUTImage
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
imageByPremultiplyingAlpha
imageByColorMatchingColorSpaceToWorkingSpace:
applyWithExtent:roiCallback:arguments:options:
definition
samplerWithImage:
imageByUnpremultiplyingAlpha
imageByColorMatchingWorkingSpaceToColorSpace:
CGColorSpace
itur2100HLGColorSpace
samplerWithImage:keysAndValues:
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
dataWithBytesNoCopy:length:freeWhenDone:
valueForKey:
_scalePolicy
newRenderJob
mediaComponentType
submit:
scalePolicy
setScalePolicy:
submitGeneric:
initWithStillReframeRequest:
_reframeResult
initWithRequest:
stillReframeRequest
wantsOutputImage
wantsOutputGeometry
wantsCompleteStage
cacheKey
render:
result
cleanUp
reframeResult
setReframeResult:
setSaliencyObservation:
saliencyObservation
setANODSubjects:
ANODSubjects
setConfidence:
confidence
setBounds:
bounds
_confidence
_saliencyObservation
_ANODSubjects
_bounds
statistics
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
writeToURL:atomically:
URLByAppendingPathComponent:
outputGeometry
setPipelineFilters:
stringValue
finalize
nu_updateDigest:
renderNode
stringByAppendingFormat:
oneToOneScalePolicy
_inputCutoff
inputCutoff
setInputCutoff:
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
initWithComposition:destinationURL:
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
sRGBColorSpace
colorSpaceFromVideoColorProperties:
outputSettings
setCompletionBlock:
submitRequest:
submitGenericRequest:
setGenericCompletionBlock:
_flavor
flavor
setFlavor:
_recipe
_cleanAperture
recipe
setRecipe:
cleanAperture
setCleanAperture:
_compressionLevel
_strategy
_windowBits
_memoryLevel
_chunkSize
setCompressionLevel:
setCompressionStrategy:
compressionLevel
strategy
setStrategy:
windowBits
setWindowBits:
memoryLevel
setMemoryLevel:
chunkSize
setChunkSize:
defaultOptions
_decompressAllAtOnce
_createBuffer
_growData
setCreateBuffer:
setGrowData:
createBuffer
growData
decompressAllAtOnce
setDecompressAllAtOnce:
increaseLengthBy:
length
dataWithLength:
compressData:options:error:
decompressData:options:error:
dataWithData:
setLength:
mutableBytes
init
raise:format:
stabilizedCropRect
flavorKey
recipeKey
_startTime
_crossfadeDuration
_loopTimeRange
initWithSettings:inputs:
initWithInput:timeRange:crossfadeDuration:startTime:
input
resolvedNodeWithCachedInputs:settings:pipelineState:error:
shouldCacheNodeForPipelineState:
nodeByReplayingAgainstCache:pipelineState:error:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
startTime
loopTimeRange
crossfadeDuration
setInputParameters:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
setVolume:atTime:
audioMixInputParametersWithTrack:
audioMix
tracksWithMediaType:
timeRange
firstEnabledVideoTrackInAsset:error:
outputVideo:
setSourceTrackIDForFrameTiming:
setInstructions:
setRenderSize:
renderSize
setSourceIdentifier:forTrackID:
setRequiredSourceTrackIDs:
numberWithInt:
trackID
setTimeRange:
unsupportedError:object:
instructions
outputVideoComposition:
inputs
conformRange:inRange:
debugDescriptionOfAssetTrack:
insertTimeRange:ofTrack:atTime:error:
addMutableTrackWithMediaType:preferredTrackID:
errorWithCode:reason:object:underlyingError:
errorWithCode:reason:object:
setEvaluatedForMode:
nodeFromCache:cache:
initWithFilterName:settings:inputs:
setVideoFrames:
missingError:object:
videoFrames
evaluationMode
initWithAdjustment:
keyFrameTime
setKeyFrameTime:
scaleKey
timeKey
numberWithLongLong:
intValue
longLongValue
radius
setRadius:
falloff
setFalloff:
falloffKey
radiusKey
_changes
_identifier
_adjustment
displayName
displayInputKeys
inputKeys
settingForKey:
hasInputKey:
enabled
setEnabled:
canBeEnabled
canHaveAuto
hasAutoKeyInSchema
isAuto
setIsAuto:
objectForKeyedSubscript:
setObject:forKeyedSubscript:
setValue:forUndefinedKey:
valueForUndefinedKey:
valuesForArrayInputKey:
setFromAdjustment:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
visualInputKeys
isEqualToAdjustmentController:
isEqual:forKeys:
_setPrimitiveValue:forKey:
_primitiveValueForKey:
settings
_isDefault
identifier
setIdentifier:
adjustment
defaultValue
values
autoKey
numberWithBool:
enabledKey
name
reframeStillWithCompletion:
reframeVideoWithCompletion:
initWithError:
initWithResult:
valueWithBytes:objCType:
result:
dictionaryWithDictionary:
_group
_queue
_result
submit:response:
submitSynchronous:error:
submitGenericSynchronous:
faceRequestWithRequest:
initWithTargetPixelCount:
sourceSelectionKey
stringForSourceSelection:
sourceSelectionForString:
_time
_homography
initWithTime:homography:
initWithDictionaryRepresentation:
dictionaryRepresentation
time
homography
mutableCopy
count
keyframesFromDictionaryRepresentations:
_homographySequence
initWithKeyframeArray:
interpolation
homographyAtTime:
sparseSequence
sampleAtTime:
initWithCount:times:values:
_computeCleanAperture:
stopAtTagFilter:
setKind:
kind
setVersion:
version
versionKey
kindKey
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
removeAssetIdentifierFromMetadataArray:
removeObjectAtIndex:
indexOfObjectPassingTest:
setValue:
metadataItem
_temperature
_tint
setInputVectorsForFilter:
temperature
setTemperature:
tint
setTint:
_videoSource
wantsOutputVideo
analysisRequest
prepare:
videoSource
setVideoSource:
asset:
ICShouldBeCanceled
ICReportProgress:
_progressHandler
_rangeMin
_rangeMax
_shouldCancelHandler
progressHandler
setProgressHandler:
rangeMin
setRangeMin:
rangeMax
setRangeMax:
shouldCancelHandler
setShouldCancelHandler:
_keyframes
_analysisType
_rawHomographies
_stabCropRect
keyframes
stabCropRect
analysisType
rawHomographies
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
_allowedAnalysisTypes
_allowedCropFraction
wantsRenderStage
allowedAnalysisTypes
setAllowedAnalysisTypes:
allowedCropFraction
setAllowedCropFraction:
nominalFrameRate
canceledError:object:
isCanceled
failureError:object:
cleanApertureOfTrack:oriented:
outputVideo
canPerformGyroBasedStabilizationForAsset:
_rawState
setTime:
initWithRequest:isRAW:
rawState
rawProperties
calculateRAWWithRequest:completion:
calculateWithRequest:completion:
faceBalanceResultFromFaceObservations:request:error:
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
readBufferRegion:withBlock:
bytesAtPoint:
frameRect
buffer
validRegion
BGRA8
RGBA8
isEqualToPixelFormat:
ARGB8
maximumValue
minimumValue
setResponseQueue:
setRegionPolicy:
initWithRect:
addRect:
imageSize
faces
initWithRequest:dataExtractor:options:
_bufferRenderClient
_imageDataClient
_useSushi
initWithComposition:useSushi:
readBufferFromImage:withRGBAfBufferBlock:
calculateColorWithProperties:completion:
_brightnessMultiplierFromImageProperties:
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
_configureRequest:
_computeGreenPercentage:
_submitGWRenderRequest:
_submitGERenderRequest:
initWithScript:
initWithSource:
submitRequest:completion:
initWithComposition:dataExtractor:options:
setTileSize:
genericRGBLinearColorSpace
setPixelFormat:
RGBAh
whiteFactor
whiteValue
rowBytes
metadata
commitAndNotifyOnQueue:withBlock:
pi_valueWithGrayColorResult:
rawCameraSpaceProperties
begin
returnStorage:
writeBufferInRegion:block:
regionWithRect:
newStorageWithSize:format:
RGBAf
bufferFactory
sharedFactory
initWithName:responseQueue:
_useTempTint:
_correctedRGBResultFromResult:
_chooseNeutralGrayForNonSushi:
_chooseTempTintForSushi:RAWProperties:brightness:
inputNeutralXYFromRGB:
valueWithRGBResult:
responseQueue
pi_grayColorResultValue
getValue:
RGBResultValue
definitionKernel
_inputBlurImage
_inputIntensity
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
initWithBounds:confidence:ANODSubjects:saliencyObservation:
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputGrainKey
inputHueKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
tone
setGrain:
grain
setHue:
_request
_inputSize
request
setRequest:
inputSize
setInputSize:
_geometry
geometry
setGeometry:
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
properties
setProperties:
_priority
_colorSpace
_pairingIdentifier
priority
setPriority:
colorSpace
setColorSpace:
pairingIdentifier
setPairingIdentifier:
displayP3ColorSpace
initWithLevel:
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_metadataProcessor
_videoCodecType
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
videoCodecType
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
bypassOutputSettingsIfNoComposition
setBypassOutputSettingsIfNoComposition:
applyVideoOrientationAsMetadata
setApplyVideoOrientationAsMetadata:
requireHardwareEncoder
setRequireHardwareEncoder:
_optimizeForSharing
_applyImageOrientationAsMetadata
_imageExportFormat
_JPEGCompressionQuality
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
JPEGCompressionQuality
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
setCompressionQuality:
defaultFormatForURL:
fileType
UTIForExtension:
pathExtension
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
exportImageToURL:composition:options:completion:
exportImageToDataWithComposition:options:completion:
exportVideoToURL:composition:options:completion:
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
exportComposition:options:completionQueue:completion:
variationForFlavor:
prepareImageExportRequest:options:completion:
prepareAuxiliaryImagesFetchProperties:options:completion:
addImageProperties:composition:options:error:
addVideoProperties:composition:options:error:
shouldTryVideoRotationFastPath:options:
_exportVideoToURL:composition:options:properties:progress:completion:
_exportVideoToURLFull:composition:options:properties:progress:completion:
submitWithProgress:completion:
setOutputSettings:
setObject:forKey:
defaultExportCodecForComposition:
setBitRateMultiplicationFactor:
setPreferredTransform:
initWithCGAffineTransform:
preferredTransformFromOrientation:size:
originalSize
setMetadata:
videoMetadataForVariation:error:
metadataConverter
invalidError:object:
setPhotoFeatureFlags:properties:error:
numberWithUnsignedInteger:
unsignedIntegerValue
photoFeatureFlags:error:
setPhotoProcessingFlags:properties:error:
photoProcessingFlagsFromProperties:error:
setImageVariation:properties:error:
auxiliaryImage
setAuxiliaryImageType:
auxiliaryImagesProperties
setCoreGraphicsInfoDictionariesByAuxiliaryType:
dictionaryRepresentationForAuxiliaryDataType:
setImageProperties:
unknownError:object:
resetImageProperties:preserveRegions:
URLByAppendingPathComponent:isDirectory:
stringByAppendingPathExtension:
stringByDeletingPathExtension
lastPathComponent
URLForDirectory:inDomain:appropriateForURL:create:error:
defaultManager
isHeifUTI:
mismatchError:object:
UUIDString
UUID
initWithProperties:
discreteProgressWithTotalUnitCount:
destinationData
setFormat:
setRenderToData:
setDestinationURL:
setMetadataConverter:
removeObjectForKey:
bilateralKernels
RGBToLabKernels
bilateralAdd1Kernel
bilateralAdd2Kernel
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
bilateralFinalizeKernel
RGBToLabKernel
LabToRGBKernel
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
samplesPerPass
boundsForPointArray:
enlargedBounds:withPoints:
bilateralAddROI:destRect:userInfo:
doBilateralPass:points:weights:sums:slope:
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
subarrayWithRange:
samplerWithImage:options:
vectorWithX:Y:Z:
vectorWithX:Y:
applyWithExtent:roiCallback:arguments:
objectAtIndex:
unionWith:
shapeWithRect:
BWBilateralKernels
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
_inputBorder
bilateralROI:destRect:userInfo:
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
_inputRadius
inputRadius
setInputRadius:
_majorVersion
_minorVersion
_subMinorVersion
_platform
initWithMajor:minor:subMinor:platform:
initWithMajor:minor:subMinor:
string
asOrderedInteger
compare:
isEqualToAdjustmentVersion:
majorVersion
minorVersion
subMinorVersion
platform
caseInsensitiveCompare:
versionWithMajor:minor:subMinor:platform:
versionFromString:
rangeOfCharacterFromSet:
invertedSet
decimalDigitCharacterSet
componentsSeparatedByString:
amountKey
inputVibrancy
inputContrast
inputCast
_isIdentity
_kernelCPos
_kernelCNeg
_kernelV_gt1
_kernelV_lt1
_kernelCast
setInputVibrancy:
setInputContrast:
setInputCast:
smartColorHDRStatistics
render:toBitmap:rowBytes:bounds:format:colorSpace:
contextWithOptions:
setLoopTimeRange:
setCrossfadeDuration:
setStartTime:
startTimeTimescaleKey
startTimeValueKey
crossfadeDurationTimescaleKey
crossfadeDurationValueKey
loopTimeRangeDurationTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeStartValueKey
_version
setPortraitInfo:
portraitInfo
canRenderPortraitEffect
setSpillMatteAllowed:
spillMatteAllowed
spillMatteAllowedKey
portraitInfoKey
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
copyOfCompositionRemovingNoOps:
valueKey
imageWithCGImage:
imageByCompositingOverImage:
_type
_source
_edgeBleed
_expandedBounds
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithType:source:identifier:confidence:
isHuman
isAnimal
type
expandedBounds
setExpandedBounds:
edgeBleed
setEdgeBleed:
decodeObjectForKey:
decodeDoubleForKey:
decodeIntegerForKey:
encodeObject:forKey:
encodeDouble:forKey:
encodeInteger:forKey:
allocWithZone:
appendFormat:
appendString:
labels
_highKeyHDR
setInputStrength:
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunDetectorsIfNecessary
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_minSalientArea
_maxSalientSubjectArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
faceObservationCache
setFaceObservationCache:
perspectiveErrorFromCoreImage:
addMethodDiagnostics:details:
addMethodResultToDiagnostics:error:setYawPitchError:
wrapAsUnexpectedError:
writeDebugDiagnosticsToDisk
getSizeOfAllFaces:
passesFaceCheck:
hasFrontFacingCameraDimentions:
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
passesBuildingCheck:
passesSaliencyCheck:
overcaptureImageProperties:
primaryImageProperties:
canGenerateNewCropRect:
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
submitVerified:
maxAutoYaw
setMaxAutoYaw:
maxAutoPitch
setMaxAutoPitch:
maxAutoAngle
setMaxAutoAngle:
minimumPitchCorrection
setMinimumPitchCorrection:
minimumYawCorrection
setMinimumYawCorrection:
minimumAngleCorrection
setMinimumAngleCorrection:
minimumConfidence
setMinimumConfidence:
maxFaceSize
setMaxFaceSize:
minimumPitchCorrectionArea
setMinimumPitchCorrectionArea:
minimumYawCorrectionArea
setMinimumYawCorrectionArea:
disableOnPanos
setDisableOnPanos:
disableOnFrontFacingCameraImages
setDisableOnFrontFacingCameraImages:
shouldRunDetectorsIfNecessary
setShouldRunDetectorsIfNecessary:
shouldRunBuildingCheck
setShouldRunBuildingCheck:
minSalientArea
setMinSalientArea:
maxSalientSubjectArea
setMaxSalientSubjectArea:
angleSeedDegreesCCW
setAngleSeedDegreesCCW:
debugFilesEnabled
setDebugFilesEnabled:
debugFilesPrefix
setDebugFilesPrefix:
debugDiagnostics
debugLineDetectionImage
setDebugLineDetectionImage:
undoOrientation:forPitch:yaw:angle:
null
dataWithBytes:length:
defaultFocalLength
setRollAngle:constrainCropRectWithTargetArea:
initWithMasterImageSize:
initWithMasterImageSize:stitchedImageSize:
isFusedOvercapture
setComposition:
nonLocalizedFailureReason
observations
hasPrefix:
PNGRepresentationOfImage:format:colorSpace:options:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
userInfo
requestVisionCleanUp
releaseCachedResources
globalSession
luminanceKey
shortValue
isGeometryIdentityForImageSize:
isCropConstrained
isCropIdentityForImageSize:
cropRect
constraintWidth
constraintHeight
angle
angleRadians
pitch
pitchRadians
yawRadians
autoCropped
isSmart
isOriginalCrop
setCropRect:
setConstraintWidth:
setConstraintHeight:
setAngle:
setAngleRadians:
setPitch:
setPitchRadians:
setYaw:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
originalCropKey
smartKey
heightKey
widthKey
yOriginKey
xOriginKey
yawKey
pitchKey
angleKey
constraintHeightKey
constraintWidthKey
setInputDecoderVersion:
inputDecoderVersion
_smartSettings
_updateSettingsWithInputLight:
computedSettings
setInputLight:
inputLight
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setStatistics:
setOvercaptureStatistics:
overcaptureStatistics
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
offsetShadowsKey
offsetLocalLightKey
offsetHighlightsKey
offsetExposureKey
offsetContrastKey
offsetBrightnessKey
offsetBlackKey
overcaptureStatisticsKey
statisticsKey
inputRawHighlightsKey
inputLocalLightKey
inputBlackKey
inputHighlightsKey
inputShadowsKey
inputContrastKey
inputBrightnessKey
inputExposureKey
inputLightKey
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
alphaCompositingKernel
dynamismMapKernel
longExposureFusionKernels
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
assetIdentifierForURL:type:useEmbeddedPreview:
imageSourceWithURL:type:useEmbeddedPreview:
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
imageSourceWithCIImage:orientation:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
newComposition
compositionByRemovingVideoAndLivePhotoAdjustments:
newAdjustmentWithName:
newAdjustmentWithIdentifier:
newImageRenderClientWithName:
geometryRequestWithComposition:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
_imageRenderRequestWithComposition:wideGamut:
newCGImageFromBufferImage:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
is3DEffect:
isPortraitEffect:
isPortraitStageEffect:
isAVAssetEditable:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForShowingOriginalWithGeometry
pipelineFiltersForRAWShowingOriginalWithGeometry
newCompositionControllerWithComposition:
adjustmentConstants
handlePIGlobalSettings:
validatedCompositionCopyForComposition:mediaType:
knownFormatsVersionsMap
updateCropAdjustmentController:after:error:
preheatEditDependencies
prepareForPerformingRequests:error:
PIMuteAdjustmentKey
PITrimAdjustmentKey
PIPortraitAdjustmentKey
PIDepthAdjustmentKey
PIRedEyeAdjustmentKey
PIAutoLoopAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PICropAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIHighResFusionAdjustmentKey
PIOrientationAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PIVideoReframeAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
getTagWithPath:error:
resetTag:input:
initWithScript:block:
allValues
isAVAssetDolbyProfile5:error:
deviceSupportsHighDynamicRangeVideo
deviceSupportsHardware10BitHEVCEncoding
isAVAssetHDR:error:
isExportable
isPlayable
isReadable
initWithTargetSize:
setExtentPolicy:
setResolvedSourceDefinition:
initWithImageSourceDefinition:videoSourceDefinition:
resolvedSourceDefinition
setAssetIdentifier:
assetIdentifier
initWithSourceDefinitions:
initWithURL:UTI:
initWithCIImage:orientation:
setUseEmbeddedPreview:
absoluteString
timeIntervalSinceReferenceDate
getResourceValue:forKey:error:
addEntriesFromDictionary:
_inputBoost
inputBoost
setInputBoost:
kernelsDictionaryWithString:
kernelsWithString:
conversionMap
mapForSerialization
base64EncodedStringWithOptions:
initWithBase64EncodedString:options:
_inputTableImage
curvesKernel
inputTableImage
setInputTableImage:
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
initWithSize:renderer:
dealloc
workingColorSpace
cancel
start:
_initializeStorage:image:error:
isReadyForMoreData
_isReadyForMoreData
markAsFinished
_markAsFinished
waitUntilDone
accumulate:error:
_appendInputFrame:
nextInputFrame
_nextInputFrame
_start
_initializeAccumulation
_initializeAccumulation:
_accumulate:
_accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_dynamismMapWithMinImage:maxImage:extent:
_exportOutputImage:format:colorSpace:toURL:uti:error:
_accumError
set_accumError:
createCGImage:fromRect:format:colorSpace:deferred:
extent
useAsCIImageWithOptions:renderer:block:
useAsCIRenderDestinationWithRenderer:block:
renderImage:rect:toDestination:atPoint:error:
setLabel:
componentMin
componentMax
imageWithCVPixelBuffer:options:
CVPixelBuffer
imageWithColor:
colorWithRed:green:blue:colorSpace:
sRGBLinearColorSpace
surfaceStoragePool
observation
_observation
_extent
setObservation:
setExtent:
_stillImage
_guideExtent
wantsRenderScaleClampedToNativeScale
registrationRequest
newRenderPipelineStateForEvaluationMode:
guideExtent
setGuideExtent:
stillImage
setStillImage:
initWithCVPixelBuffer:options:
initWithTargetedCVPixelBuffer:options:
waitUntilCompletedAndReturnError:
initWithPixelBuffer:
newPixelBufferOfSize:format:
renderScale
videoProperties:
prepareNode
outputImage:
outputImageGeometry:
nodeByReplayingAgainstCache:error:
setScale:
prepareNodeWithPipelineState:error:
_shouldWaitForDependentJobs
initWithComposition:responseQueue:
_pipelineFilters
initWithComposition:tag:responseQueue:
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
posterFrameTime
setPosterFrameTime:
locallySupportedFormatVersions
currentFormatVersion
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
formatVersionForAdjustment:identifier:
adjustmentDataFormatVersionForComposition:
indexOfObject:
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
trajectoryHomography
_asset
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
timedMetadataArray
initWithAVAsset:
overwriteTrackingMetadataWithPlist:
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
extractMetadata
dataValue
dataType
items
nextTimedMetadataGroup
startReading
initWithAssetReaderTrackOutput:
addOutput:
canAddOutput:
initWithTrack:outputSettings:
assetReaderWithAsset:error:
initWithContentsOfFile:
encodedPixelSizeOfTrack:oriented:
formatDescriptions
tracks
exceptionWithName:reason:userInfo:
canProvideMetadataForAVAsset:
captureDebugDirectoryForComposition:
sourceDefinitions
endTime
setEndTime:
rate
setRate:
rateKey
endScaleKey
endKey
startScaleKey
startKey
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
bytesPerPixel
initWithSize:format:
mutableBytesAtPoint:
RG16
faceBalanceKernels
linearWideGamutColorSpace
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
inputOrigI
setInputOrigI:
inputOrigQ
setInputOrigQ:
inputWarmth
setInputWarmth:
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
inputPointsR
setInputPointsR:
inputPointsG
setInputPointsG:
inputPointsB
setInputPointsB:
inputPointsL
setInputPointsL:
tableImageFromRed:green:blue:luminance:
calculateCurveTable:
curvePointsFromDictionaries:
initWithImageProvider:width:height:format:colorSpace:options:
initWithLength:
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
colorBalanceKernel
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
applyInputConversion:
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_map
getMap
HDRFilterForSDRFilter:
substringFromIndex:
alignment
setAlignment:
alignmentKey
_settings
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
decoratorRenderFiltersForImages
decoratorRenderFiltersForVideos
forceGlassesMatteOff
setForceGlassesMatteOff:
forceSpillMatteOff
setForceSpillMatteOff:
allowSpillMatteOnOlderPortraitV2Captures
setAllowSpillMatteOnOlderPortraitV2Captures:
globalSettings
IPXEditSettings
PUEditSettings
falseColorHDR
setFalseColorHDR:
initWithAutoLoopExportRequest:
initWithVideoExportRequest:
autoLoopExportRequest
renderer:
metalRenderer
shouldUseMetalRenderer
initWithMetalDevice:options:
boolSettingForKey:defaultValue:
setUpContext:
toRect
initWithNode:context:
node
toDictionary
setError:
isObject
contextForContext:
currentContext
jsContext
bundleForClass:
URLForResource:withExtension:
newPhotosPipelineAtSourceURL:error:
initWithURL:
newPhotosPipeline:
_value
_point
initWithCGPoint:value:
distanceFromPoint:
isEqualToPoint:
point
value
_faceRequest
apertureRedEyeResultFromFaceObservations:imageSize:
normalizedPoints
pointCount
rightEye
leftEye
landmarks
cancelAllRequests
renderContext
inputLightMap
inputLightMapWidth
inputLightMapHeight
inputGuideImage
inputLightMapImage
inputSmartShadows
_shadowKernelHDR
_polyKernelHDR
localLightHDRStatisticsNoProxy
elementByteSize
rowElements
format
bufferColorspace
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
bytes
image
_calculateBlackAndWhiteSettingsFromBufferImage:
initWithTargetPixelSize:
CIFormat
inputNeutralGamma
inputTone
inputHue
inputGrain
inputSeed
inputScaleFactor
getNonNormalizedSettings:
createHueArray
hueArrayImage:
smartBlackWhiteKernel
setInputNeutralGamma:
setInputTone:
setInputHue:
setInputGrain:
setInputSeed:
setInputScaleFactor:
imageWithBitmapData:bytesPerRow:size:format:options:
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
hasCorrections
inputCorrectionInfoKey
calculateSettingsForImageHistogram:
calculateSettingsForSingleChannelHistogram:suffix:
percentile:
histogram
setHistogramCalculationColorSpace:
histogramCalculationColorSpace
luminance
blue
green
smartToneAdjustmentController
smartColorAdjustmentController
smartBWAdjustmentController
cropAdjustmentController
redEyeAdjustmentController
livePhotoKeyFrameAdjustmentController
videoPosterFrameAdjustmentController
depthAdjustmentController
trimAdjustmentController
slomoAdjustmentController
effectAdjustmentController
effect3DAdjustmentController
portraitAdjustmentController
orientationAdjustmentController
autoLoopAdjustmentController
highResFusionAdjustmentController
rawNoiseReductionAdjustmentController
sharpenAdjustmentController
whiteBalanceAdjustmentController
noiseReductionAdjustmentController
definitionAdjustmentController
vignetteAdjustmentController
videoReframeAdjustmentController
sourceSelectAdjustmentController
videoStabilizeAdjustmentController
videoCrossfadeLoopAdjustmentController
semanticEnhanceAdjustmentController
smartToneAdjustmentControllerCreatingIfNecessary:
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentControllerCreatingIfNecessary:
orientationAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
_editable
initWithX:y:editable:
initWithDictionary:
isEditable
_kernelBneg
_kernelBpos
_kernelC
_kernelC_hdr
_kernelH
_kernelRH
smartToneHDRStatistics
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
alignImage:transform:extent:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
inputStillImage
setInputStillImage:
inputMaskImage
setInputMaskImage:
inputRenderScale
setInputRenderScale:
inputVideoScale
setInputVideoScale:
inputAlignmentExtent
setInputAlignmentExtent:
inputAlignmentTransform
setInputAlignmentTransform:
imageByApplyingTransform:highQualityDownsample:
applyWithExtent:roiCallback:inputImage:arguments:
writeToTIFF:
currentDirectoryPath
debugDumpIntermediateImages
valueAtIndex:
CGRectValue
loadFusionTuningParameters
_debugDumpIntermediateImages
dictionaryWithContentsOfFile:
stringSettingForKey:defaultValue:
_processedRenderNodeForComposition:input:pipelineState:error:
evaluate:input:pipelineState:error:
scaledVector:
orientedNode:withOrientation:
originalCleanAperture
straightenTransformWithAngle:extent:
perspectiveTransformWithPitch:yaw:roll:imageRect:
sharpnessWithIntensity:
grainInputSeedFromFrameTime
createSloMoWithInput:startTime:endTime:rate:error:
trimInput:startTime:endTime:error:
livePhotoKeyFrameMetadataFromNode:time:error:
initWithInput:
isEqualToNumber:
cropNode:cropRect:cropSettings:
initWithInput:scale:
isCIFilterAvailable:propertyName:
scaleMultiplyOfScalar:
cameraCalibrationData
underlyingAVDepthData
auxiliaryImage:
auxiliaryCoreGraphicsInfoDictionary:
vectorWithFloats:
auxiliaryImageFromComposition:type:error:
scaledSize
scaleNode:scale:error:
cacheNode:type:settings:error:
isHDR
initWithCGColorSpace:
allAssetsCanUseHDRPipeline
auxiliaryImageType
enableHDRSupport
endGroupWithName:error:
addTagWithName:inputNode:error:
inputForPath:error:
transformNodeWithInput:transform:error:
initWithAffineTransform:
renderNodeFromSource:settings:error:
beginGroupWithName:error:
valueWithCMTime:
hasStaticTime
mediaTypeForComposition:
isEnabled
initWithPipelineState:
P3Kernel
displayP3LinearColorSpace
sourceSelectSchema
rawSchema
rawNoiseReductionSchema
smartToneSchema
smartColorSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
highResFusionSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
retouchSchema
vignetteSchema
orientationSchema
definitionSchema
noiseReductionSchema
whiteBalanceSchema
levelsSchema
curvesSchema
selectiveColorSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
photosCompositionSchema
registeredPhotosSchemaIdentifier
registerPhotosSchema
registerRenderPipeline:forIdentifier:
dictionaryForKey:
registerSchemas:error:
renderPipelineForIdentifier:
deserializeFromDictionary:error:
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
_reframeRect
performNextActionWithCompletion:
shouldPerformAction:
hasPerformedAction:
markActionAsPerformed:
performReframeWithCompletion:
performHorizonCorrectionWithCompletion:
performPerspectiveCorrectionWithCompletion:
shouldAllowPerspectiveCorrection
processStillReframeResult:
processVideoReframeResult:
processHorizonResult:
processPerspectiveResult:
candidacy
setCandidacy:
finalizerError
setFinalizerError:
performedActions
setPerformedActions:
reframeRect
setReframeRect:
rollAngleDegrees
setRollAngleDegrees:
pitchAngleDegrees
setPitchAngleDegrees:
yawAngleDegrees
setYawAngleDegrees:
setKeyframes:
setStabCropRect:
getValue:size:
floatForKey:
initWithDisposition:composition:
_disposition
disposition
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
descriptionForCandidacy:
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
imageProperties:
undoExifOrientation:error:
shouldPerformAutoCrop
setShouldPerformAutoCrop:
shouldPerformAutoStraighten
setShouldPerformAutoStraighten:
shouldUseAutoStraightenVerticalDetector
setShouldUseAutoStraightenVerticalDetector:
autoStraightenVerticalAngleThreshold
setAutoStraightenVerticalAngleThreshold:
autoStraightenDominantAngleDiffThreshold
setAutoStraightenDominantAngleDiffThreshold:
maxAutoStraighten
setMaxAutoStraighten:
minAutoStraighten
setMinAutoStraighten:
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
stitchedOvercaptureRect:primaryRect:forComposition:error:
initWithMasterImageRect:stitchedImageRect:
setRollRadians:
integralCropRect:
updateCropAdjustment:after:error:
_stats
_updateSettingsWithInputColor:
setInputColor:
inputColor
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
offsetSaturationKey
offsetCastKey
inputCastKey
inputSaturationKey
inputColorKey
attributeVibrancyKey
attributeCastKey
_inputCorrections
hueSatLumTable
inputCorrections
setInputCorrections:
convertFromIPT:
selectiveColorKernels
convertToIPT:
iptHueAngleFromRed:green:blue:
colorWithRed:green:blue:alpha:colorSpace:
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
_sampleMode
_rawTime
nu_evaluateWithPipelineState:error:
rawTime
setRawTime:
sampleMode
setSampleMode:
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
initWithKeyframes:stabCropRect:input:
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
_evaluateImage:
_stabilizeImage:cleanRect:cropRect:transform:geometry:
keyframeSequence
setKeyframeSequence:
inputVideoProperties
setInputVideoProperties:
frameDuration
setFrameDuration:
shouldApplyWatermark
setShouldApplyWatermark:
pi_imageByApplyingStabilizationWatermark
vectorWithCGPoint:
inputForKey:
initWithExtent:renderScale:orientation:
setSize:
RGBToYIQKernel
YIQToRGBKernel
whiteBalanceKernel
_strength
_warmth
isDefaultWarmth:
warmth
setWarmth:
setY:
setI:
setQ:
vectorWithX:
submitSynchronous:
setDataExtractor:
isHDRComposition:
_force
_options
force
setForce:
setOptions:
options
internalComposition
initWithRequest:options:
_location
_touchDiameter
initWithComposition:location:touchDiameter:
inputISO
inputAmount
_interpolateGrainKernel
_grainBlendAndMixKernel
_paddedTileKernel
setInputISO:
setInputAmount:
setPerservesAlpha:
writeImage:fileURL:
writeCGImage:fileURL:
writeCGImage:fileURL:options:
writeImage:toTemporaryDirectoryWithBasename:
writeImage:toDirectoryAtPath:withBasename:
stringByAppendingPathComponent:
path
buildNumber
currentSoftwareVersion
_humanFaceBoundsContainmentThreshold
_humanBodyBoundsContainmentThreshold
_humanBodyExpandedBoundsContainmentThreshold
_humanBodyBoundsContainmentCoefficient
_petBoundsContainmentThreshold
_petExpandedBoundsContainmentThreshold
_petBoundsContainmentCoefficient
_facePaddingFactor
_bodyPaddingAmount
_overscanPercentageAllowed
_unwantedSubjectStartingThreshold
_unwantedSubjectInclusionThreshold
_dominantToPeripheralSubjectRatioThreshold
_minimumCorrectionThreshold
humanFaceBoundsContainmentThreshold
setHumanFaceBoundsContainmentThreshold:
humanBodyBoundsContainmentThreshold
setHumanBodyBoundsContainmentThreshold:
humanBodyExpandedBoundsContainmentThreshold
setHumanBodyExpandedBoundsContainmentThreshold:
humanBodyBoundsContainmentCoefficient
setHumanBodyBoundsContainmentCoefficient:
petBoundsContainmentThreshold
setPetBoundsContainmentThreshold:
petExpandedBoundsContainmentThreshold
setPetExpandedBoundsContainmentThreshold:
petBoundsContainmentCoefficient
setPetBoundsContainmentCoefficient:
facePaddingFactor
setFacePaddingFactor:
bodyPaddingAmount
setBodyPaddingAmount:
overscanPercentageAllowed
setOverscanPercentageAllowed:
unwantedSubjectStartingThreshold
setUnwantedSubjectStartingThreshold:
unwantedSubjectInclusionThreshold
setUnwantedSubjectInclusionThreshold:
dominantToPeripheralSubjectRatioThreshold
setDominantToPeripheralSubjectRatioThreshold:
minimumCorrectionThreshold
setMinimumCorrectionThreshold:
_shouldAttemptReframe
_sceneContainsPet
_sceneContainsHuman
_sceneContainsMultipleSubjects
_mutableSubjects
_ruleSystem
_overscanBounds
_viewBounds
initWithConfiguration:expandedSubjectConfiguration:overscanBounds:viewBounds:image:
initWithConfiguration:overscanBounds:viewBounds:subjects:
evaluationData
invalidateCaches
calculateReframedRect
canIncludeSubject:boundsPercentageInside:expandedBoundsPercentageInside:
clamppedSubjectBoundsForEdgeBleed:
candidateRectForSubject:
amountOfOverscanUsedByRect:
shouldAllowCandidateRect:forUnwantedSubjects:
confidenceWithBounds:
overscanBounds
viewBounds
shouldAttemptReframe
mutableSubjects
ruleSystem
sceneContainsPet
setSceneContainsPet:
sceneContainsHuman
setSceneContainsHuman:
sceneContainsMultipleSubjects
setSceneContainsMultipleSubjects:
setStateObject:forKey:
state
keyForSubjectWithIndex:prefix:
evaluate
addRule:
arrayByAddingObjectsFromArray:
sortedArrayUsingComparator:
archivedDataWithRootObject:requiringSecureCoding:error:
subjectDirectionForImageOrientation:
detectedSubjectsForImage:context:
initWithOptions:
edgesKey
session
encodedPixelSize
pixelRect
clapRect
viewRect
processGroup
_frameProvider
_revision
initWithEncodedPixelSize:orientation:clapRect:viewRect:config:
setFrameProvider:
startReframingAtTime:
updateWithTrackedSubjects:atTime:
updateWithEstimatedCameraMotion:atTime:
finishReframingAtTime:
processReframe:completion:
reframedViewRectAtTime:
frameProvider
revision
.cxx_construct
defaultRevision
defaultConfigForRevision:
reframerWithRevision:encodedPixelSize:orientation:clapRect:viewRect:config:
v1Session
debugSceneSegments
debugReframeSegments
debugKeyframes
debugSubjectsAtTime:
debugCameraMotionAtTime:
numberWithLong:
setWithCapacity:
_usedInReframing
_frameDominance
_cameraCorrelation
frameDominance
setFrameDominance:
cameraCorrelation
setCameraCorrelation:
usedInReframing
setUsedInReframing:
_velocity
_acceleration
velocity
setVelocity:
acceleration
setAcceleration:
v2Session
debugRawReframedViewRectAtTime:
debugCenterOfInterestAtTime:
_isVirtualHead
_isVirtualTail
isVirtualHead
setIsVirtualHead:
isVirtualTail
setIsVirtualTail:
ROIForCenterPoint:radius:
convertFloat:toFixed16:count:
convertFixed16:toFloat:count:
processWithInputs:arguments:output:error:
roiForInput:arguments:outputRect:
formatForInputAtIndex:
outputFormat
copyPixelsFromImage:srcRect:destImage:destOrigin:
initWithMutableBuffer:colorSpace:validRegion:
initWithBuffer:colorSpace:validRegion:
initWithSize:format:rowBytes:bytes:
initWithSize:format:rowBytes:mutableBytes:
RGBA16
region
bytesPerRow
baseAddress
inputSpots
applyWithExtent:inputs:arguments:error:
vectorWithCGRect:
pointValue
_aperture
_portraitStrength
_minimumAperture
_maximumAperture
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVerion:depthVersionInfo:
aperture
setAperture:
portraitStrength
setPortraitStrength:
minimumAperture
setMinimumAperture:
maximumAperture
setMaximumAperture:
portraitMajorVersion
setPortraitMajorVersion:
portraitMinorVersion
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
valuesAtCaptureFromImageProperties:error:
portraitLightingEffectStrength
depthBlurEffectSimulatedAperture
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
depthBlurEffectRenderingParameters
depthDataQuality
isDepthDataFiltered
unsignedIntValue
_calculateWithImageProperties:valuesAtCapture:completion:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
isStillImageDisparity:
focusRectDictionaryFromMetadata:
focusRectDictionaryFromRect:
canApplyPortraitEffectsWithMetadata:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
portraitInfoDictionaryFromCameraMetadata:
luminanceNoiseAmplitude
apertureFocalRatio
maximumApertureFocalRatio
minimumApertureFocalRatio
focusRectangle
faceOrientation
objectsAtIndexes:
indexesOfShallowDepthOfFieldObservations
unarchivedObjectOfClasses:fromData:error:
faceObservationsData
roll
faceOrientationIndex
faceJunkinessIndex
allPoints
nose
autoCropFilter
exifOrientationAndCropStraightenOnly
rawFaceBalanceFilter
rawSourceFilterIncludingOrientation
sourceFilterNoOrientation
sushiLevel1Filter
noRedEyeFilter
noTrimFilter
noMuteFilter
noCropFilter
iosCropToolFilter
stripAllTimeAdjustmentsFilter
noGeometryFilter
noOrientationFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
inputToCropFilter
stopAtTagIncludeGeometryFilter:
stopAtTagIncludeOrientationFilter:
applyOrientationFilter
autoloopStabilizedVideoFilter
overcaptureSourceFilter
primarySourceFilter
spatialOvercaptureVideoSourceFilter
oneShotPortraitV2ExportFilter
resetTag:input:error:
socPseudoColorFilter
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
warmFace
setWarmFace:
warmFaceKey
warmTintKey
warmTempKey
tintKey
temperatureKey
grayQKey
grayIKey
grayYKey
grayWarmthKey
grayStrengthKey
faceQKey
faceIKey
faceWarmthKey
faceStrengthKey
colorTypeKey
stringForColorType:
colorTypeForString:
_writeDiagnosticFilesForReframer:metadata:
_createStabilizedKeyframesFromReframer:videoTrack:viewRect:timedMetadata:error:
_createKeyframesFromReframer:videoTrack:viewRect:timedMetadata:error:
initWithKeyframes:confidence:stabCropRect:
UTF8String
writeToURL:error:
lastObject
copyCGImageAtTime:actualTime:error:
setAppliesPreferredTrackTransform:
setRequestedTimeToleranceAfter:
setRequestedTimeToleranceBefore:
setApertureMode:
initWithAsset:
videoOrientationForAssetPreferredTransform:
preferredTransform
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
redEyeSpotsWithCorrectionInfo:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
remapPortraitV2Strength:portraitEffectKind:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
versionForPortraitEffect:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
canRenderDepth
setDepthInfo:
depthInfo
capturedAperture
setGlassesMatteAllowed:
glassesMatteAllowed
glassesMatteAllowedKey
depthInfoKey
apertureKey
copyKeyframesTrimmingToTimeRange:
stabCropRectKey
keyframesKey
indexOfObject:inSortedRange:options:usingComparator:
uniqueInputNode
dictionariesFromPoints:
_defaultCurveArray
autoValuesForBlackPoint:whitePoint:
replaceObjectAtIndex:withObject:
computeCurvesForImageHistogram:
setParameters:
setColorMatrix:
curvePointAtIndex:blackPoint:whitePoint:histogram:
insertObject:atIndex:
@16@0:8
@"CIImage"
v24@0:8@16
v16@0:8
f16@0:8
v20@0:8f16
B16@0:8
@24@0:8@16
B32@0:8@16@24
v24@0:8d16
d16@0:8
q16@0:8
v32@0:8q16d24
v32@0:8@16q24
@"NSNumber"
@"PIReframeSubject"
@"NSMutableSet"
{CGPoint="x"d"y"d}
@36@0:8@16@24f32
@36@0:8{CGPoint=dd}16f32
@36@0:8@16Q24f32
B32@0:8{CGPoint=dd}16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
Q16@0:8
{CGPoint=dd}16@0:8
@28@0:8f16@20
Q32@0:8@16@24
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidModify"B"hasDidModifyMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
@24@0:8^{_NSZone=}16
v32@0:8@16@24
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
v24@0:8q16
#24@0:8@16
@32@0:8@16@24
v20@0:8B16
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@40@0:8@16@24o^@32
B48@0:8@16@24@32o^@40
@32@0:8@16o^@24
@"NSData"
@"NSString"
v24@0:8Q16
@"NSArray"
@"PIExpandedSubjectCalculatorConfiguration"
@"VNSaliencyImageObservation"
@40@0:8@16@24@32
@32@0:8@16^{CGImage=}24
@48@0:8@16@24@32@40
@32@0:8{CGPoint=dd}16
@52@0:8f16{CGRect={CGPoint=dd}{CGSize=dd}}20
v32@0:8@16d24
d40@0:8@16d24^B32
@"<NUScalePolicy>"
v24@0:8@?16
@"PIReframeResult"
B24@0:8o^@16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@"VNSaliencyImageObservation"16@0:8
@"NSArray"16@0:8
@"<NURenderStatistics>"16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8@"Protocol"16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
@"NSString"16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@"NSURL"
@56@0:8@16@24@32@40@48
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
v20@0:8i16
i16@0:8
@?16@0:8
@40@0:8@16@24^@32
{?="value"q"timescale"i"flags"I"epoch"q}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?=qiIq}16@0:8
{?={?=qiIq}{?=qiIq}}16@0:8
v40@0:8{?=qiIq}16
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
q24@0:8@16
@24@0:8q16
{?="columns"[3]}
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
@"NUKeyframeSequenceMatrixFloat33"
{?=[3]}40@0:8{?=qiIq}16
@"NSDictionary"16@0:8
@"AVAsset"
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
@28@0:8@16B24
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@"NUBufferRenderClient"
@"NUImageDataClient"
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
@72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48@56@64
@"NUImageExportRequest"
{?="width"q"height"q}
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageGeometry"
@"NUPriority"
@"NUColorSpace"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
v64@0:8{?={?=qiIq}{?=qiIq}}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@"PIFaceObservationCache"
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
v48@0:8q16^d24^d32^d40
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@32@0:8@16q24
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
@40@0:8@16{CGSize=dd}24
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"NSMutableArray"
@"NSError"
@40@0:8{?=qq}16@32
B40@0:8@16@24o^@32
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
{CGVector="dx"d"dy"d}
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
^{CGColorSpace=}16@0:8
{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@40@0:8{CGPoint=dd}16d32
d32@0:8{CGPoint=dd}16
@"NUFaceDetectionRequest"
@40@0:8@16{?=qq}24
^{CGColorSpace=}
@"NSMutableData"
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
@20@0:8B16
@36@0:8@16B24#28
@36@0:8d16d24B32
@"CIVector"
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
B24@0:8Q16
@32@0:8q16@24
@24@0:8Q16
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
@"PIStillReframerConfiguration"
@"NURuleSystem"
@104@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32{CGRect={CGPoint=dd}{CGSize=dd}}64@96
@96@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56@88
B40@0:8@16^d24^d32
d48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
@32@0:8Q16@24
Q24@0:8q16
{shared_ptr<VRFSession>="__ptr_"^{VRFSession}"__cntrl_"^{__shared_weak_count}}
(CenteredRect=""{?="x"d"y"d"w"d"h"d}""{?="xy""wh"}"xywh")
@112@0:8{?=qq}16q32{?={?=qq}{?=qq}}40{?={?=qq}{?=qq}}72r^(?={?=dd}{?={?=ii}{?=ii}})104
v48@0:8@16{?=qiIq}24
v56@0:8{CGVector=dd}16{?=qiIq}32
{CGRect={CGPoint=dd}{CGSize=dd}}40@0:8{?=qiIq}16
(?={?=dd}{?={?=ii}{?=ii}})24@0:8q16
@120@0:8q16{?=qq}24q40{?={?=qq}{?=qq}}48{?={?=qq}{?=qq}}80r^(?={?=dd}{?={?=ii}{?=ii}})112
r^{Session=^^?{?=qiIq}{?=qiIq}@?{unordered_map<long, std::__1::shared_ptr<VRFTrack>, std::__1::hash<long>, std::__1::equal_to<long>, std::__1::allocator<std::__1::pair<const long, std::__1::shared_ptr<VRFTrack> > > >={__hash_table<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::__unordered_map_hasher<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::hash<long>, true>, std::__1::__unordered_map_equal<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::equal_to<long>, true>, std::__1::allocator<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> > > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::hash<long>, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::equal_to<long>, true> >=f}}}{CatmullRom<double __attribute__((ext_vector_type(2)))>={map<double, double __attribute__((ext_vector_type(2))), std::__1::less<double>, std::__1::allocator<std::__1::pair<const double, double __attribute__((ext_vector_type(2)))> > >={__tree<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::__map_value_compare<double, std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::less<double>, true>, std::__1::allocator<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<double, std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::less<double>, true> >=Q}}}}}
@40@0:8{?=qiIq}16
{CGVector=dd}40@0:8{?=qiIq}16
^{Session=^^?{?=qiIq}{?=qiIq}@?{unordered_map<long, std::__1::shared_ptr<VRFTrack>, std::__1::hash<long>, std::__1::equal_to<long>, std::__1::allocator<std::__1::pair<const long, std::__1::shared_ptr<VRFTrack> > > >={__hash_table<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::__unordered_map_hasher<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::hash<long>, true>, std::__1::__unordered_map_equal<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::equal_to<long>, true>, std::__1::allocator<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> > > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::hash<long>, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<long, std::__1::__hash_value_type<long, std::__1::shared_ptr<VRFTrack> >, std::__1::equal_to<long>, true> >=f}}}{CatmullRom<double __attribute__((ext_vector_type(2)))>={map<double, double __attribute__((ext_vector_type(2))), std::__1::less<double>, std::__1::allocator<std::__1::pair<const double, double __attribute__((ext_vector_type(2)))> > >={__tree<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::__map_value_compare<double, std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::less<double>, true>, std::__1::allocator<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<double, std::__1::__value_type<double, double __attribute__((ext_vector_type(2)))>, std::__1::less<double>, true> >=Q}}}}}
{CGPoint=dd}40@0:8{?=qiIq}16
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
B48@0:8@16@24@32^@40
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
i20@0:8i16
{?="major"i"minor"i}
@44@0:8f16f20f24f28i32{?=ii}36
{?=ii}16@0:8
v24@0:8{?=ii}16
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
B80@0:8@16@24{?={?=qq}{?=qq}}32@64o^@72
@64@0:8@16d24{?={?=qq}{?=qq}}32
@44@0:8@16@24B32o^@36
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
