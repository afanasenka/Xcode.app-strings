SQLite format 3
@(#)PROGRAM:MobileSpotlightIndex  PROJECT:Matador-2148
kMDItemRelatedUniqueIdentifier
journalRepair.1
 !"#$%&'()*+,-./0123456789:;<=>?@abcdefghijklmnopqrstuvwxyz[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`ABCDEFGHIJKLMNOPQRSTUVWXYZ{|}~
FV
""#"$"%"&"<"A"C"D"E"G"H"I"M"m"a"b"d"p"e"q"r"t"s"u"v"x"w"y"z"
FH
0K0L0M0N0O0P0Q0R0S0T0U0V0W0X0Y0Z0[0\0]0^0_0`0a0b0d0e0f0g0h0i0o0p0r0s0u0v0x0y0{0|0
0o0q0r0t0u0w0x0z0{0}0
`!p!a!q!b!r!c!s!d!t!e!u!f!v!g!w!h!x!i!y!j!z!k!{!l!|!m!}!n!~!o!
!p!`!q!a!r!b!s!c!t!d!u!e!v!f!w!g!x!h!y!i!z!j!{!k!|!l!}!m!~!n!
ABCDEFGHIJKLMNOPQRSTUVWXYZ
0123456789abcdef
tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt7
tttttttttttt
ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
?333333
?333333
?ffffff
?333333
?333333
?ffffff

InRange(_kMDItemGroupId,2,2)
InRange(_kMDItemGroupId,1,1)
InRange(_kMDItemGroupId,3,3)
InRange(_kMDItemGroupId,4,4)
InRange(_kMDItemGroupId,5,5)
InRange(_kMDItemGroupId,6,6)
InRange(_kMDItemGroupId,7,7)
InRange(_kMDItemGroupId,8,8)
InRange(_kMDItemGroupId,9,9)
InRange(_kMDItemGroupId,10,10)
InRange(_kMDItemGroupId,11,11)
InRange(_kMDItemGroupId,12,12)
InRange(_kMDItemGroupId,13,13)
InRange(_kMDItemGroupId,14,14)
InRange(_kMDItemGroupId,15,15)
InRange(_kMDItemGroupId,16,16)
InRange(_kMDItemGroupId,18,18)
bcdefghijk#lm$%&'()*+,-nopqrst.02468:<>@BDFHJLNPRTVXZ\^`uvwxyz/13579;=?ACEGIKMOQSUWY[]_a{|}~
-0123456789AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz#$%&'()*+,./:;<=>?@[\]^_`{|}~
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
 !"#$%&'()*+,-./0123456789:;<=>?@abcdefghijklmnopqrstuvwxyz[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
!%-MASTER-FID-%!
get_persistent_id_store
release_persistent_id_store
_sqlite_insert
_sqlite_bulkEnd
sync_persistent_id_store
_psid_insert_locked
remove_path_locked
rename_path
.migratedphotolibrary/
.migratedaplibrary/
.photolibrary/
.aplibrary/
.photoslibrary/
psid.db
FileTree_Overlay.c
fileId >2
FileTree_UpdateSet.c
directory->fileid>=2
depth >= 0
%s:%u: failed assertion '%s' %s got file id %lld
newDirectory.fileid >2
newDirectory.fileid != directory->fileid
directory->children->nodes[slot].fileid<=1
dir==0
directory->fileid
directory->children->childCount < directory->children->pageSize-1
parents[i] > 0
%s:%u: failed assertion '%s' %s Got parent with id %lld
(uint64_t)parents[i-1] > 2
directory->children->nodes[slot].fileid==p1[0]
(uint32_t)outChildren == (uint32_t)root->children->childCount
lastPosting
uniqued
getFlagsFromAttributes
multivalued
nosearch
Attributes
SIIndexInternals.cpp
fieldName.ptr()
setDatastoreLocalizedAttributes
((flags & DB_FIELD_EXTENDED_ATTR) && db_corespotlight_store(sdb)) == 0
:EA:
:PR:
_kMD
:INC:
_kMDItemIncomingCounts
_kMDItemIncomingMailCounts
_kMDItemIncomingVideoCallDates
_kMDItemOutgoingVideoCallDates
_kMDItemIncomingAudioCallDates
_kMDItemOutgoingAudioCallDates
_kMDItemSizingIsNeeded
kMDItemContentType!='com.apple.ical.ics.todo' || _kMDItemFinderExcluded!=1
_kMDItemGroupId!=6 || (kMDItemContentType=='com.apple.ical.ics.todo' || _kMDItemFinderExcluded!=1)
count < (CFIndex)4294967295U && count>=0
tmpB
SIQuery.cpp
qp->_free_cache_data==(void*)ContentIndexQueryNodeDispose
SIUINT32Set
newLevel->children[i]==0
v==key
popped<peeked
popped==peeked
SIUINT64Set
com.apple.MobileSMS
com_apple_mail_read
com_apple_mail_flagged
com_apple_mail_flagColor
com.apple.searchstressattr
_kMDItemStateInfo_com.apple.searchstressattr.state.test
com.apple.mobilemail
com.apple.mobilenotes
_kMDItemStateInfo_com.apple.mobilemail.contentIndex
com_apple_mobilemail_transaction
kSIRepairedIndex
kSIConsistencyCheck
kSITokenizerUseCRF
kSITokenizerVersion
kSITokenizerUnigrams
kSIRepairSizes
kSIIdentifierHashVersion
activityJournal
tmp.Cab
Cab.created
tmp.Lion
Lion.created
com.apple.SpotlightServer
kMDItemUserTags
kMDItemSupportFileType
_kMDItemSnippet
_kMDItemStorageSize
_kMDItemPersonaID
Shutdown
Preheat
Throttled Volume Query
Volume Query
Throttled Index Query
Utility Index Query
Index Query
Fast Index Query
Set Attributes
Flush
Compaction
Background Helper
Helper
v24@?0^{__CFArray=}8q16
kMDStoreUUID
si_create_indexmetadata
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/index/SpotlightIndex/SpotlightIndex.c
void syncIndex(si_sync_ctx *, Boolean)
sync
unknown
sync err
store.db.recover
.store.db.recover
/store.db.recoverStr-%d.map.header
/store.db.recoverStr-%d.map.offsets
/store.db.recoverStr-%d.map.data
/store.db.recoverStr-%d.map.buckets
syncIndex
notify_lowspace
com.apple.Spotlight.lowdiskspace
DeviceNumber
kMDSIndexDeferSyncCount
kMDSIndexSyncCount
ConsumedJournalSerialNumber
JournalSerialNumber
GroupAssignments
database.recoverscantime
si_set_property
kMDStoreProperties
si_create_propertydict
kIndexRemappingData
kIndexRemappings
kIndexRemappingIndex
kMDStoreAccumulatedCounts
si_storesizes
si_store_propery_cache
dirty
fast flush
needs shadow
clean
SpotlightIndex.c
!ctx->syncDirtyChunks
[%p] %s
post_shadow_datastore
!(s->state==kSIIndexStateNeedsShadow && state==kSIIndexStateDirty)
!(s->state==kSIIndexStateDirty && state==kSIIndexStateClean)
s->sdb_state!=kSIIndexStateNeedsShadow
s->directory_state!=kSIIndexStateNeedsShadow
tmp.spotlight.state
si_write_index_state
_si_dump_index_state
shadow_datastore
commit_sync_datastore
sync_datastore
tmp.spotlight.loc
ctx->syncSet && ctx->liveSet
dirStore.overlay
v32@?0i8^q12i20^q24
si_get_object_for_identifier_createParentDBO
_kMDItemBundleID
_kMDItemDomainIdentifier
_si_get_object_for_identifier_createParentDBO
com.apple.spotlight.missingparent
void SIInitialIndexingEnded(SIRef, int32_t)
void si_initialIndexingEndedQueueOnHold(si_sync_ctx *, Boolean)
void si_initialIndexingEndedQueueOnSet(si_sync_ctx *, Boolean)
ContentIndexWritable(indexSet->index[indexSet->currentIndex])
void si_initialIndexingEndedQueueOnFlush(si_sync_ctx *, Boolean)
int32_t SISetScanCount(SIRef, CFIndex, _Bool)
oldLiveSet->indexCount==0 || (CFIndex)ContentIndexBaseDocId(oldLiveSet->index[0]) >= count
%s:%u: failed assertion '%s' %s Expected current index to be writable. {%u of %u}. %s
indexSet->currentIndex==~0 || ((uint32_t)indexSet->indexCount > (uint32_t)indexSet->currentIndex && ContentIndexWritable(indexSet->index[indexSet->currentIndex]))
live
SISetScanCount
void si_set_scan_count(void *, Boolean)
si_set_scan_count
%s:%u: failed assertion '%s' %s No writable index available for %s
(ci_rc != ContentIndex_OpenedNew) || (indexSet->currentIndex==~0) || ContentIndexWritable(indexSet->index[indexSet->currentIndex])
!ref->liveSet || ref->liveSet->currentIndex==~0 || ContentIndexWritable(ref->liveSet->index[ref->liveSet->currentIndex])
could not create live index
state==kSIIndexStateDirty
live.0.
Failure in db_shrink_cache at si_initialIndexingEnded
void si_initialIndexingEnded(si_sync_ctx *, Boolean)
void setupAndIssueMerge(SIRef, uint32_t, _Bool, xpc_activity_t, _Bool *, dispatch_group_t)
void si_mergeIndex(void *, Boolean)
Partial
Full
Normal
Vacuum
Voluntary
Forced
SetupDeferQueue
start+count <= oldIndexSet->indexCount
oldIndexSet->indexCount+(mergeCount-1) >= preCount
!ref->workqueues.queues[SI_DEFER_QUEUE_IDX]
DarkMerge
Index merge in dark wake
si_mergeIndex
com.apple.spotlight.mds.index-darkwake
ref->workqueues.queues[SI_DEFER_QUEUE_IDX]
OuterMerge
tmp.merge.%ld.
InnerMerge
merge
com.apple.spotlightindex.%s.%s.%d
SetupRemapping
ref->liveSet->currentIndex==~0 || ((uint32_t)ref->liveSet->indexCount > (uint32_t)ref->liveSet->currentIndex && ContentIndexWritable(ref->liveSet->index[ref->liveSet->currentIndex]))
ref->syncSet->currentIndex==~0 || ((uint32_t)ref->syncSet->indexCount > (uint32_t)ref->syncSet->currentIndex && ContentIndexWritable(ref->syncSet->index[ref->syncSet->currentIndex]))
scan
ContentIndexGetId(oldIndexSet->index[start])==anchor
si_swapIndexSet(ref, oldIndexSet, indexSet, indexSetPtr,1)
Failure in db_shrink_cache at si_remapForIndex
indexContactGraphData
setCSAttributes2
deletecount
indexingcount
datasize
indexingtime
com.apple.searchd.indexingtime.%@
com.apple.searchd.datasize.%@
processOneCS
dbop && *dbop
_kMDItemRelatedObjectsWithBundle
(fieldFlags & (DB_FIELD_UNIQUED_VALS|DB_FIELD_LOCALIZED_STR)) == 0
kMDItemDocumentIdentifier
identifierCStrSize
_kMDItemRelatedActivityLaunchCount
.lproj
kMDItemExpirationDate
_kMDItemInterestingDate
kMDItemInterestingDate_Ranking
__fpdefault/
_Bool processOneCS(SIRef, int64_t, CFStringRef, CFStringRef, int, MDPlistObject, MDPlistObject, CFStringRef, size_t, CFAllocatorRef, _Bool)
kMDItemAuthorEmailAddresses
kMDItemContentCreationDate
kMDItemRecipientContactIdentifiers
kMDItemAuthorContactIdentifiers
_kMDItemOutgoingSMSCounts
_kMDItemIncomingSMSCounts
_kMDItemOutgoingMailCounts
_kMDItemOutgoingCounts
%@ %@
%@%@
__fp/
%s%@
dyn.
public.data
:EA:_kMDItemUserTags
Marker
kMDItemArtist
kMDItemDisplayName
public.text
DeviceId
kMDItemWorkerHandled
kMDPreviewImageData
kMDItemSeedLastUsedDate
kMDItemApproximateModTime
_kMDItemTimeMachineMarkerNeedsFixup
_kMDItemSupportFileType
_kMDItemBackupNameSpace
_kTimeMachineOldestSnapshot
_kTimeMachineNewestSnapshot
:MD:kMDItemIsUploaded
kMDItemIsUploaded
:MD:kMDItemIsUploading
kMDItemIsUploading
:MD:
kMDItemPlayCount
_kMDItemRelatedActivityLastLaunchDate
kMDItemDateAdded_Ranking
kMDItemDateAdded
kMDItemDueDate_Ranking
kMDItemDueDate
kMDItemCompletionDate_Ranking
kMDItemCompletionDate
kMDItemStartDate_Ranking
kMDItemStartDate
_kMDItemApplicationLastLaunchedDate_Ranking
_kMDItemApplicationLastLaunchedDate
kMDItemContentModificationDate_Ranking
kMDItemContentCreationDate_Ranking
kMDItemMailDateLastViewed_Ranking
com_apple_mail_dateLastViewed
kMDItemMailDateReceived_Ranking
com_apple_mail_dateReceived
kMDItemAttributeChangeDate
_kMDItemTimeMachinePath
kMDItemThumbnailData
kMDItemHiddenAdditionalRecipientContactIdentifiers
kMDItemAdditionalRecipientContactIdentifiers
kMDItemPrimaryRecipientContactIdentifiers
kMDItemHiddenAdditionalRecipients
kMDItemAdditionalRecipients
kMDItemPrimaryRecipients
_kMDItemIsFromImporter
com_apple_metadata_modtime
com.apple.Music
indexSet->indexCount
indexSet->index[indexSet->indexCount-1]
ContentIndexValidIndex(indexSet->index[indexSet->indexCount-1])
base
v32@?0^{__CFString=}8[1024c]16Q24
v16@?0^{__CFDictionary=}8
LSItemContentTypes
CFBundleTypeName
CFBundleDocumentTypes
public.item
kMDItemKind
_si_delete_attributes_inner
FPParentFileItemID
_kMDItemEngagementDate
_kMDItemRenderDate
com.apple.spotlight.category
_kMDItemClientBundleID
_kMDItemClientExternalID
_kMDItemRelatedBundleID
_kMDItemHasClientData
kMDItemRelatedUniqueIdentifier
_kMDItemUserActivityRequiredString
kMDItemFileItemID
kMDItemFileProviderID
kMDItemPhysicalSize
v40@?0r*8Q16{?=*{?=IC}}24
__class:
CSLocalizedString
v24@?0{?=*{?=IC}}8
_kMDItemStateInfo_%@
setCSAttributes2_block_invoke_4
setCSAttributes2_block_invoke_2
com.apple.searchd.indexingcount
com.apple.searchd.indexingcount.%@
com.apple.searchd.deletecount
com.apple.searchd.deletecount.%@
com.apple.FileProvider
com.apple.CloudDocs.MobileDocumentsFileProvider
indexFd != -1
syncCount < 2147483647
%s%d
deferAttr.
com.apple.MobileAddressBook
:INC:_kMDItemIncomingCounts
:INC:_kMDItemOutgoingCounts
:INC:_kMDItemIncomingMailCounts
:INC:_kMDItemOutgoingMailCounts
com.apple.spotlight.mds.index-lifecycle
noindex
notokenize
void si_sync_index_delayed0(si_sync_ctx *, Boolean)
void si_sync_index_delayed1(si_sync_ctx *, Boolean)
void si_sync_index_delayed2(si_sync_ctx *, Boolean)
si_recycleForBadIndex
indexSet2->currentIndex==~0
!ContentIndexWritable(indexSet2->index[i])
si_swapIndexSet(ref,oldIndexSet2,indexSet2,&ref->syncSet,0)
si_swapIndexSet(ref,oldIndexSet1,indexSet1,&ref->liveSet,1)
Unknown
ExternalVacuum
Deletes
Count
ExternalForce
ExternalInitialIndexingEnd
ExternalScanEnd
HoldCount
UpdateCount
InitialIndexingEnded
void si_scanEnded(si_sync_ctx *, Boolean)
void setupAndIssueMergeScan(SIRef, int32_t, _Bool, xpc_activity_t, _Bool *, dispatch_group_t)
ScanEnded
!ContentIndexWritable(indexSet->index[i-1])
indexLiveSet->currentIndex==~0 || ContentIndexWritable(indexLiveSet->index[indexLiveSet->currentIndex])
ContentIndexValidIndex(cindex)
ContentIndexWritable(cindex)
si_getSyncIndex
si_writeBackDBO
callbacks
SICreateNewIndex
_kMDXXXX___DUMMY
_SIShutdownIndex
com.apple.spotlight.index.shutdown.shortlived
fs_only || newIndex->dirStore
SIInitIndex
%s scheduler for index at %s
newIndex->workqueues.schedulers[schedId]==0
newIndex->workqueues.queues[queueId]==0
%s scheduler for spindle %d
0 <= count && count < pathBufferSize
fs_only||newIndex->dirStore
process terminating
rebuild for tokenizer
open persistent id store error
database.shutdowntime
YukonRecomputedSizes
journalFdPtr
!(sourceOid==destPath[i] || destPath[0] == destPath[i])
moveDirectoriesInner
dbo->parent_oid == directoryStoreGetParent(ref->dirStore, dbo->oid)
%s:%u: failed assertion '%s' %s Got parent id %lld for oid %lld
dbo->parent_oid>=2 || dbo->parent_oid==-1
sourcePath[0]>=2
destPath[i]>=2
processOneChildlessDirectory
failed
parent===1
dbo->parent_oid == parent
dbo->parent_oid==0
handleMovingContent
_SIResolveDirectory
i<=ctx->count
i+advanceLen<=ctx->count
processOneFile
CFArrayGetValueAtIndex(inValues,i) == valueArray[i+1]
valueArray[0]==kCFNull
setAttributesBulk_block_invoke
%s:%u: failed assertion '%s' %s src: %d id: %d oid: %lld parent: %lld options: %x extra: %p
(CFTypeRef)ctx->attrdict!=(CFTypeRef)kCFNull
_setAttributes
processing oid: %lld source: %d %s
processing oid: %lld source: %d
_kMDItemBackupMoveMarker
_kMDItemUserTags
ctx->attrdict==((void*)0)
void _setAttributes(si_set_attr_ctx *, _Bool, Boolean)
:MD:_kMDItemBackupMoveMarker
:MD:kMDItemPath
indexing: %s
_kMDItemDeleted
journal replay
preparseMobileJournal
playBackMobileJournal
deleteCSAttributes
com.apple.searchd.indexingtime
deleteCSAttributes_block_invoke_2
 canceled
setCSAttributes1
setCSAttributes1_block_invoke_3
setCSAttributes1_block_invoke_2
si_getsizes
LOW_LATENCY
_kMDItemOwnerUserID
kMDItemWhereFroms
_SIInitSDB
i12@?0B8
!ref->uniqueLocalizedTerms
_si_init_localized_terms
database.localizedtermsuuid
com.apple.spotlight.index.free
guarded_dup
SIIndexingMallocZone
invalid path
Rebuilding index
_SIOpenIndex
Rebuilding index because of repeated crashes
open meta info error
_SIOpenIndexFilesWithState
!fast_flush_failed
newIndex->syncSet->indexCount==0
newIndex->liveSet->indexCount==0
s->directory_state!=kSIIndexStateFastFlush
db_corespotlight_store(newIndex->store) || version == 99 || version == 100
scanCount == newIndex->syncSet->indexCount
liveCount == newIndex->liveSet->indexCount
handleDirStoreOverlay failed
recover datastore error
invalid datastore
ds dirty, rs needs shadow 
ds needs shadow, rs dirty
invalid reverse store
restore db dirty pages failed
restore rs dirty pages failed
invalid index version recovering
invalid index version reindexing
.store.db
open index error
invalid term update set from lion
open dirstore failed
open datastore failed
index setup error
init index error
index not created with unigrams error
missing system dbo
void si_compactReadOnlyIndexes1(void *, Boolean)
void si_compactReadOnlyIndexes2(void *, Boolean)
void si_compactReadOnlyIndexes3(void *, Boolean)
void holdAndIssueMerge(SIRef, int32_t, SIIndexSetRef, _Bool, _Bool)
tmp.
tmp.SnowLeopard
tmp.spotlight
tmp.store.recovery
tmp.journals.
new_path[ptr-direntry->d_name-4]=='.'
si_handle_tmp_files
handleIndexRepair
_kMDItemWillModify
si_read_index_state
com.apple.searchd.indexes.uncompacted.merge.immediate.count
v12@?0C8
com.apple.searchd.indexes.uncompacted.merge.scheduled.count
IndexOpenCompact
com.apple.searchd.indexes.merge.scheduled.count
void setupAndIssueMergeCleanup(SIRef, int32_t, _Bool, xpc_activity_t, _Bool *, dispatch_group_t)
com.apple.searchd.indexes.merge.immediate.count
com.apple.searchd.indexes.count
si_repair_sizes
si_repair_sizes_block_invoke
Failure in db_shrink_cache at si_index_inactivate
doFastFlushIndex
si_setstorecookie
indexSpindleId==si_indices[0]->indexSpindleId
_SIShutdownSetup
fastflush
kIndexCheckDupOids
recovery
void si_sync_index_delayed_if_dirty0(si_sync_ctx *, Boolean)
void si_sync_index_delayed_if_dirty(si_sync_ctx *, Boolean)
void _SIContinueIssueMerge0(void *, Boolean)
void _SIContinueIssueMerge(void *, Boolean)
 (on battery)
indexCount <= indexSet->indexCount
void _SIContinueIssueMerge2(void *, Boolean)
Merge(2)
void _SIContinueIssueMerge2(void *, Boolean)_block_invoke
flush err
_SIFlushAndSyncIndex
PermissionCache
Validate
Permission cache in dark wake
void si_bulk_delete_attributes(si_bulk_delete_ctx *, Boolean)
SIGetLargestOid
v16@?0@?<v@?B>8
Failure in db_shrink_cache at setIndexingCaughtUp
SISetCSAttributes
mobile_journal
finishRegisterQuery
int32_t SIGetMaxTransactionID(SIRef)
SIGetMaxTransactionID
SISetTransactionCount
prepareForTransaction
void _SIIssueFullMergeWithGroup(SIRef, dispatch_group_t)
void _SIIssueMerge(SIRef, int)
_si_merge_for_badness_on_flush_queue
_si_merge_for_badness_on_compact_queue
void _si_merge_for_badness_on_compact_queue(void *, Boolean)
com.apple.spotlightindex.verify
si_verify
verify err
si_CleanupCommit
_SIReverseStoreIterate
_SIDirectoryStoreIterate
>>>>>>>>
<<<<<<<<
Scheduler state:
_SIIssueSchedulerDump
low disk space
_kMDItemImporterResult
(count & 0x7 ) == 0
i<=count/8
SICopyCorrections
com.apple.lastuseddate#PS
LISearchFileNameContains
LISearchObjTypeReturnAll
kMDItemIsTrashed
compute_transitions
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/index/ContentIndex/transition_table.mm
it != unprocessed.end()
store_stream_init_fd
store_stream_flush
kSISchedulerQOSClass
SIScheduler.c
queue
%s:%u: failed assertion '%s' %s Bad CRC on work unit. %p %p %p %p %p %p
cu->u.crc==compute_workunit_crc(cu->u)
work_fun
root
child->parent==scheduler
MQ: %s
com.apple.metadata.spotlightindex.mq.%s
com.apple.metadata.spotlightindex.%s
scheduler->suspended
scheduler->running==0
scheduler->force_resumed == 1 || scheduler->suspended==0
%sScheduler %p %s dq:%p parent:%p; %d suspensions suspended:%s (stop waiting: %s stopped: %s)
%sWork queue %p; dq: %p %ld items enqueued
SISimpleQueue.c
queue->end == (queue->end&(queue->size-1))
queue->start == (queue->start&(queue->size-1))
simple queue
destroyed simple queue
%s:%u: failed assertion '%s' %s Expected valid queue entry type. Got %d
SIResultQueue.c
queue->lowWaterMark!=0
d156@?0^{?=^SQ}8Q16Q24d32d40{ci_rankingbits_s=TI}48q80i88i92i96i100i104B108B112B116i120C124C128d132^{_SIWordTrieFragmentBundleIDs=}140^v148
SICompletions.cpp
depth < 20
%s:%u: failed assertion '%s' %s No query work queue for priority %d
SIJob.c
ref->workqueues.queues[SI_QUERY_QUEUE_IDX+priority]!=0
ref->workqueues.queues[SI_FS_QUEUE_IDX]!=0
com.apple.spotlight
RootDirectory
General
Query
LiveQuery
Scheduler
Store
Fetch
Path
State
Power
Completions
com.apple.spotlightindex
BurstTrie.c
bt_openTrie
bt_recoverTrie
getNum(t->baseFat[s].termInfo.termId.ptr)==0
BurstTrie.h
ptrM(newptr).next == ptr.next
getNum(b->termInfo.termId.ptr)==0
flatStoreGetOffset(ptr) < storageGetCount(&t->flatStore)
oldPtr.kind == FLAT
%s:%u: failed assertion '%s' %s %d < %d
BurstTrie-Internal.h
b->size
offset<=0x3FFFFFFF
offset<t->bases.size
%s:%u: failed assertion '%s' %s s: %d, c: %d
ptr.next<=getNum(t->baseFatCount)
bt_shadowTrie
shadowFlatStore
ptr.kind
ptr.next
ptr.kind || ms->err
*string
old==s
ms->err
j<=count
z<=count
j+z<=count
(flatStoreGetOffset(range[k].info) >= flatStoreGetOffset(range[k-1].info) && range[k-1].info.next !=0) || range[k].info.next==0
TrieMergeUpdates
%s:%u: failed assertion '%s' %s termId: %d next_term_id: %d
termInfo->tu->termId < positions->next_term_id
%s:%u: failed assertion '%s' %s termId: %d prevTermId: %d j: %ld count: %ld
termId > prevTermId || (termId == prevTermId && j == count)
node->offset
_dumpTrie
s<=getNum(t->baseFatCount)
!(offset>flatPagePtr && offset<flatPageEnd)
%s:%u: failed assertion '%s' %s next: %d, max: %d
ptr.next < ctx->trie_fat_max
ptr.next < ctx->trie_max
ctx->la
match_function
_bt_findBulk_block_invoke_2
v16@?0^{storage_reader_t=^{storage_t}^{storage_reader_window_s}Bi}8
child.next < ctx->trie_fat_max
child.next < ctx->trie_max
%s:%u: failed assertion '%s' %s invalid len
str_len
v36@?0{?=Ii}8^{lt_trie_node=*^^{lt_trie_node}CCC}16*24i32
v28@?0{?=Ii}8*16i24
ch_start==0
%s:%u: failed assertion '%s' %s max depth exceeded: %d
ctx->stringLen<CI_UTF8CHARS_BUFFER_SIZE
v12@?0{TokenRange=ss}8
i24@?0^{FindTermIDsContext=^B^{_TrieHead}^{ForwardDirectoryStore_s}^{FileTree_Overlay_s}^^{FlatPageSearchBucket}i{?=Ii}QQQ[1052C]I^{QueryNode}I^^{?}^^{?}III^vII^{AllocSlab}I**^{QueryNode}^{?}^{?}^{levenstein_automaton}^{automaton_state}ifBB}8*16
match 
v16@?0^{FindTermIDsContext=^B^{_TrieHead}^{ForwardDirectoryStore_s}^{FileTree_Overlay_s}^^{FlatPageSearchBucket}i{?=Ii}QQQ[1052C]I^{QueryNode}I^^{?}^^{?}III^vII^{AllocSlab}I**^{QueryNode}^{?}^{?}^{levenstein_automaton}^{automaton_state}ifBB}8
pushMove
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2148/spotlight/generic/MoveHolder.c
holder->count==0 || holder->count + holder->data == ((char*)current) + sizeof(PossibleFileMoves_t)+sizeof(oid_t)*current->count
_CISetEmergency
ContentIndex.c
(!baseId) || newIndex->base==baseId
copyVolumeInfoStr
(%s, t: 0x%x, st: 0x%x, f: 0x%x)
_CIUpdateState
%sindexHead
open canceled
No meta info
invalid meta info
success: no data in index, rebuilding
create index error
_CIOpenBulk
needs recovery
recover index error
open recovered index error
open index shadow error
_ContentIndexSyncIndexBulk
_CISyncContextDestroy
indexShadowAndCommitBulk
_indexCommitShadowBulk
_CISyncContextShadow
_indexShadowBulk
_CISyncContextCommitHeader
_CISyncContextCommitData
io_state[i]==0
indexCommitSyncBulk
_CISyncContextSync
indexPerformSyncBulk
indexPrepareForSyncBulk
gatherAndLockIndexCallback
_CIFlushCache
_CICompact
tmp.can_write.%d.%d
ContentIndexVerifyIndex
_CIDisableUpdates
%s_%d.
_CIMergeDeletes
newDocId > newBase && newDocId <= newMax
((newIndex)->coreSpotlight?({ ((uint8_t *)(newIndex)->groups)[(docID)]; }):({ uint32_t __where=(uint32_t)(docID); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((newIndex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
_MergeDeletes
_CIUpdateContent
((cindex)->coreSpotlight?({ ((uint8_t *)(cindex)->groups)[(relDocId)]; }):({ uint32_t __where=(uint32_t)(relDocId); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((cindex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
_CIUpdateGroupAndDate
_CIDelete
_CIReassign
ref->payloadCount <= 1
_CIRebaseDocId
_CIDocIdForOID
(ref)->groupMap[gslot]
((ref)->coreSpotlight?({ ((uint8_t *)(ref)->groups)[(i)]; }):({ uint32_t __where=(uint32_t)(i); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((ref)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
_CIDeleteDuplicates
ref->readOnly
ref->payloadMaxCount >= ref->payloadCount
_CIFindTokens
_CIPreHeatIndex
_CIGetOIDForDocId
_CIGetGroupForDocId
_CIAddOids
((cindex)->coreSpotlight?({ ((uint8_t *)(cindex)->groups)[(i)]; }):({ uint32_t __where=(uint32_t)(i); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((cindex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
_CIDeleteByOidsBulk
_CIMoveDirectory
expandMap
expandUnsafeMapNew
%s:%u: failed assertion '%s' %s offset: %lld, freeRegion: %lld, kind:%d
FlatStore.c
flatStoreGetOffset(ptr)+roundup2(pageEnd+int_sizeof(*page) <= freeRegion)
%s:%u: failed assertion '%s' %s ps:%d, pe:%d, pk:%d, po:%llx, ss:%llx se:%llx
pageSize >= pageEnd && pageSize && (__builtin_popcount(pageSize+int_sizeof(*page)) == 1)
%s:%u: failed assertion '%s' %s %ld, ps:%d, pe:%d, pk:%d, po:%llx, ss:%llx se:%llx
ms->currentStringLen <= CMPBUFFER_SIZE
%s:%u: failed assertion '%s' %s flat store
ms->pageOffset.next==_ptr.next
ptr.kind==FLAT
flatStoreGetOffset(ptr) < freeRegion
ms->type == kTermInfoTypeId
oldEntry.pfxLen + oldEntry.len <= CMPBUFFER_SIZE
newEntry.len
(int)pageEnd==storePageEnd(page)
pageEnd<pageSize
pageEnd < pageSize
pageEnd + newEntrySize <= pageSize
termID == getNum(newEntry.termInfo.termInfo.termId.ptr)
i < entry.len
i<=newEntry.len
entry.len > 0
entry.len==entryCopy.len
entry.pfxLen==entryCopy.pfxLen
len2 <= len1
storePageEnd(page) <= storePageDataSize(page)
flatStoreGetOffset(info) < storageGetCount(store)
reSize<=4096*16
flatStoreGetOffset(*newOffset) < storageGetCount(store)
%s:%u: failed assertion '%s' %s len:%d cursor:%d, pe:%d, ps:%d, valid cursor:%s
FlatStore.h
entry->len > 0 || pageCursor == v2_vInt32Size(0)
pageCursor<=pageEnd
_checkFlatPage
entry.len
oldPageEnd == iter.pageEnd
iter.pageCursor==iter.pageEnd
entry->len > 0 || iter->pageCursor == v2_vInt32Size(0)
iter->pageCursor <= iter->pageEnd
pageSize > 0
__builtin_popcount(pageSize+(uint32_t)sizeof(*page)) == 1
pageEnd <= pageSize
!compressed
(!compacted && t->type==kTermInfoTypeId) || (compacted && t->type!=kTermInfoTypeId)
*canceled || iter.pageCursor==iter.pageEnd
_indexEmergencyUnmap
index docId array
index date array
index group array
bases
fat bases
flat
termsIds
position header table
log_map_access_error_mini
log_map_access_error
JHContentIndex.c
index_FlushCache
%s:%u: failed assertion '%s' %s Expected cindex->_oldSet==0, got %p
cindex->_oldSet==0
indexUpdates
indexFindBulk
*canceled || pqcount_bulk_TermIdQueue_t(q_pqueue) == 0
!((renamed != cindex->trie.renamed) || (!isCompact && (cindex->flags & kIndexFlagCompact)))
%s:%u: failed assertion '%s' %s failed to read offset for term %d
TermIdStore.h
(intptr_t)ptr!=-1
parentDirFd!=-1
indexHead
indexIds
indexDates
indexBigDates
indexGroups
shadowIndexHead
shadowIndexGroups
shadowIndexTermIds
shadowIndexPositionTable
indexPostings
indexTermIds
indexPositions
indexPositionTable
openPayload
open_index_file
indexRestoreFromBuffer
indexRestoreHeaderFromBuffer
openIndex
invalid term update set
restoring term update set failed
len > 0 && len <= 32
indexShadowFiles
0 == TermUpdateSetTermCount(cindex->_deltaSet)
_indexShadowGroups
index_verify
verifyTermsCallback
indexMarkDirtyForce
indexMarkDirty
indexMakeInvalid
indexPrepareForSync
indexCommitSync
fullShadowIndex
indexDirectory
shadowIndexDirectory
indexCompactDirectory
shadowIndexCompactDirectory
indexArrays
shadowIndexArrays
recoverIndex
setDocumentAttributes
(cindex)->groupMap[gslot]
((cindex)->coreSpotlight?({ ((uint8_t *)(cindex)->groups)[((uint32_t)(oldDocID-cindex->base))]; }):({ uint32_t __where=(uint32_t)((uint32_t)(oldDocID-cindex->base)); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((cindex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
v60@?0^{TermBurstTrie={?=AI}^{AllocSlab}{TermTrieFat=^{tree_node}[256q]}[4{?=^vq}][256C]}8^{TermTrieFat=^{tree_node}[256q]}16B24^AI28@?<v@?^{tree_node={?=b63b1}{?=b63b1}{TermUpdate=(?={?={RelativePosting=I{?=II}}I}{?={UpdatePosting=I{?=IQ}}})S[0C]}}>36@?<v@?^vi>44@?<v@?^vi>52
%s:%u: failed assertion '%s' %s Expected non-zero docid
docid
((cindex)->coreSpotlight?({ ((uint8_t *)(cindex)->groups)[(docid)]; }):({ uint32_t __where=(uint32_t)(docid); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((cindex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
indexGrowDocumentPayloads
deleteDocument
((cindex)->coreSpotlight?({ ((uint8_t *)(cindex)->groups)[(payloadIndex)]; }):({ uint32_t __where=(uint32_t)(payloadIndex); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((cindex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
reassignDocument
index_compact
tmp.%scmpt.
newTermIDMap
%s:%u: failed assertion '%s' %s missing positions for term %d (%d)
%s:%u: failed assertion '%s' %s inconsistent term counts (%d %d)
%s:%u: failed assertion '%s' %s no posting for term (%s %s)
%s:%u: failed assertion '%s' %s duplicate term id (%d)
0 == uint32_map_get(ctx->newTermIds, (uint32_t)termId)
0 == uint32_map_get(ctx->newTermIds, termId)
termIdStore
positions header table
preflight_compact
%s:%u: failed assertion '%s' %s invalid rename %s %s
newPrefix[0]!='l'
initPayload
expandPayload
JHPayload.c
wb->buffer
pd->payloadIndex >= pd->payloadLimit
PayloadWriteBufferWrite
pd->payloadIndex - pd->payloadLimit >= sizeof(PulseHeaderDisk)
JHPayload.h
src == ptr || wb->err
pos == pd->payloadIndex
%s:%u: failed assertion '%s' %s i:%d count:%d last:%d termId:%d prevTermId:%d flags:%x
(termId > prevTermId) || (i == last && termId == prevTermId)
postingStart-postingTermIdStart
initMF
%s:%u: failed assertion '%s' %s NULL file reference
MFMalloc.c
m->fdPtr
Watchdog workloop
Watchdog timer queue
si_indexingWatchdogInit_block_invoke_2
Indexing watchdog fired, delta:%lld, startTime:%lld, itemCount:%ld suspendTime:%lld resumeTime:%lld endtime:%lld
-001_
n2s.c
conversionSucceeded
(exponent >= 0) && (exponent <= 0x7FF)
PQueue.c
!offset || offset_t_GET_VALUE(queue->offsets[i])<offset*2
!offset || PositionOffset_t_GET_VALUE(queue->offsets[i])<offset*2
storage.c
inStorage->_window1.mappedStart==0&&inStorage->_window1.mappedStart==0&&inStorage->_window1.memory==((void*)0)
_storageMapInit
%s:%u: failed assertion '%s' %s mmap(%p, offset: %llx, size: %ld) error:%d, fSize:%lld
storage_copy_read_window
reader->storage->_window1.memory
%s:%u: failed assertion '%s' %s offset: 0x%lld, freeRegion: 0x%lld
(head==0) || (head < storage->_freeRegion)
_storeageSetFreeListHead
storageHeaderRestore
storageInit
storageSyncPages
_storageExpand
storageTruncate
idx < (16)
((uint32_t)8<<idx) >= inSize - baseSize
rsize+baseSize >= inSize
success
result < inStorage->_freeRegion
result
%s:%u: failed assertion '%s' %s offset: 0x%llx, freeRegion: 0x%llx
inOffset==0 || inOffset < inStorage->_freeRegion
storageMoveWindow
inSize - baseSize <= rsize
inOffset < inStorage->_size
inOffset < inStorage->_freeRegion
inOffset+inNewSize <= inStorage->_freeRegion
oldIdx <= newIdx
freeListVerify
TermUpdateSet.c
docID > node->docId
termUpdateList->termId < positions->next_term_id
DocPosting.h
updateTermCount == ctx.termCount
TermUpdateSetRestore
positions->node_table
(stats.positionInfoUsed+stats.postingInfoUsed+stats.termInfoUsed) < 4294967295U
termUpdateSet->reportedSize <= gTotalCurrentUsage
%s:%u: failed assertion '%s' %s Expected non-zero docID
docID
%s:%u: failed assertion '%s' %s expected non-zero docID for field
message:%3C
%s:%u: failed assertion '%s' %s expected non-zero docID for value
getContentTokensCallback
%s:%u: failed assertion '%s' %s expected non-zero docID for term %s
ctx->docID
ptr==end
%s:%u: failed assertion '%s' %s ref:%d sz:%d cnt:%d pos:%d pos_prev:%d freq:%d doc:%d off:%llx %s
position > node->info.position
info
%s:%u: failed assertion '%s' %s expected non-zero docID for term
tu->termLen <= (1024+20)
SIUserCtx
SIUserCtx.c
CFArrayGetTypeID()==CFGetTypeID(languages)
v12@?0s8
%s:%u: failed assertion '%s' %s invalid posting 0x%llx for 0x%llx
FileTree.c
postingOffset
!(offset & (1ull << 63))
(offset & (1ull << 63))
!flat
getNum16(page->depth)==(unsigned)pageDepth
pageSize
%s:%u: failed assertion '%s' %s Expected depth %d; page has %d
getNum16(page->depth)==(unsigned)depth
storePageEnd((StorePageRef)page) <= storePageDataSize((StorePageRef)page)
storePageDataSize((StorePageRef)page)
__builtin_popcount((storePageDataSize((StorePageRef)page))/((int)(sizeof(disk_offset_t))) + 1) == 1
storePageEnd((StorePageRef)page)/sizeof(CIDirectory_t) <= storePageDataSize((StorePageRef)page)/sizeof(CIDirectory_t)
__builtin_popcount((storePageDataSize((StorePageRef)page))/((int)(sizeof(CIDirectory_t))) + 1) == 1
result && (offset_t)(intptr_t)result != inOffset
%s:%u: failed assertion '%s' %s Got end %d and size %d
storePageEnd((StorePageRef)page) <= storePageDataSize((StorePageRef)page) && storePageDataSize((StorePageRef)page)
v20@?0i8^q12
flat
!((pageOffset & (1ull << 63)) || pageOffset==0)
flat==0
(directoryStorePageGetItemCount(subPage)+1) * 5 < (directoryStorePageGetSize(subPage)) * 4
element==0
*flat==0
*flat
outPage.leafPageOffset != getOffset(newChildPage)
page
%s:%u: failed assertion '%s' %s file tree
getOffset(info) < storageGetCount(store)
reSize<=((1<<(10))* sizeof(CIDirectory_t))
%s:%u: failed assertion '%s' %s error %d expanding from old:%d new:%d
newRealOffset
getOffset(*newOffset) < storageGetCount(store)
dir.fileId.storeOID!=0
getStoreOID(dir.fileId)!=1
page->items[i].childPage.offset==0
page->items[i].fileId.storeOID!=markerOid.storeOID
specialBits == 0
!(((realOffset & (1ull << 63)) ==0) && (realOffset & (1ull << 62 ))==(1ull << 62 ))
!(page->items[i].fileId.storeOID==markerOid.storeOID)
refPage->pathDepth==getNum16(page->depth)
getStoreOID(newDirectory.fileId) >= 2
getStoreOID(newDirectory.fileId)==fileid
elem==0
directoryStorePageGetSize(page)*4>directoryStorePageGetItemCount(page)*5
%s:%u: failed assertion '%s' %s Expected offset (%llx) to be less than free region (%llx)
CHILDLESS(offset) || MASKPAGE(offset) <store->_freeRegion
getStoreOID(root->fileId)==2
%s:%u: failed assertion '%s' %s %lld != %lld
element->fileId.storeOID == source.fileId.storeOID
directoryStoreMoveDirectory
target.directory
rootDirectory->children==0
rootDirectory->lastPosting.posting.docId==0
rootDirectory->fileid == getStoreOID(root.directory->fileId)
directoryStoreMergeUpdateSet
%s%s
directoryStoreFile
flushForwardStore
!result
realOffset==(64)
dirStoreInit
dirStore->store._fdPtr!=0
directoryStoreFile.shadow
openForwardStore
shadowForwardStore
.shadow
postings<9223372036854775807LL
getStoreOID(root.directory->fileId)==2
!target.directory->childPage.offset
!((refPage.pageOffset[0] & (1ull << 63)) || refPage.pageOffset[0]==0)
directoryStoreMakePathWithPostingsOffset
!postingsOnly
dumpDirectoryStore
itemCount==count
.shadow.shadow
0==strstr(shadowpathPtr, ".shadow.shadow")
0==strstr(path, ".shadow.shadow")
%s%s.shadow
directoryStoreGetParent
directoryStoreGetPath
directoryStoreWriterGetParent
%s:%u: failed assertion '%s' %s state:%d header:%llx
getNum(page->metadata)==kIndexShutDownStateDirty
parent
getStoreOID(element->fileId)==item && &page->items[slot]==element
directoryStoreSetParentForMove
item!=parent
element==0 || (getStoreOID(element->fileId)==item && getOffset(element->childPage) == (offset_t)parent)
parent == _directoryStoreGetParent(store, item)
%s:%u: failed assertion '%s' %s %d, %d, %d, %llx
depth< olddepth
%s:%u: failed assertion '%s' %s %d, %d, %d, %llx, %llx
item!=parents[depth]
%s:%u: failure log '%s' %s %d, %d, %d
hitPath==-1
depth<512
item!=inItem
directoryStoreEnsurePath
directoryStoreWriterGetPath
_reverseStoreIterate
itemCount == 0
flushReverseStore
commitSyncReverseStore
shadowReverseStore
%s:%u: failed assertion '%s' %s invalid state
metadata != kIndexShutDownStateFastFlush
reverseDirectoryStore
reverseDirectoryStore.shadow
%s:%u: failed assertion '%s' %s Expected bitmap to be clean for index in state %x. Dirty bit at index %lx
dirtyBitIx == kCFNotFound || dirtyBitIx >= (CFIndex)(store->storage._freeRegion/STORAGE_SHADOWPAGESIZE)
commitShadowReverseStore
getOffset(addr) == (1024+64)
reverseDirStoreInit
store->state != 0xd00d0dd0 || readOnly
get_state
recoverReverseStore
reverseStoreUpdateState
reverseStore.updates
bits
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/index/SpotlightIndex/SILiveQuerySupport.c
siquerynode.c
s!=NONE
source!=NONE
node->kind<=QN_FACTOR
count==scount
sinode->node.mnode.nodeCount>=2
%s:%u: failed assertion '%s' %s Unexpected node type %x
node->lchild==lchild
node->rchild==rchild
_kMDItemFS
_kMDItemOID
_FPIsTrashed
_kMDItemIndexID
_kMDItemPSIDPath
_FPItemIdentifier
_kMDItemCachedIcon
_kMDItemRenderValues
kMDItemFinderOpenDate
kMDStoreLastHealthCheck
kMDStoreAccumulatedSizes
_kMDItemCachedTextContent
kMDItemFSLocked
_kMDItemThumbnailData
_kMDItemFSGroupId
_kMDItemSyntheticGroupId
_kMDItemThumbnailDataPath
_kMDItemFSDataSize
_kMDItemFSSize
_kMDItemFileId
kMDItemLabelIDs
_kMDItemFSDisplayName
_kMDItemFSRsrcSize
_kMDItemEngagementValues
_kMDItemServerVersion
_kMDItemFSForkSize
_FPParentFileItemID
_kMDItemIndexGroupId
_kMDItemOIDPath
_kMDItemFSContentType
_kMDItemOIDParent
_kMDItemFSContentTypeTree
_kMDItemOnBootVolume
_kMDItemAllOldestSnapshotDates
_FPUserFSUSBFileProviderID
_kMDItemAllNewestSnapshotDates
com.apple.filesystems.UserFS.FileProvider
_kMDItemDisplayNameWithExtensions
/%lld
store_properties
oid_path
flags
index_id
parent_oid
_kMDItemEngagementData
kMDLabel_
_kMDItemThumbnailDataExists
kMDItemTitle
kMDItemRecipientAddresses
kMDItemAuthorAddresses
kMDItemInstantMessageAddresses
kMDItemEmailAddresses
kMDItemFinderComment
kMDItemHeadline
kMDItemDescription
kMDItemNamedLocation
kMDItemAlternateNames
kMDItemParticipants
kMDItemAuthors
kMDItemFullyFormattedAddress
kMDItemPerformers
kMDItemGenre
kMDItemProducer
kMDItemDirector
kMDItemInformation
kMDItemCountry
kMDItemStateOrProvince
kMDItemCity
kMDItemCreator
kMDItemIdentifier
kMDItemCoverage
kMDItemContributors
kMDItemPublishers
kMDItemOrganizations
kMDItemMusicalGenre
kMDItemLyricist
kMDItemComposer
kMDItemAlbum
kMDItemProjects
kMDItemEditors
kMDItemTextContent
kStorePropertyHealthCheckCompleteTime
_kMDItemRenderData
NSFileProviderRootContainerItemIdentifier
_kMDItemFileName
SIQueryMallocZone
compute_non_combining_chars
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2148/spotlight/index/ContentIndex/levenstein_automaton.mm
time.
absolute(
iso(
today
yesterday
two_days_ago
three_days_ago
this_week
this_month
this_year
v32@?0d8*16Q24
%04i-%02i-%02iT%02i:%02i:%02iZ
SISearchCtx_METADATA.cpp
db->store
_store->store
nodeSize>=nodeCount
gatherIndexInfo
idx==ctx->cinodeCount
v28@?0^{__CFString=}8^{_ContentIndexDocSet=}16i24
(size_t)docs[i]<j
cidocs[(size_t)docs[i]]
performSearch_METADATA
%s:%u: failed assertion '%s' %s Got matching bits %llx, mask %llx
(matchingBits.rankword & self->matchMask) == 0
Query result pack queue
Query result check queue
!page
readSDBForOids_block_invoke_2
packItems
err==0
!dbo || !dbo->oid || dbo->oid == oid
^v44@?0i8{?=(?={?=^vI}{?=^vQ}{?=*Q}{?=*I}*BCSIQTcsiqtdfdq^v)}12Q28^v36
kMDItemRecipients
kMDItemContentModificationDate
d32@?0^I8^B16^B24
fUniqueingSets
fPlistBytes[slot]
fOids[slot]
batchCount > i
processItems_block_invoke_2
fCompletionAttributeVector
kMDItemEmailConversationID
fAttributeVector
B16@?0Q8
_performSearch_degenerate
_performSearch
self->currentOid!=0
attributeCount==0
Recycle for error during query
CIIndexSet
<CIIndexSet: %p count: %u isBitMap: %d>
<CIIndexSet: %p count: %u>
CIIndexSet.c
(int32_t)set->_hole <= (int32_t)set->_count
(int32_t)hole < (int32_t)set->_size
(int32_t)hole <= set->_count
at < set->_blob[hole-1]
set->_blob
set->_count >= set->_hole-hole
oldData >= hole
bitcount>=0
start <=end
startSlot <=endSlot
startSlot < set->_size
sourceStart >= sourceEnd
PostingChunk.c
rb->current > docID
%s:%u: failed assertion '%s' %s Offset past bounds; incoming %ld, current %ld, buffer length %ld, val %llu from %d
offset < bufferLength
%s:%u: failed assertion '%s' %s %d, %d
docID > delta
docID < maxValidId
firstDocID < maxValidId
secondDocID <= maxValidId
%s:%u: failed assertion '%s' %s %ld, %ld
newOffset <= bufferLength
delta <= docID
docID >= firstDocID
docID+1 < maxValidId
docID <= maxValidId
packCtx->chunkChanges
bitVectorCount > 0
docIDLast > docIDStart
docIDLast < docIDEnd
docIDLast >= docIDs[i]
SIWordTrieContainer
[%d,%d] %s
v40@?0{?=qq}8d24^B32
assert_invalid_doc_type
ContentIndexQuery.c
newSet->children.constraint_count==0
bChild < docSet->children.count
ContentIndexDocSetsCreateIterator
!(!target || target->docIdSetType==Empty || target->docIdSetType==Mute)
ContentIndexDocSet_Step
iterator->context->positionLock == ContentIndexGetPositionsLock(iterator->docSets[0]->ref)
ContentIndexDocSet_PositionRead
docs->children.docSets[childNum]!=docs
iterator
group>0 && group <= (((coreSpotlight)?255:18)+1)
dropping
collecting
ContentIndexDocSetResolveOIDsAndGroups_Step
lt_trie_make_with_unicode
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/index/ContentIndex/transition_trie.c
user_data != _LTTrieNoUserData
lt_trie_make_with_icu_element_at_index
com.apple.spotlightui
SIBullseyeNoForceUnigrams
versionNineResetSentinelData
versionNineUpdateDataAndLength
markItemAsRenderedOrEngaged
db_match_address
core-db.c
db_set_garbage_collector
db_set_dirty_callback
db_datastore_largest_oid
db_get_object_count
db_dirty_datastore
db_fast_dirty_datastore_if_necessary
db_is_dirty
db_get_size
db_lock_datastore
db_unlock_datastore
db_downgrade_datastore
db_flush_datastore
db_commit_sync_datastore
db_shadow_datastore
db_commit_shadow_datastore
db_commit_shadow_complete_datastore
db_release_datastore_no_sync
db_shrink_cache
db_ensure_open_files
db_cooldown_files
db_copy_field_ids_with_buffer
db_copy_field_ids_with_buffer_locked
db_get_id_for_field
db_get_fields_generation
db_create_obj
db_create_obj_with_buffer
db_get_obj_callback
db_store_obj
db_update_obj
db_update_obj_callback
db_perform_callback
db_validate_obj
db_delete_obj
db_delete_obj_with_flags
db_create_id_for_value
db_add_field
db_add_field_with_cache
db_delete_fields_with_flags
db_delete_field
db_get_field
db_get_field_locked
db_get_field_by_id
db_get_offsets_for_fields
db_get_id_for_string
db_get_string_for_id_locked
db_get_field_name_for_id_locked
db_get_string_for_id
db_get_field_name_for_id
db_get_field_id_limit
db_get_localized_string
db_next_field
db_clear_docids_setup
db_clear_docids
db_clear_docids_cleanup
db_open_files
db_apply
db_get_dirty_chunks
db_create_static_strings
db_garbage_collect_strings
db_copy_delete_localized_term_ids
db_serialize_cache
db_obj_iter_create_with_filter
store.updates
db_store_dirty_chunk_info
dirty_chunks[i].pgnum > dirty_chunks[i-1].pgnum
!info->dirty_chunks
db_restore_dirty_chunk_info
info->dirty_chunks
mapping
/private/var/db/Spotlight-V100/%s-%s
mds64-crash-state
check_crash_state
ContentIndex_catch_exception_raise
setPC
ContentIndexExceptionHandler.c
td->cleanUpCount >= position
td->cleanUpCount
ContentIndex_catch_exception_raise_state
ContentIndex_catch_exception_raise_state_identity
krc==KERN_SUCCESS
%s:%u: failed assertion '%s' %s Active handlers > MAX_CI_THREAD_COUNT
entry
%s:%u: failed assertion '%s' %s invalid count %d
entry->count == -1
entry->count==0
td->itemCount < td->itemSize
td->itemCount
seqNum <= td->items[j].seqNum
seqNum == td->items[td->itemCount].seqNum
td->cleanUpSize
__THREAD_SLOT_KEY
loc>0 && loc<=1024
td->onThreadCleanUpSize
td->onThreadCleanUpCount
td->onThreadCleanUpCount >= position
slab_allocator.c
kr == 0
%s:%u: failed assertion '%s' %s Got 0 from calloc for allocation of count %ld size %ld
CIMerging.c
p||count==0||size==0
lastItem.item.termLen==0 || TermItem_compare(&lastItem, &sourceTerm[0]) < 0
sourceTerm[k].idx<sourceTerm[k-1].idx
moredata&(1<<sourceTerm[i].idx)
iterateTermsForIndexes
%s:%u: failed assertion '%s' %s corrupt ro index need to rebuild %s
!buffers->badIndex
strlen((char*)strbuf) == workItem.termLen
mergeIndexDataTrampoline
B28@?0i8^q12Q20
tmp.merge.termIdFile.%d
mergeIndexData
(newIndex)->groupMap[gslot]
4096*(gslot*8+bslot) <=slot
4096*(gslot*8+bslot+1) >slot
((newIndex)->coreSpotlight?({ ((uint8_t *)(newIndex)->groups)[(docId)]; }):({ uint32_t __where=(uint32_t)(docId); uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)((newIndex)->groups[__slot])); (__word >> __shift) & 0x3F ; }))==(inval&0x3F)
(((newIndex->coreSpotlight?({ ((uint8_t *)newIndex->groups)[docId]; }):({ uint32_t __where=(uint32_t)docId; uint32_t __slot = __where/5; uint32_t __shift= (__where-(__slot*5))*6; uint32_t __word = ((uint32_t)(newIndex->groups[__slot])); (__word >> __shift) & 0x3F ; }))&0x1F)) == 0
newIndex->payloadCount + newIndex->base <= sources[idx_cnt-1]->payloadCount + sources[idx_cnt-1]->base
hitCount
hitCount<=idx_cnt
_last>(uint32_t)idx
idx<idx_cnt
unpackAndCleanse
offset < readBuffer->mappedSize
nxtLink==0||!isCompact
changeHolder->count == 0 || changeHolder->hole
next != nxtLink
packContext.packbuffer[k-1] > packContext.packbuffer[k]
ctx->positionsBuffer.intCount != 1
word>>28 == 0
termIdFd == -1
Merge
vacuum
MergeSuccess
MergeCanceled
MergeException
MergeError
Canceled
Complete
absolute:  %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu
delta:     %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu
count:     %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu, %lu
bytes:     %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu total: %lld
term - bytes:     %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu total: %lld
term  - words:     %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu total:%lld
term  - packings:     %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu, %llu,%lld
total optimized term byte count: %llu
total optimized term byte count with overhead: %llu
B20@?0^{?={?=QQI*i}q}8i16
lastId>=lastStringId
sourceItems[i].item.postingOffset
buffer->ints[buffer->intCount-1]==0
next >= context->last_offset
!hasCompressedPositions
docIdDelta
readDataForOneDocument
context->docId
nextPos < (1<<27)
context->innerOffset < context->readBuffer.mappedSize || context->readBuffer.mappedSize==0
buffer->intCount==0 ||buffer->ints[buffer->intCount-1]==0
buffer->lastPosition == (uint32_t)-1 || buffer->lastPosition == 0
ctx->flip
first < 1<<27
buffer->ints[lowIndex] < 1<<27
buffer->ints[highIndex] < 1<<27
firstOffset < 1<<27
%s:%u: failed assertion '%s' %s Unexpected first offset %d, %d, %ld
(offset & 0x3) == 0
buffer->lastInput == 0 || buffer->lastInput > position
position != 0 || buffer->lastPosition != 0
%s:%u: failed assertion '%s' %s expected %d > %d
i==0 || context->packbuffer[i-i] > context->packbuffer[i]
context->packbuffer[i] > context->packbuffer[i+1]
context->count<=1 || context->packbuffer[context->count-2] > context->packbuffer[context->count-1]
context->count<=2 || context->packbuffer[context->count-3] > context->packbuffer[context->count-2]
changes->hole>0
%s:%u: failed assertion '%s' %s Offset %lld past end of buffer %ld
next < context->readBuffer.mappedSize
_excReadBufferPosContextMatch
_excReadBufferMatch
mergeIndexData_block_invoke
count <= k
sdb2_rwlock.c
waiter->threadid
list->head==0
lock->writer != pthread_self()
%s:%u: failed assertion '%s' %s Last wr lock at %d new attempt at %d
/var/log/CDIS.custom
LANGUAGE=%s
AppleLanguages
UNICODE MATCH:%s
addValue
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/server/Foundation/MDPlistBytesAdditions.c
%s:%u: failed assertion '%s' %s NLStringTokenizerCreate err:%d
CITokenizer.c
version>=0 && version<=kCITokenizerVersionCurrent
%s:%u: failed assertion '%s' %s alloc err:%d (%x)
state->uniChars
(startOffset>=0) && (startOffset<=inBufferLen)
zh-Hant
zh-HK
/usr/lib/libmecabra.dylib
MecabraCreateWithOptions
MecabraAnalyzeString
MecabraCancelAnalysis
MecabraGetNextCandidate
MecabraCandidateGetAnalysisString
MecabraCandidateGetSurface
fd_create_protected
fd_obj.c
obj->_magic==(0xFCFCFCF3)
obj->prev
g_fd_list->item_count==0
g_fd_list->item_count
g_fd_list->item_count>=0
!obj->prev
obj->fd == fd
obj->open_count
_fd_acquire_fd
g_fd_list->fd_count>=0
processed++ < g_fd_list->item_count
processed == g_fd_list->item_count
_safe_open_at
%s:%u: failed assertion '%s' %s Too many open files %d (%d) (%d)
%s:%u: failed assertion '%s' %s Too many open files in system %d
obj->fd != -1
g_fd_list->item_count!=0
g_fd_list->head
fd_create_sibling_protected
_fd_ftruncate
fd_lseek
fd_pread
!obj->forbidder
fd_pwrite
fd_write
fd_rename
%s:%u: failed assertion '%s' %s Unexpected delete of %s from %d
!unlinked
RLEOIDArray
SIValueSet
CombLevel_s
SIValueSetInternals.h
s->sharedDepth>=0
((sizeof(ValueType) * 8) -4*(1+(s->sharedDepth))) <= sizeof(ValueType)*8
s->sharedDepth
(key & fMask) == fPfx
holder->GetRawCount() <= (63)
startPrefix == (startPrefix & startMask)
%s:%u: failed assertion '%s' %s next: 0x%llx, max: 0x%llx
FindTermIDs.c
flatStoreGetOffset(child) < ctx->flat_max
%s:%u: failed assertion '%s' %s invalid key_len %d
key_len>0 && key_len <= 4
t->size>=t->count
%s:%u: failed assertion '%s' %s grow buckets error, bucket count:%d
buckets
%s:%u: failed assertion '%s' %s duplicate pages %ld %ld
%s:%u: failed assertion '%s' %s type: %d
icu_utils.c
b->_type>kICUBaseType&&b->_type<kICUBaseEndType
%s:%u: failed assertion '%s' %s unexpected type %d
ctx->_base._type==kICURangeType
ctx->_base._type==kICUSearchType
v28@?0*8I16^{?=^{term_expansions}[8I]CC[0q]}20
rules_offset <= rules_size
expansions_offset <= rules_size+expansions_size
string_offset <= total_size
ne->expansions[i][needed-1] == 0
%s:%u: failed assertion '%s' %s invalid utf8 %u
%s:%u: failed assertion '%s' %s invalid char %u
%s:%u: failed assertion '%s' %s token len %d
p->token_len<TOKEN_MAX
[last tertiary ignorable]
[last primary ignorable]
[variable top]
[last regular]
[import 
%s:%u: failed assertion '%s' %s unexpected %d
%s:%u: failed assertion '%s' %s len %d
key_len
%s:%u: failed assertion '%s' %s bad type: %d
%s:%u: failed assertion '%s' %s bad length: %d
expansion_len<EXPANSION_LIMIT
size <= 256
%s:%u: failed assertion '%s' %s size: %d count:%d
b->size>=b->count
v36@?0i8^S12i20^S24i32
%s:%u: failed assertion '%s' %s %s count %d
ctx->count>1
NO_MATCH
MATCH
%s:%u: failed assertion '%s' %s %s level: %d count: %d
cur_state.level+1>=ctx->count
cur_state.level+1<ctx->count
cur_state.level==0 && (cur_state.level+1<ctx->count)
cur_state.level+1!=ctx->count
cur_state.cur==(u_int8_t*)string
item.start_mask&U_MASK(u_charType(utf8_to_code_point((u_int8_t*)cur_state.cur)))
__checkIndexSet
unexpected index base
overlapping doc ids
Cache/%x/%llx.%s
Cache/%4.4x/%4.4x/%4.4x/%lld.%s
%s:%u: failed assertion '%s' %s no field name for id %d of localize id %d
SIStoring.c
field_name
v24@?0r*8^{db_field=SSII[0C]}16
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/index/SpotlightIndex/SIStoring.c
void _SIRecomputeSizesWithCallback(SIRef, off_t, dispatch_block_t)
%s:%u: failed assertion '%s' %s magic:%llx
ctx->magic==RECOMPUTE_SIZE_MAGIC
canceled
%s:%u: failed assertion '%s' %s magic:%llx ctx:%p ref:%p
ref->magic == (0xc0de10de10dec0de)
void si_recompute_sizes(void *, Boolean)
v24@?0^q8Q16
 inflight
repair_lookupPath
%s:%u: failed assertion '%s' %s Got parent[%d] with id %lld depth: %d
newpath[i] > 0
updateWithNewPath
kMDItemPrimaryRecipientEmailAddresses
kMDItemAdditionalRecipientEmailAddresses
kMDItemHiddenAdditionalRecipientEmailAddresses
kMDItemRecipientEmailAddresses
psid_lookupPath
si_populategroup
si_writeBackAndIndex detected corrupted sdb on entry
_kMDItemContentIndexVersion
ContentIndexWritable(content_index)
si_writeBackAndIndexWithLiveQueryToggle
db->liveSet->indexCount
content_index
com_apple_system_prefs_keywords
kMDItemKeywords
set attributes err
%s:%u: failed assertion '%s' %s si_writeBackDBO failed %d
si_writeBackAndIndex detected corrupted sdb on exit
_kMDItemStateInfo
com.apple.searchd
FPDomainIdentifier
_kMDItemExternalID
computePathFP
computePathFS
dbo->parent_oid||dbo->oid==2
SIStoring.h
indexSet->currentIndex==~0 || (uint32_t)indexSet->currentIndex<(uint32_t)indexSet->indexCount
indexSet->currentIndex==~0 || ContentIndexWritable(indexSet->index[indexSet->currentIndex])
live.%d.
!ContentIndexWritable(indexSet->index[i])
!found
indexSet->index[spot-1]==ctx->idx
%s:%u: failed assertion '%s' %s This should be impossible; this thread is supposed to be  suspended when the other thread changes index sets.
OSAtomicCompareAndSwapPtrBarrier(oldIndexSet,indexSet,(void* volatile*)indexSetPtr)
!ContentIndexWritable(ci)
void _swapIndex1(struct flush_index_ctx *, Boolean)
void _swapIndex(struct flush_index_ctx *, Boolean)
compact err
void _swapIndex2(struct flush_index_ctx *, Boolean)
startIndex < indexSet->indexCount
indexSet->indexCount >= j
swapIndex
v40@?0^{__SI=Q{SIFileOps=^?^?^?}{SIGuardedFd=iQ}isIi^{SIWatchDog}^{__CFDictionary}{si_missing_oids_s={os_unfair_lock_s=I}^{__RLEOIDArray}^{__RLEOIDArray}}{si_missing_oids_s={os_unfair_lock_s=I}^{__RLEOIDArray}^{__RLEOIDArray}}{os_unfair_lock_s=I}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFBag}^{__CFDictionary}Bi[14^{si_scheduler_token_s}]iI{?=[14^{_si_work_scheduler}][16^{_si_workqueue}]}^{datastore_info}{CIMetaInfo=i^{fd_obj}iIIIIIIIIqqiiB}QQ{_opaque_pthread_mutex_t=q[56c]}^{ContentIndexList}^{ContentIndexList}iII^{_SI_PersistentIDStore}{__SIStoreToken={?=CCCCCCCCCCCCCCCC}^{__CFUUID}}ACAIddBiI^{__CFDictionary}{_opaque_pthread_mutex_t=q[56c]}{_opaque_pthread_mutex_t=q[56c]}{_opaque_pthread_mutex_t=q[56c]}{_opaque_pthread_mutex_t=q[56c]}^{__CFSet}^{__CFDictionary}Q^{__CFBag}^{__CFDictionary}{_opaque_pthread_mutex_t=q[56c]}{_opaque_pthread_mutex_t=q[56c]}{_opaque_pthread_cond_t=q[40c]}iIIIIIIIIIIIIIIIIIIIBBAq^{__CFDictionary}{os_unfair_lock_s=I}^{__CFDictionary}^{__CFBitVector}^{si_mobile_journal}^{si_mobile_journal}AqAqd^?^vd{?=^{fd_obj}IIIBq{os_unfair_lock_s=I}}III^{FinderDateFields}{_opaque_pthread_mutex_t=q[56c]}^{fd_obj}^{fd_obj}ii^{_SIIndexCallbacks}^{__CFArray}^{__CFArray}qqqQIIiiBBBBBBBABB^{si_scheduler_token_s}BBBBBQ[4096c]{os_unfair_lock_s=I}b1b1b1b1b1b1b1b1b1b1b1b1b1b2b1^i^{__CFSet}^{__SIUINT64Set}^{ReverseDirStore_s}^{FileTree_Overlay_s}^{__CFSet}{AccumulatedCounts_s={_opaque_pthread_mutex_t=q[56c]}[256q][256I]}^{TermUpdateSet}{_opaque_pthread_rwlock_t=q[192c]}[16C]Bi^{datastore_info}AIBB[5i]iI}8^{_xpc_activity_s=}16^B24^{dispatch_group_s=}32
i8@?0
tmp.%ld.
flush
flush cache err
void _flushCache(struct flush_index_ctx *, Boolean)
ctx->suspend_token == 0
_Ranking
kMDItemUseCount
kMDItemFSSize
kMDItemFSCreationDate
kMDItemFSOwnerUserID
kMDItemFSNodeCount
kMDItemFSTypeCode
kMDItemFSCreatorCode
kMDItemFSFinderFlags
kMDItemFSHasCustomIcon
kMDItemFSIsStationery
kMDItemLangugeStrId
_kMDItemDisplayNameWithExtensionsSynth
SIFetching.c
flattenedIndex < totalFlattenedFieldCount
sortedFieldIdPairIndex < totalFlattenedFieldCount
cannedRequiredAttributeCount == cannedRequiredAttributeIndex
dbIter
kMDItemPath
_fillPlistBytes
_kMDItemFinderExcluded
_showAllExtensions
uid==0
kSISDBPages
kSISDBIterators
kSISDBObjects
kSISDBCacheHits
kSISetupTime
kSITermTime
kSIPostingsTime
kSIAttributeTime
kSISetupTimeFirstBatch
kSITermTimeFirstBatch
kSIPostingsTimeFirstBatch
kSIAttributeTimeFirstBatch
kSIStartTime
kSIEndTime
kSIEndTimeFirstBatch
kSIWaitTime
kSIActiveTime
kSIQoSLevel
_kMDItemFSDisplayKind
kMDLabel_zya2exypzrhulknkk5enqbj33y
kMDLabel_yekauorssrbpta3hdteqgbglma
kMDLabel_26x2uentpjgt7lka65qdcazuya
kMDLabel_j3w3eydksrbbzleqdyl7kbqkuu
kMDLabel_kqde5prblvaibjifrcn4saxjwi
kMDLabel_veyixt7jjffe3g7nfailqcbxny
kMDLabel_qygkxhrfarhtxanqhi264amkku
kMDLabel_hlsi7t7nerhynemqvydgeb26de
_kMDQueryItemInScopeForRankingOnly
%s/%s
_kMDItemQueryPath = "stuff"
_kMDItemBundleID=com.apple.searchd
SIQueryC.c
CFGetTypeID(completionQueryString) == CFStringGetTypeID()
Spotlight
Bullseye
_kMDItemSDBInfo
kMDItemContentTypeTree
public.message
com.apple.mail.emlx
com.apple.mail.eml
com.microsoft.entourage.virtual.message
com.apple.ichat.transcript
public.contact
public.vcard
com.apple.addressbook.person
com.apple.addressbook.group
com.microsoft.entourage.virtual.contact
com.microsoft.entourage.virtual.group
com.apple.systempreference.prefpane
public.font
public.bookmark
com.apple.safari.bookmark
com.apple.safari.history
public.to-do-item
public.calendar-event
com.apple.ical.bookmark
com.apple.ical.bookmark.todo
com.apple.ical.ics.event
com.apple.ical.ics.todo
com.microsoft.entourage.virtual.event
com.microsoft.entourage.virtual.task
public.movie
com.apple.quicktime-movie
public.mpeg-video
public.mpeg-4
public.mpeg
public.3gpp
public.3gpp2
com.apple.application-bundle
com.apple.application-file
com.apple.dashboard-widget
public.folder
com.apple.mount-point
public.audio
public.mpeg-4-audio
com.apple.protected-mpeg-4-audio
com.adobe.pdf
com.apple.localized-pdf-bundle
public.presentation
com.microsoft.powerpoint.ppt
com.apple.keynote.key
com.apple.iwork.keynote.key
public.image
com.apple.motion.project
com.apple.iwork.pages.pages
com.apple.iwork.pages.sffpages
com.apple.iwork.pages.template
com.apple.iwork.pages.sfftemplate
public.rtf
com.apple.rtfd
com.apple.flat-rtfd
com.microsoft.word.doc
org.khronos.collada.digital-asset-exchange
public.plain-text
public.html
public.xhtml
public.shell-script
public.source-code
public.unix-executable
com.apple.xcode.project
com.apple.xcode.model
com.apple.xcode.archive
com.apple.xcode.docset
com.apple.xcode.projectdata
com.apple.xcode.dsym
com.apple.xcode.configsettings
com.apple.xcode.usersettings
com.apple.xcode.strings-text
com.apple.xcode.plugin
com.apple.xcode.mom
com.apple.property-list
dyn.ah62d4rv4ge81a7dk
dyn.ah62d4rv4ge80u5pbsa
com.apple.dashcode.xml
com.apple.dashcode.css
com.apple.dashcode.javascript
com.apple.dashcode.json
com.apple.dashcode.manifest
com.apple.interfacebuilder.document
com.apple.interfacebuilder.document.cocoa
com.apple.rez-source
com.apple.iphone.developerprofile
com.apple.iphone.mobileprovision
com.apple.coreanimation-bundle
com.apple.coreanimation-xml
com.sun.java-class
com.apple.scripting-definition
com.apple.dt.document.workspace
com.apple.dt.document.scheme
com.apple.dt.ide.plug-in
com.apple.dt.dvt.plug-in
com.apple.dt.document.snapshot
com.apple.dt.bundle.unit-test.objective-c
com.apple.instruments.tracetemplate
com.apple.quartzdebug.introspectiontrace
com.apple.applescript.text-object
com.apple.applescript.data-object
com.apple.applescript.url-object
com.apple.applescript.alias-object
com.apple.symbol-export
com.apple.mach-o-binary
com.apple.mach-o-object
com.apple.mach-o-executable
com.apple.x11-mach-o-executable
public.object-code
com.microsoft.windows-executable
com.microsoft.windows-dynamic-link-library
com.sun.java-archive
com.sun.web-application-archive
com.apple.xcode.plugindata
com.apple.dt.playground
com.apple.iwork.numbers.sffnumbers
com.apple.iwork.numbers.numbers
com.apple.iwork.numbers.template
com.microsoft.excel.xls
org.openxmlformats.spreadsheetml.sheet
public.spreadsheet
public.xml
com.apple.log
com.apple.crashreport
com.apple.spinreport
com.apple.panicreport
com.apple.shutdownstall
com.apple.hangreport
public.json
public.log
public.content
kCIBitCount
kCIMatchArray
kMDItemFSFileId
kMDQueryResultTopMatchedField
kMDQueryResultGroupId
SIQueryAddResultFilter
query->_liveUniquedSet==0
query->_liveUniquedQuery==0
query->queryNodes==0
kMDItemLogicalSize
kMDItemContentType
_kMDItemFinderFlags
_kMDItemCreatorCode
_kMDItemTypeCode
_kMDItemIsStationery
_kMDItemNodeCount
_kMDItemIsExtensionHidden
_kMDItemHasCustomIcon
_kMDItemFinderLabel
_kMDItemOwnerGroupID
_kMDItemContentChangeDate
_kMDItemCreationDate
kMDItemFSIsExtensionHidden
kMDItemFSLabel
kMDItemFSOwnerGroupID
kMDItemFSContentChangeDate
kMDItemFS
_kMDQueryScope
kMDItemSubject
v32@?0r*8s16^{TokenRange=ss}20s28
i20@?0^{__ContentIndex=}8B16
_kMDItemDisplayNameWithExtensions == "X" || (_kMDItemDisplayNameWithExtensions != * && kMDItemDisplayName == "X")
%s:%u: failed assertion '%s' %s Bad generated query mid-node; unexpected type %d
midNode->type == AND_NODE
_kMDItemQueryPath
Flat
%s=*||%s=*||%s=*||%s=*||%s=*||%s=*||%s=*
!(%s=*||%s=*||%s=*||%s=*||%s=*||%s=*||%s=*)
%llx
_kMDItemQuery
adds
processScopes
_kMDItemTimeMachinePath = "stuff"
_kMDItemTimeMachinePath%lld
_kMDItemTimeMachinePath = "stuff" && _kTimeMachineOldestSnapshot<=0 && _kTimeMachineNewestSnapshot>=0))
%s%lld
_kMDItemTextContentIndexExists
%s:%u: failed assertion '%s' %s Bad query node; unexpected op %d
%s:%u: failed assertion '%s' %s Bad query node; unexpected type %d
!isNegativeQuery(node->lchild)
holder
_copyFile
copyFile.c
wLen <= actual
copyFileFallback
si_activity_journal.c
offset+dataSize<1024
journalAttr.
### invalid type %d at offset %ld
%lld
### unknown type %d at offset %ld
v16@?0r^{si_playback_info=IIIIq(?={?=IdIIIqqQ}{?=IQ}{?=II}{?=q*iccc}{?=ii*I}{?=dQ}{?=i}{?=q})}8
CreateIndex
OpenIndex
OpenIndexShadow
UpdateItem
DeleteItemError
AddJournalItem
UpdateJournalItem
DeleteJournalItem
SyncIndex
ShadowIndex
ShadowIndexError
UnlinkJournal
MergeScan
ShutDown
Dirty
%F %T
%s pid: %d time: %s syncCount: %d scanCount: %d liveCount: %d serialNumber: %lld consumedSerialNumber: %lld recoverTimeStamp:%s (%ld)
%s pid: %d time: %s syncCount: %d scanCount: %d liveCount: %d serialNumber: %lld consumedSerialNumber: %lld
%s pid: %d time: %s syncCount: %d scanCount: %d liveCount: %d
CloseIndex
%s time: %s
SyncIndexError
%s time: %s recoverTimeStamp:%s (%ld)
NewJournal
%s journal.%d
AddItemError
%s oid: 0x%llx %lld transaction: %d
%s oid: 0x%llx %lld
DeleteItem
%s id: %d oid: 0x%llx %lld
NewLiveIndex position: %d base: %lld
MergeLive
%s position: %d count: %d
NewBundleGroup %d 0x%x %s
Playback start
Playback end
%s transaction: %d id: %s oid: 0x%llx serial: %lld
%s id: %s oid: 0x%llx serial: %lld
%s transaction: %d id: %s oid: 0x%llx serial: %lld read: %d
%s id: %s oid: 0x%llx serial: %lld read: %d
%s transaction: %d id: %s oid: 0x%llx serial: %lld read: %d flagged: %d
%s transaction: %d id: %s oid: 0x%llx serial: %lld read: %d flagged: %d color: <null>
%s transaction: %d id: %s oid: 0x%llx serial: %lld read: %d flagged: %d color: %d
yMdHms
%04d-%02d-%02d %02d:%02d:%02d
_CIMetaInfoSync
indexState
CIMetaInfoCreate
IOService
%02x
IOMACAddress
IOPropertyMatch
IOPrimaryInterface
CURRENT_PROJECT_VERSION
CIMetaInfoRead
ContentIndexCommon.c
metaInfo->shadowedGeneration <= metaInfo->cleanGeneration
CIRemapping
remapping%ld
remapping%d
kIndexRemappingBarrier
newId>0 && newId < count
qsort_oids.c
TermIdStore.c
getNum(ts->termIdCount)
getNum(ts->termIdSize) >= getNum(ts->termIdCount)
%s:%u: failed assertion '%s' %s tid:%d, ct:%d, fr:%llx
termID+count == ts->store._freeRegion / sizeof(disk_TermData)
ts->store._freeRegion == ((offset_t)getNum(ts->termIdCount)) * sizeof(disk_TermData)
readBuffer
termIdStoreShadow
TermTrie.c
trie->dataAllocator
level->follows[i]==0
l->data.termLen <= (1024+20)
level->follows[pos]==0
%s:%u: failed assertion '%s' %s invalid length for %s
term_len
__builtin_popcount(compactLevel->size)==1 && slot > 1 && slot <= 5
depth<15
count<size
cur->data.termLen>=term_offset
tmpLevel.payload==((void*)0)
TermTrieKindList==((unsigned)((tmpLevel.follows[cur->data.termData[term_offset]]) & 3))
((void*)((tmpLevel.follows[e->data.termData[term_offset]]) & ((~(uintptr_t)0)-3)))!=0
result!=0
level->size>=level->count
__builtin_popcount(entry_size)==1 && slot > 1 && slot <= 5
term_len <= CI_UTF8CHARS_BUFFER_SIZE
[fl] 
[l] 
[f] 
(0) 
(2) 
[*] 
poslock.c
result==0
kIndexOptionReadOnly
B16@?0i8i12
SIOpenIndexAtPathWithCallbacks_block_invoke_3
Suspend
Resume
index flush suspend queue
CIPositionRun.c
SI_PersistentIDStore
SIVirtualPSIDSupport.c
depth<=513
_oidParentForOid
/Network/
malloc_size(map)==0
CIPayloadData.c
posPtrStart!=0
CIPositions.h
0 == kr
oi == i
docId == 0 || newDocID < docId
newDocID
CIPositionRun.h
run->buffer
%s:%u: failed assertion '%s' %s base:%p ptr:%p value:%d info next:%d size:%d next:%d len:%d docId(%d):%d 
posStart < (uint8_t*)(positions->info_store+positions->_info_next.infoptr)
*(PositionInfoRef*)posStart < positions->_info_next.infoptr
next < positions->_info_next.infoptr
bytes[0]!=0
len == 0 || bytes[len-1]==0
posStart[len-1]==0
%s:%u: failed assertion '%s' %s o:%lld, pc:%d dc:%d flags:%d
node->offset || postingCount==docCount
PayloadIterator.c
!offset || AnnotatedPositionOffset_t_GET_VALUE(queue->offsets[i])<offset*2
queue->top <= queue->end
current >= offset
current <= offset
last>=offset
current >= last
last<offset
(((iter->ptr) & 0x3FFFFFFFFFFFFFFF))==next
((iter->ptr) & 0x3FFFFFFFFFFFFFFF)==next
%s:%u: failed assertion '%s' %s nxtLink: %lld. compact
iter->compact == false
%s:%u: failed assertion '%s' %s Unexpected value for nextLink: %lld. next=%lld barrier=%lld split=%lld
(OFFSET_GET_VALUE(nxtLink) <= barrier && next>barrier)|| (OFFSET_GET_VALUE(nxtLink)<iter->split)
!queue->split || queue->splitPoint>=offset_t_GET_VALUE(value)
(((iter->ptr) & 0x3FFFFFFFFFFFFFFF)) >= ((oldPtr) & 0x3FFFFFFFFFFFFFFF) || oldPtr==0x3FFFFFFFFFFFFFFF
range.location > 0
range.length == 1
bits == ((void*)0)
changes->hole <= changes->count
i <= changes->hole
changes->hole<=changes->count
updateSetInfo
infoBase
iter->ptr_documentsContainingTerm
item!=0
pqcount_ItemIteratorRef(queue)!=0
!bulkIter->split ||bulkIter->future[i]->ptr <= 2*bulkIter->split
pqcount_AnnotatedPositionOffset_t(&iter->futureOffsets)==0
pos>=last
_CIPositionIterate_NewCompressed
pos>0
_CIPositionIterate_Compressed
_SLPositionIterate_Compressed
!iterator->split ||(((iterator->future[i]->ptr) & 0x3FFFFFFFFFFFFFFF)) <= 2*iterator->split
!iterator->split ||(((futureDeferredArrayCtx->array[i]->ptr) & 0x3FFFFFFFFFFFFFFF)) <= 2*iterator->split
iterator->futureCount==0
iter->split==iterator->split
iter->ptr > iterator->split && iter->ptr <= iterator->split*2
!iterator->split ||(((iter->ptr) & 0x3FFFFFFFFFFFFFFF)) <= 2*iterator->split
iter->ptr > iter->split && iter->ptr <= iter->split*2
!header.nextMinor || header.nextMinor > next || (header.nextMinor <= barrier && next>barrier)|| header.nextMinor<iter->split
!header.nextMajor || header.nextMajor > next || (header.nextMajor <= barrier && next>barrier)|| header.nextMajor<iter->split
!header.nextMajor ||header.nextMajor> header.nextMinor || (header.nextMajor <= barrier && header.nextMinor>barrier) || header.nextMajor<iter->split
!header.nextMajor
tmp.offset > iter->split && tmp.offset <= iter->split*2
iter->nextMajor==0
_CIPositionIterator_Skip
_SLPositionIterator_Skip
iter->split!=0
pqcount_ItemIteratorRef((PQueue_ItemIteratorRef*)&iter->futureOffsets)==0
pqcount_ItemIteratorRef(&deferredCtx->deferreds) == 0 && deferredArrayCtx->arrayCount==0 && futureDeferredArrayCtx->arrayCount==0
!iter->skip
oldSlot+delta >=1
_SLPositionIterate
value.offset!=0
iter->futureOffsets._offsets[i].offset<=iter->split*2
iterator->split != 0
!iter->initialOffsets.splitPoint ||tmp.offset < 2*iter->initialOffsets.splitPoint
tmp.offset
MDUnicodeConverter
CIQuery.c
field >= 1
oqcount_offset_t(payloadQ) == oqcount_offset_t(positions[i])
PQueue.h
!queue->split || queue->splitPoint>=PositionOffset_t_GET_VALUE(value)
!queue->split
PayloadIterator.h
!queue->split || queue->splitPoint>=AnnotatedPositionOffset_t_GET_VALUE(value)
asserted
__CIMatchQueryNodesLazy
kSearchItemTypeInvalid!=item_type
TermTrie.h
str_len==resolve_len
v16@?0^{RelativePosting=I{?=II}}8
v16@?0^{tree_node={?=b63b1}{?=b63b1}{TermUpdate=(?={?={RelativePosting=I{?=II}}I}{?={UpdatePosting=I{?=IQ}}})S[0C]}}8
v24@?0q8^{lt_trie_node=*^^{lt_trie_node}CCC}16
v16@?0q8
v20@?0^v8i16
%s:%u: failed assertion '%s' %s %d
termLen <= CI_UTF8CHARS_BUFFER_SIZE
pos<(secondaryCount+nodeCount)
v28@?0I8^{RelativePosting=I{?=II}}12^{RelativePosting=I{?=II}}20
fields
loc 
v20@?0^{QueryNode=**Iii^{icu_search_context}^{icu_regex}^{levenstein_automaton}I}8I16
lastpos>info->position
(value & 1) == 0
last > nextPos
%s:%u: failed assertion '%s' %s Expected valid doc set type. Got %d
CIIndexSet.h
set->_blob[set->_hole-1] > at
right < (int32_t)set->_size
set->_hole-1 < (int32_t)set->_size
posting
position
_PayloadScannerReadNextChar
CIPayloadCompact.c
PayloadScannerPosition(scanner) == position
%s.mds.%d.%d.compactPayloads1.idx
%s.mds.%d.%d.compactPayloads2.idx
termId < maxTermId
p1.hasLength==p2.hasLength
(pe1.docId == 0 || pe2.docId == 0) || pe1.docId > pe2.docId || (type1 != postingTypeNormal || type2 != postingTypeNormal)
pe1.docId == 0
pe2.docId == 0
*canceled || PayloadPulsesCount(&ctx.src) > PayloadPulsesCount(&ctx.dst)
PayloadPulsesCount(&ctx.src) == 1
CICompactPayloads
PayloadScannerPosition(&ctx.scanner) > p.offset && PayloadScannerPosition(&ctx.scanner) < p.offset+p.length
%s:%u: failed assertion '%s' %s offset:%lld, free_region:%lld
storage.h
inOffset<inStorage->_freeRegion
%s:%u: failed assertion '%s' %s offset:%lld, size:%lld, free_region:%lld
!check_size||inOffset+inSize<=inStorage->_freeRegion
(offset_t)(intptr_t)result !=inOffset
PayloadScannerPosition(s) + docInfoLength <= s->end
_type >= 0 && _type <= 2
i < 5
i < 10
%s:%u: failed assertion '%s' %s offset: %lld end: %lld
PayloadScannerPosition(s) + remaining <= s->end
list
list->items
PayloadScannerPosition(s) + runLength <= s->end
type >= 0 && type <= 2
position && position < inMap->count
reorderInfo[reorderCount].newTermId
tmp.scratch.%d.%d
scratch_file_create
/mds_scratch.%d.%d
/tmp/%s
positionLen > 2
!delta||delta<docId
processed==positionLen||scratch->err
!(next & 0x01)
position == 0
position != 0
delta <= position
data_len >= 2
data_len > 2
processed==positionLen
PayloadPulsesReorder
scratch_file_grow
PayloadPulsesWrite
%s:%u: failed assertion '%s' %s %s
ldb.c
len > (off1-off)
copyDataForUniquedValue
%s:%u: failed assertion '%s' %s Expected offset %ld to fit in size %ld
fLen >= fOffset
field
%s:%u: failed assertion '%s' %s field:%d(%s), type:%d, flags:0x%x
last_id<dbfs[i].name_id
field_id!=0
*offset <= buffer_size
%s:%u: failed assertion '%s' %s Overflow %ld + %ld
(size_t)dbo->used_bytes+(size_t)dbf->data_len < (size_t)UINT32_MAX
dbo->size == (old_size<<1)
%s:%u: failed assertion '%s' %s Buffer overflow %ld + %ld > %ld
(size_t)dbo->used_bytes+(size_t)dbf->data_len <= (size_t)dbo->size
inflateDBFData
sizeof(db_uint32_t) + (size_t)dbo->used_bytes <= (size_t)dbo->size
%s:%u: failed assertion '%s' %s Buffer overflow: %ld + %ld > %ld
*offset+len <= buffer_size
%s:%u: failed assertion '%s' %s dbo overflow: %ld + %ld > %ld
(size_t)len + (size_t)dbo->used_bytes <= (size_t)dbo->size
%s:%u: failed assertion '%s' %s Expected %ld + %ld <= %ld
sizeof(uint8_t) + (size_t)dbo->used_bytes <= (size_t)dbo->size
sizeof(uint16_t) + (size_t)dbo->used_bytes <= (size_t)dbo->size
sizeof(uint32_t) + (size_t)dbo->used_bytes <= (size_t)dbo->size
sizeof(uint64_t) + (size_t)dbo->used_bytes <= (size_t)dbo->size
%s:%u: failed assertion '%s' %s Expected len %ld to contain new data size %ld - %ld
(size_t)len >= (size_t)(fOffset-saveOffset)
(size_t)dbf->data_len + (size_t)dbo->used_bytes <= (size_t)dbo->size
%s:%u: failed assertion '%s' %s Expected offset %ld plus len %ld to fit in size %ld
*offset+dbf->data_len <= buffer_size
%s:%u: failed assertion '%s' %s Expected %ld <= %ld
%s:%u: failed assertion '%s' %s Expected len %ld plus used_bytes %ld to fit in dbo %ld
last_offset == pdbo_offset
%s:%u: failed assertion '%s' %s Unexpected type %d
db2_open_query_with_expr
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/core-db/serial-db2/sdb2_query.c
db2_read_query
sdb2_die
%s:%u: failed assertion '%s' %s %s:%d : %s : %s
sdb2.c
db2_page_uncompress_swap
lzfse
deflate
db2_create_datastore
Multiple threads entering in sdb!
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2146.1/spotlight/core-db/serial-db2/sdb2.c
(offset & ((1 << 12)-1)) == 0
store.db
%sStr-%d.map
Multiple threads in sdb! (leaving)
v16@?0^v8
db2_sync_datastore
(dst->const_flags & 0x8) == 0
db_compress_cache
db_split_page
ldb.h
b0 < 0xE0
in_cached==entry->cache_dbps
_flush_cache_entry
(write_size & ((1 << 12)-1)) == 0
page_release_dirty_compressed
db_shove_page
do_string_update || (flags & 0x000000f0)==0
map_check_size
!updatedDirty
(size_t)(*dbpp)->size <= malloc_size(*dbpp)
%s:%u: failed assertion '%s' %s obj_iter_fetch_page: ERR: tried to read attr name table data! pgnum 0x%x, flags 0x%x
((*dbpp)->flags & DB_PAGE_STRING_DATA)==0
_page_fetch_with_fd
(dbp->flags & DB_PAGE_STRING_DATA)==0
(size_t)dbp->used_bytes <= sz
i20@?0q8I16
flush_updateset_locked_block_invoke
flush_updateset_locked_block_invoke_2
_page_delete_obj_by_oid_and_type
prev
%s:%u: failed assertion '%s' %s Unexpected
dbp->used_bytes != sizeof(db_page)
map_update
%s:%u: failed assertion '%s' %s %s : ERR: map_update: page offset doesn't match! 0x%x != 0x%x
dbme->pgnum == pgnum
%s:%u: failed assertion '%s' %s dbo ends past end of page ([%p, %p] > [%p, %p])
dbo<=end
i32@?0q8r^{?=I[0C]}16B24i28
update
_real_page_insert_obj
page_split
copy_end < end
dbo<end || prev_dbo<end
page_find_oid_with_flags
_page_update_obj
sync_dirty_chunks
num_chunks == dst->num_chunks
map_write
page_free
(((off_t)(pgnum) << (dst->pg_shift)) & ((1 << 12)-1))==0
page_release
_page_alloc_fetch
_add_dirty_chunk
%s:%u: failed assertion '%s' %s ERR: Can't add dirty chunks to a read-only db %s
%s:%u: failed assertion '%s' %s ERR: Chunk size is null
size
_dirty_datastore_locked
sdb_string_table_zone
db2_update_datastore_state
i12@?0i8
update_db_header
db2_check_datastore
db2_get_datastore
load_map
load_string_table
prev_dnt->_next_ptr==0
num_string_pages == 0
string_id == str_index
_kMDItemRelatedObjects
_kMDItemRelatedObjectsWithBundleID
_kMDItemLaunchString
_kMDItemActivityLaunchDates
_kMDItemRankingLaunchStrings
_kMDItemRankingLaunchDates
_kMDItemGroupId
_kMDItemShortcutLastUsedDate
kMDItemLastUsedDate
kMDItemLastUsedDate_Ranking
kMDItemUsedDates
_kStoreMetadataVersion
kMDStore
db2_dirty_datastore
copy_datastore
db2_shrink_cache
db2_flush_datastore
db2_commit_sync_datastore
db2_shadow_datastore
db2_commit_shadow_datastore
db2_commit_shadow_complete_datastore
db2_release_datastore_no_sync
dbo->used_bytes >= sizeof(external_db_obj)
db2_clear_docids
newSize<=oldSize
db2_apply
_get_string_and_length_for_id
!(flags& 0x80000000)
grow_string_table
string_table->dirty_page == 0 ||string_table->dirty_page==dnt
this_ret!=17
string_table->dirty_pgnum==cur_pgnum || (int)string_table->dirty_pgnum==-1
string_table->dirty_page==cur_dnt || string_table->dirty_page==0
db2_create_obj_postamble
db2_store_obj
db2_store_obj_inner
_insert_obj
_page_obj_exists_by_oid_and_type
db2_store_obj_preamble
db2_cas_obj
cas_obj
db2_update_obj_callback
db2_delete_obj_with_flags
(ssize_t)subiters[base+i]->count>=0
v16@?0Q8
obj iter read queue
dboi->version==0xdb2
dboi->isSuspended
iter==&dboi->subiterator
subiter_fetch_page
iter->dbp->signature == 0x64627032 || (iter->dbp->signature==0 && iter->dbp->size==0)
iter->next_dbp == 0
%s:%u: failed assertion '%s' %s obj_iter_fetch_page: ERR: page came back compressed! pgnum 0x%x
(dbp->flags & DB_PAGE_COMPRESSED)==0
subiter_fetch_next_page_block_invoke
subiter_fetch_next_page_block_invoke_2
slab_allocator.h
(char*)old_ptr + old_size == (char*)slab->current
^v32@?0^v8Q16Q24
db2_perform_callback
db2_get_obj_callback
db2_add_field_with_cache
field_flags & DB_FIELD_ARRAY_VAL
%s:%u: failed assertion '%s' %s field:%d extras:%d expected:%d %s
tag1.info.extras==tag2.info.extras
%s:%u: failed assertion '%s' %s field:%d type:%d type:%d %s
tag1.info.type==tag2.info.type
(db_uint32_t)name_id!=((db_uint32_t)~0) && (db_uint32_t)name_id!=((db_uint32_t)-2)
dbf->flags & DB_FIELD_ARRAY_VAL
B24@?0r*8r*16
_data_map_needs_sync(dst->_string_table[i]) == 0
dst->string_table[i]->dirty_page == 0
(dst->dbm->flags& 0x0001) == 0
db2_set_dirty_chunks
kMDItemLanguages
db2_garbage_collect_strings
i20@?0^{data_map_s=}8i16
v24@?0^I8Q16
db2_deserialize_cache_block_invoke
core-query.c
up_count>2
up_count==0
count==0
bad op
^v32@?0^{query_node=^{query_node}^{query_node}^{query_piece}^vSCQb1b1b1Q}8^v16^v24
^v16@?0^{query_node=^{query_node}^{query_node}^{query_piece}^vSCQb1b1b1Q}8
node:%p hash:%llx type:%d qp:%p op:%d fn:%s s:%@ lc:%p rc:%p
<null>
node:%p hash:%llx type:%d lc:%p rc:%p
<NULL>
db_compare_val
<<anon store>>
stack_depth < 2048
parent_item->state == CompareStackItemStateComplete
q && q->type == 0x04 && q->qp && q->qp->string_buffer
zh-Hans
v32@?0*8q16^q24
v32@?0Q8Q16r*24
%s%s%s
(%d) 
ALWAYS FALSE node
@0x%p
ALWAYS TRUE node
@0x%p
OR node     @ 0x%p
AND node    @ 0x%p
NOR node     @ 0x%p
NAND node    @ 0x%p
INRANGE
OUTRANGE
factor node @ 0x%p flags 0x%x value <%s%s(%s,%s,%s)>
FIELD_MATCH
OUT_FIELD_MATCH
factor node @ 0x%p flags 0x%x value <%s%s(%s
factor node @ 0x%p flags 0x%x value <%s%s[] %s %s>
factor node @ 0x%p flags 0x%x value <%s%s[%s%d] %s %s>
factor node @ 0x%p flags 0x%x value <%s%s %s %s>
we got garbage for node @ 0x%p (type %d qp 0x%p)
*16@?0*8
$time.iso(%s)
true
false
InRange
FieldMatch
v12@?0I8
created.
date
kMDItemUserCreatedUserHandle
user
kMDItemUserCreatedDate
downloaded.
kMDItemUserDownloadedUserHandle
kMDItemUserDownloadedDate
modified.
kMDItemUserModifiedDate
kMDItemUserModifiedUserHandle
printed.
kMDItemUserPrintedDate
kMDItemUserPrintedUserHandle
received.
kMDItemUserSharedReceivedDate
sender
kMDItemUserSharedReceivedSender
receivers
kMDItemUserSharedReceivedRecipient
transport
kMDItemUserSharedReceivedTransport
senderHandle
kMDItemUserSharedReceivedSenderHandle
receiverHandles
kMDItemUserSharedReceivedRecipientHandle
sent.
kMDItemUserSharedSentDate
kMDItemUserSharedSentSender
recipients
kMDItemUserSharedSentRecipient
kMDItemUserSharedSentTransport
kMDItemUserSharedSentSenderHandle
recipientHandles
kMDItemUserSharedSentRecipientHandle
flushsize
sdb2_qsort.cpp
q>=left
right < count
sdb_uniquing_zone
db-common.c
z->zone_name == __null
data == name - ht->extra_bytes - sizeof(db_uint32_t)
slot<table->size
probe!=start
bucket.used
slot < table->size
com.apple.spotlightindex.purgablectrl
com.apple.metadata.framework.sdb_page_cache
page-cache.c
dbp->used_bytes<=dbp->size
page_cache_fetch
i16@?0^{db_cache_entry_s=I^{db_page}^{db_page}iqIq}8
%s:%u: failed assertion '%s' %s Pgnum: %u dbp:%p cached_pgnum::%u cached_dbp:%p index:%d size:%d
cache->cache_pgnum[i]==pgnum || (cache->cache_pgnum[i]==0 && (flags & PAGE_DIRTY)==0)
%s:%u: failed assertion '%s' %s Bad cache page at index %d; max %d, start %d
cache->cache_entries[i].cache_dbps->flags & DB_PAGE_COMPRESSION_ENABLED
cache->cache_pgnum[i] == pgnum
cache->cache_pgnum[i] != pgnum
dbp != cache->cache_entries[i].cache_dbps
pgnum != cache->cache_pgnum[i]
dbp->flags & 0x0000000C
entry.cache_dirty==0
entry.cache_dbps == dbp
page_cache_deserialize_entries
v12@?0B8
kMDItemFSInvisible
kMDItemFSName
kMDQueryResultMatchedDisplayNameField
kMDQueryResultMatchedFields
kMDQueryResultContentRelevance
v28@?0I8S12^q16I24
executeSearchCtx_Start
executeSearchContextCracked_2 (overflow)
executeSearchContextCracked_2
lowWaterRoutine
en-US
v24@?0^{?=QQQQQdd{ci_rankingbits_s=TI}^{__CFString}iiii[3I]dddfiCBBBB}8^B16
SISearchCtx.h
ffillPtr[slot] == 0
ffillPtr[0] == 0
SISearchCtx.cpp
kr==0
v20@?0q8B16
executeSearchContextCracked
self->_stoken[i]==0
v20@?0^{search_ctx_s=^{SISearchCtx}d*^vB@?}8C16
query
i24@?0r^v8r^v16
kMDQueryResult
SpotlightRelevance
GroupId
MatchedExtraQueriesField
MenuRelevance
NewMatchedExtraQueriesField
ContentRelevance
accurate_realpath
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2148/spotlight/generic/MDFileUtil.c
rc == 0
/.vol/%llu/%llu
state_find_max_i
/Library/Caches/com.apple.xbs/Sources/SpotlightCore_Sim/Spotlight-2148/spotlight/index/ContentIndex/generic_state.c
s != 0
_data_map_rdlock
v8@?0
_data_map_unlock
.header
_data_map_commit
_data_map_sync_header
_data_map_sync_data
data ro header
data header
data storage
data offsets
data buckets
data_map_init
%s.map
%s.header
%s.data
%s.offsets
%s.buckets
data_map_destroy
disk_utils.h
(b4 & 0x80) == 0
_data_map_get_data_entry
_data_map_rehash
_data_map_get_offset_entry
_data_map_get_bucket_entry
%s:%u: failed assertion '%s' %s 
%s %s
data_map_flush
data_map_ids_get_locked_with_hash
data_map_id_get_with_key_noextra
bit_vector.h
bitIndex >= 0
newCapacity > bv->capacity
newBV
data_map_id_get_with_key
data_map_get_extra_with_key
_data_map_wrlock
data_map_get_data_locked
i20@?0^{data_map_s={_opaque_pthread_rwlock_t=q[192c]}I^{header_map_read_only}I^{fd_obj}I^{header_map}^{fd_obj}II*I^{fd_obj}II^{offset_entry_t}I^{fd_obj}II^{bucket_entry_t}IIIII{os_unfair_lock_s=I}^{bit_vector}BBBBBBi}8i16
bv->_cfbv
tmp.%s
_data_map_garbage_compact_collect
%s/sqlite_
abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789
/var/tmp
/usr/tmp
/tmp
/dev/urandom
%s:%d: sqlite3BtreeOpen failed; dbname:%s; rc = %d
sqlite3BtreeCursor failed; rc = %d
sqlite3BtreeMoveto failed; rc = %d
psid: could not find master fid; rc = %d
psid: %s : danger! master fid %d looks bad! resetting
psid: creating db: %s
(%d), page size %d
Couldn't begin transaction; rc = %d
Couldn't create table; rc = %d
%s:%d: psid: %s : danger! ps store table id %d looks bad! resetting
Couldn't create cursor; rc = %d
Couldn't insert key; rc = %d
Couldn't commit btree; rc = %d
%s:%d: release_psid_store: danger! master_fid %d looks bad.
%s:%d: psid-release: Failed to update the master fid! (0x%x)
%s:%d: sqlite3BtreeInsert failed; rc = %d
sqlite3BtreeInsert failed in _sqlite_bulkEnd; rc = %d
%s:%d: sqlite3BtreeCommit failed; rc = %d
sync_psid_store: danger! master_fid %d looks bad. not syncing.
%s:%d: psid-sync: Failed to update the master fid! (0x%x)
Couldn't sync btree; rc = %d
Couldn't commit; rc = %d
psid_insert: master fid corrupted (%d)
%s:%d: psid-insert: failed to store fid 0x%x for path %s
psid-insert: fid 0x%x for path %s
%s:%d: psid-insert: failed to store path %s for fid 0x%x
psid-insert: store path %s for fid 0x%x
_sqlite_get: buffer is too small
get_path_for_id: bogus part len %d (%d/%d/%s)
get_path_for_id: bogus looking part fid (cur fid %d, part fid %d name %s)
get_path_for_id: path index too large! (%d %d : %s)
get_path_for_id: cur fid %d should have parent fid 2 but part->fid == %d
%s:%d: psid-remove: Could not delete the file-fid record for fid %d
sqlite3BtreeDelete failed; rc = %d
%s:%d: psid-rename: begin error %d updating the file-fid record for fid %d pid %d / %s. 
%s:%d: psid-rename: Could not update the file-fid record for fid %d
%s:%d: psid-rename: failed to insert new record for %d / %s.
%s:%d: psid-rename: failed to delete old record for %d / %s
%s:%d: psid-rename: end error %d updating the file-fid record for fid %d pid %d / %s. 
fsgetpath like %d/%llx
======^^^^^ si_psid_check_sandbox sandbox (NOT IMPLEMENTED!) count:%ld
found %lld for %llx
Got error
found %x for %s
found %s for %llx
%s:%d: Invalid type:%d for schema field:"%s"
%s:%d: Error:%d setting field:%s for oid:%lld
Trying to store %@ = %@
No attribute %s for %llx
No dbo for %llx
We do have a primary query
We do have %d secondary queries
True live query: (%lld) %@
live query:%p time:%f query:%@
*warn* Got a journal exception at address:%p, map:[%p, %p), name: %s.
*warn* map:[%p, %p), size: %llx
*warn* Stat failed for name: %s.
*warn* Failed getting store cookie
*warn* Failed updating index state
%s:%d: Caught mach exception
Starting sync!
%p state: %s
clean -- skip sync
(%d)Unlinked journal %s
synced SIRef:%p recoverTime:%llu
deleting %s
Recovery Complete!
Finished sync!
*warn* Index already unavailable, error:%d, reason:"%s", options:0x%lx
*warn* Marking the index as unavailable, error:%d, reason:"%s", options:0x%lx, path:%s
fstat(%d) err: %d
%s:%d: low space for device %d (%s)
%s:%d: Couldn't get index property dictionary
Creating index property dictionary
*warn* Failed storing sizes (%d)
%s:%d: Failed to add field "%s", length:%ld, rc:%d
%s:%d: Failed to store the dbo for field "%s", rc:%d
%s:%d: db_commit_shadow_datastore err:%d
%s:%d: commitShadowReverseStore err:%d
%p open index state error:%d
%s:%d: %p open index state error:%d
%s:%d: %p write index state error:%d
%p write state:%s
%s:%d: (%p ver:%d main:%s sdb:%s path:%s scan:%d %s, live:%d %s)
(%p ver:%d main:%s sdb:%s path:%s scan:%d %s, live:%d %s)
%s:%d: db_shadow_datastore err:%d
%s:%d: shadowReverseStore err:%d
%s:%d: si_set_index_state err:%d
%s:%d: db_commit_sync_datastore err:%d
%s:%d: commitSyncReverseStore err:%d
%s:%d: db_flush_datastore err:%d
%s:%d: flushReverseStore err:%d
%s:%d: store dirStore overlay err:%d
%s:%d: db_store_dirty_chunk_info err:%d
%s:%d: reverseStoreStoreDirtyBitmap err:%d
%s:%d: Failed to fetch the bundleId/identifier field, oid:0x%llx(%lld), rc1:%d, rc2:%d
%s:%d: Invalid type for bundleId/identifier field, oid:0x%llx(%lld), type1:%d, flags1:0x%x, type2:%d, flags2:0x%x
%s:%d: Missing identifier field, oid:0x%llx(%lld), type:%d, flags:0x%x
%s:%d: Missing bundleId field, oid:0x%llx(%lld), type:%d, flags:0x%x
%s:%d: Failed to update the dbo for oid:0x%llx(%lld), flags:0x%lx, rc:%d
Finished initial indexing of %s
Advanced transaction id
SISetScanCount:%ld full:%s
full scan:%ld
SISetScanCount: Counted %d live indexes
*warn* %s called on fs-only index
%s:%d: Couldn't create live store at %s.
si_set_scancount: Counted %d live indexes
Starting cleanup for transactions below %d
Attempt to merge (%s/%s/%s/%s)
Failed to transfer live indexes
Merging (%s/%s/%s/%s/%d)
%s:%d: invalid range - size:%d start:%d count:%d
size:%d start:%d count:%d
mergeCount:%d != count:%d
Skipped merge (%s/%s/%s/%s/%d/%d)%s
*warn* Failed to reset cpumon_params
Not tracing com.apple.spotlight.mds.index-darkwake %s %d %s %s %ld
Outer Merge - count:%d live:%s %s
%s:%d: error (%d) getting free space
*warn* Merge canceled - low disk space (%lld %lld %lld)
used:%lld, free:%lld
cindex was added during merge, old start %d new start: %d cindex count:%d
%s:%d: Merging failed
%s:%d: Failed to merge; index at %d is writable
%s:%d: Failed to merge; index at %d is current
Inner Merge - count:%d live:%s %s
%s:%d: open remp failed: %s
remapping canceled
Updating item for remap failed with error %d
%s:%d: ~~~ No ctx->usedDates.
Index contact graph data; %ld items
Defer work for %@
Enqueue work for %@
Callback for %@
#index too much enqueued (%lld/%lld), bundleID:%@ - deferring callback
%s:%d: Invalid journal entry, diskRecord:%p, journalEntryOffset:%lld, journalEntrySize:%ld
%s:%d: Invalid journal entry, magic:0x%08lx, journalEntryOffset:%lld, journalEntrySize:%ld
%s:%d: Invalid journal entry, size:%ld, journalEntryOffset:%lld, journalEntrySize:%ld
%s:%d: Invalid journal entry, size:%ld, extraSize:%ld, journalEntryOffset:%lld, journalEntrySize:%ld
%s:%d: Invalid journal entry, checkSum:0x%08lx, storedCheckSum:0x%08lx, journalEntryOffset:%lld, journalEntrySize:%ld
%s:%d: Invalid journal entry, bundleID:%p, journalEntryOffset:%lld, journalEntrySize:%ld
Finalizing journal %p %p %lx %s
%s:%d: bad identifier %@
*warn* Failed to fetch the dbo for identifier:%@, bundleID:%@, rc:%d
*warn* Failed to find the db field "%s" for identifier:%@, bundleID:%@, rc:%d
*warn* Failed to fetch the dbo for relatedIdentifier:%@, bundleID:%@, rc:%d
Found the dbo for relatedIdentifier: %@, bundleID: %@, identifier: %@, oid: %lld
*warn* Failed to fetch the dbo for relatedIdentifier:%@, bundleID:%@, identifier:%@, rc:%d
Updated "%s" field for relatedIdentifier:%@, bundleID:%@, identifier:%@
Failed to fetch the dbo for relatedIdentifier:%@, bundleID:%@, rc:%d (dropping %@)
Remapped related identifier for %@ to relatedIdent %@, bundleID:%@
Deleting item, bundleID:%@ identifier:%@
Index Add bundleID:%@ identifier:%@
Index update bundleID:%@ identifier:%@
isDummy :%@ %@
*warn* update requires existing item :%@ %@
Deleting importer fields failed, rc:%d
db didn't find any existing values
%s:%d: marking item as rendered/engaged failed
%s:%d: db get field failed in counts code
date:%x
found parent oid: %lld (%@) for %@
%s:%d: Failed to update the index for bundleId:%@, serial:%lld, options:0x%x, oid:0x%lld(%lld), updateErr:%d
%s:%d: No write back for bundleId:%@, identifier:%@ serial:%lld, options:0x%x, oid:0x%lld(%lld)
incoming or outgoing counts size mismatch: identifier=%s incomingArraySize=%lld outgoingArraySize=%lld incomingMailArraySize=%lld outgoingMailArraySize=%lld incomingSMSArraySize=%lld outgoingSMSArraySize=%lld
~~~ authorIsMe, recipients: %@
~~~ !authorIsMe, authorContacts: %@
Schedule index flush
Flush not required
*warn* Skipping %@ %@ already had %@
Skipping :EA:%@ %@
New last used date: %@
Skipping :MD:%@ %@ already had %@
%s%@ = %@
Skipping :MD:%@ %@
si_delete_attributes_inner oid: %lld
%s:%d: Deleting item failed with error %d
couldn't get dbo for oid: %lld
Deleting related item for %s (%s , %s)
Found related item for %s (%s , %s), oid: %lld
Failed to find related item for %s (%s , %s)
Index delete bundleID:%@, identifier:%@
Get_base for journal %s
Error writing to log file: %d
%s:%d: recycle %s
Failed to get cpumon_params
*warn* Failed to set cpumon_params
_SIIssueSplit called
%s:%d: Unexpected transaction id %d != %d
Deleting empty object failed with error %d
%s:%d: Error %d storing dbo(%llx,%x,%x,%llx,%llx,%llx,%x)
%s:%d: Error %d updating dbo(%llx,%x,%x,%llx,%llx,%llx,%x)
Creating New Index
Opened %s successfully
Try %s/%s
%s:%d: Could not create new content index
Creating store at %s/%s.
%s:%d: Could not create store at path '%s/%s'
%s:%d: Could not create reverse dir store at path '%s'
Failed at %d (%d)
Index shut down starting for index at %s.
%s:%d: setDir 2 error %d (%s)
Index closed for %s after %f seconds.
Index shut down finished for index at %s after %f seconds.
Shedulers stopped for %s after %f seconds.
awakenPreheat entered for %p
awakenPreheat continued for %p
awakenPreheat skipped for %p
Shutdown started
Shutdown ended after %f seconds
%s:%d: rebuild index for tokenizer (%d) %@
setting low latency: %s
Suspending root scheduler for %p (%s)
Created volume scheduler %p
Created index scheduler %p
Found %s, size:%lld, syncCount:%d, first:%d, last:%d
Moving %s to %s, syncCount:%d, first:%d, last:%d
No journals to replay for %p, syncCount:%d, first:%d, last:%d, journalSerialNumber:%lld, consumedJournalSerialNumber:%lld
Replaying %d journals for %p, syncCount:%d, first:%d, last:%d, journalSerialNumber:%lld, consumedJournalSerialNumber:%lld
No defer journals to replay for %p, deferSyncCount:%d, first:%d, last:%d, journalSerialNumber:%lld, consumedJournalSerialNumber:%lld
Replaying %d defer journals for %p, syncCount:%d, first:%d, last:%d, journalSerialNumber:%lld, consumedJournalSerialNumber:%lld
Missing %s, syncCount:%d, first:%d, last:%d
Replaying %s, syncCount:%d, first:%d, last:%d
Unlinking dropped file %s
journalSerialNumber:%lld, consumedJournalSerialNumber:%lld
Played back %ld records (skipped %ld), read %lld/%lld bytes
Do attribute transfer.
SITransferAttributes from %lld to %lld
Leave.
Move for bad oid: %lld
No parent for oid: %lld
oid: %lld moved to parent oid: %lld
parent %lld unchanged for %lld
%s:%d: Got parent %lld for %lld. Expected %lld (doc %llu)(%d)
%s:%d: Write error %d updating parent
Fixed up (formerly) childless item %lld, new parent %lld
Skipped fix up; item %lld, new parent %lld %s
%s:%d: move dropped %llx retry count exceeded
oid: %lld moved to parent oid: %lld (file)
Enqueue attribute change %llx.
PUSH REPAIR oid: %lld, f:%x
Begin setattr with %ld items on %ld threads
%s setattr
Index going idle; sync immediately
Set attributes waited for %f seconds
Dummy coming in oid: %lld
deleting oid: %lld
%s:%d: ctx->source: %d != source: %d oid: %lld options: %x
DEQUEUE oid: %lld, o: %x t: %d
Duplicate in flight oid: %lld
*warn* Unexpected transaction id %d. Expected %d. Attempting repair
*warn* Transaction id is now %d
*warn* Items's transaction id %d is too low for the current index %d. Discarding oid %llx.
isDummy oid: %lld
oid is zero: %lld
Couldn't get the group id.
Dictionary claims the importer was the origin, but trail tells us it was not. Treating as normal setAttr call.
%@ = %@
%s:%d: Couldn't update index oid: %lld options: %x updateErr: %d resolveErr: %d
No write back for %lld
Schedule index flush.
All recovery items processed
Dummy for oid %lld
Canceled oid: %lld
*warn* Starting internal consistency check for %s
*warn* Finished internal consistency check for %s. Checks: %d Missing:%d Inconsistent:%d Missing deletes:%d
Index/sdb inconsistency for (sdb)oid %lld; index has oid %lld. doc id: %lld. path: %s
delete attributes consistancy oid: %lld
delete attributes consistancy 2 oid: %lld
Index/sdb inconsistency; wrong doc id for oid %lld; has %lld. path:%s
delete attributes consistancy 3 oid: %lld
*warn* Skipping consistency check for %s
%s:%d: Invalid journal entry - nil bundleID, magic:0x%08lx, size:%ld, pos:%ld, end:%ld
%s:%d: Invalid journal entry, magic:0x%08lx, size:%ld, pos:%ld, end:%ld
%s:%d: Invalid journal entry, magic:0x%08lx, size:%ld, pos:%ld, end:%ld, len:%ld(%ld)
Played back %ld records (skipped %ld), read %lld/%lld bytes, consumedJournalSerialNumber:%lld, minReplaySerialNumber:%lld, maxReplaySerialNumber:%lld
Deactivating journal %p %p %lx %s
Interrupting indexing; process quitting
Playback finished.
Set attributes waited for %f seconds%s
%s:%d: Invalid bundleID %ld %@
Activated journal %p %p %lx %s
Size %d: %lld
%s:%d: Failed to get kMDStoreAccumulatedSizes
Count %d: %d
%s:%d: Failed to get kMDStoreAccumulatedCounts
%s:%d: Failed to get metadata dbo
restored localized terms
new loc term %d
%s:%d: Failed to protect fd with %d %d
%p Open fd %s
last_crash_delta: %ld for %s
%s:%d: check_crash_state: %d for %s
%p si state: %s
%s:%d: %p CIMetaInfoRead err:%d
%s:%d: %p Too many live indexes %d/%d
%s:%d: %p db_update_datastore_state err:%d
%s:%d: %p ContentIndexOpenBulk err:%d
opened SIRef:%p from fast flush with recovery time %llu
%p open from fast flush canceled:%d
*warn* %p open from fast flush failed:%d
%s:%d: Ignoring missing path store
%s:%d: reverseStoreUpdateState err:%d
%s:%d: si_write_index_state err:%d
%s:%d: openReverseStore err:%d
%s:%d: %p ContentIndexUpdateState err:%d
%s:%d: %p si_write_index_state err:%d
opened index %p with recovery time is %llu)
%p _SIOpenIndexFilesWithState: %d
%s:%d: open meta info error %d
db_check_datastore: %d
file didn't exist, try shadow
reverse store state: %x
*warn* datastore dirty, reverse store needs shadow -- forcing repair (%u, %u, %u)
*warn* datastore needs shadow, reverse store dirty -- forcing repair
*warn* Index version %d out of date, expected %d, recovering
*warn* Index version %d out of date, expected %d, reindexing
ContentIndexOpenBulk: %d opened %p with recovery time %llu
Could not open existing content index
*warn* datastore clean, index dirty, recovering...
Disable updates for index in transaction %d
renaming: %s to %s
%s:%d: move error:%d, %s to %s
unlink (%s)
%p repair - catchup scan time stamp: %s, base: %ld, repair count: %d, remair max: %ld
*warn* clean scan count mis-match expected:%d got %d
*warn* shadow scan count mis-match expected:%d got %d
*warn* clean live count mis-match expected:%d got %d
*warn* shadow live count mis-match expected:%d got %d
%s:%d: %p read index state error:%d
%s:%d: %p invalid version:%d
%p read state:%s
%p _SIOpenIndex: %d %s
Gathering size data for repair
Gathering size data for repair (%lld, %lld)
Skipping because index is shut down
flushing idle index at %s.
%s:%d: ContentIndexSyncIndexBulk err:%d
*warn* Failed setting store cookie (%d)
shut down starting for spindle:%d count:%d
shutdown setup complete for spindle:%d after %f seconds.
shutdown sync complete for spindle:%d after %f seconds.
shutdown sync-fsync for spindle:%d after %f seconds.
shutdown commit data complete for spindle:%d after %f seconds.
shutdown commit-fsync for spindle:%d after %f seconds.
shutdown commit header complete for spindle:%d after %f seconds.
shut down complete for spindle:%d after %f seconds.
Index shut down starting for index at %d %s.
%s:%d: setDir 1 error %d (%s)
SICloseIndex, terminating:%d
*warn* Starting duplicate oid check for %s
*warn* Finished duplicate oid check for %s. Missing deletes:%d
*warn* Skipping duplicate oid check for %s
Recovery issued for %s
defer vacuum
Vacuum scheduled
Vacuum needed%s
Merge(2) scheduled
Merge(2) started
Vacuum started
full vacuum needed - count: %lld, live count: %lld, delete count: %lld, live delete count: %lld
Starting forced sync!
Finished forced sync!
oid: %lld > max_oid: %lld
Delete (bulk) oid: %lld
time stamp%s
Do attribute change.
%s:%d: bad object %@
%s:%d: missing bundle %p 0x%x %@
#index too much enqueued (%ld); defer callback for work unit of %ld
Execute query %@
Starting query %@
QueryId=%{signpost.description:attribute}lld CurrentQoS=%{signpost.description:attribute}x JobType=%{signpost.description:attribute}d
Search waited %f seconds on the scheduler at qos 0x%x
finishRegisterQuery %@
Search was active (setup) for %f seconds on the scheduler at qos 0x%x
no tags
long defer query for %@
defer query for %@
do defer query for %@
don't defer query for %@
Stalled getAttr because task had pending sets
_SIStartPreheatScheduler for %p
Started initial indexing of %s
*warn* %s called on fs-only or null index %p
%s:%d: No live index
Not supported for read only index
%s:%d: Could not create new live index %@
%s:%d: Prepare for transaction %d
%s:%d: transfer_live_indexes failed
verifying %s
%s:%d: verify index: %s, err: %d, (%d %d) , (%d, %d)
%s:%d: %@
Do directory move.
*warn* invalid corrections commit
*warn* no correction dict passed
*warn* exceeded max for %@
*warn* correctDict exceeded max for %@
%s:%d: store_stream_init err:%d
%s:%d: store_stream_flush write err:%d
%s:%d: store_stream_flush sync err:%d
Push %p to tags %p
Created tag bag for %p
QOS work_fun: %d %p
qos: 0x%x %@
QOS enqueue_work: %d %p
Freeing %p
Remove scheduler %s from %s
Scheduler qos: 0x%x relative_priority: %d %@ %@
Workqueue qos: 0x%x relative_priority: %d %@ %@
%d != %d
Stopped %s
boosting %@
Peek for %p to %p
Peek tags for %p on queue %s
Found tags for %p on queue %s
No tags for %p on queue %s
Peek for  %p on queue %s with tagbag %p
Found oid bag for %p on queue %s
Empty oid bag for %p on queue %s
No oid bag for %p on queue %s with tags %p
Enqueueing result: %d %d
Canceling result queue
*warn* Non-numeric in factor array at index %ld: %@
*warn* Bad value in factor array at index %ld: %@
completion weight array too large (%ld): %@
*warn* completion weight array incomplete (%ld): %@
*warn* Non-numeric in weight array at index %ld: %@
*warn* Bad value in weight array at index %ld: %@
Completion v2 options: 0x%x count:%llu
Completion v1 count:%llu
Creating unescaped string %s with from %s
 weight_F:%g score_F:%g len_F:%g fragment_F:%g wf1_F:%g wf2_F:%g wf3_F:%g phrase_F:%g field_F:%g thread_F:%g shortcut_F:%g used_F:%g age_F:%g
len:%ld scoringFragmentCount:%ld fragmentCount:%ld age:%f score:(%llu, %llu) weight:%d computed score:(%g)
Comparing to parent at %ld (%g) %g
Dropping weak child %ld (%g) %g
Dropping weak parent (%g) %g
Dropping similar completion of length %ld
Pop: %ld %ld %f (%llu,%llu) %d (%g)
Dropping dangling parent (%g) %g
Creating suggestion string %@, type %d, with completion %s length:%d weight:%d score:%g
Canceling query in SICancel at QoS: %d for job id:%p
QOS si_sdb_enqueue: %d priority: %d
No RootDirectory set for Suggestions
%s:%d: fstat err:%d %s
%s:%d: FAT: bad file size:%d (expected %d - %d) %s
%s:%d: fd_mmap err: %d, %ld
%s:%d: open %s err: %d
%s:%d: COMP: bad file size:%d (expected %d - %d) %s%s
%s:%d: fstat err:%d %s%s
%s:%d: FLAT: file size:%lld (expected %lld - %lld) %s%s
%s:%d: FLAT: bad file size:%lld (expected %lld - %lld) %s%s
%s:%d: Could not recover %s
syncTrie took %f seconds
Re-burst!
%s:%d: copyFile error, src: %s, dst: %s
shadowIndexCompactDirectory took %f seconds
%s:%d: write err: %d, %s
shadowIndexDirectory took %f seconds
%s:%d: open err: %d, %s
%s:%d: fd_open err: %d
%s:%d: read err: %d
%s:%d: pread err: %d
%s:%d: pwrite err: %d
shadowIndexArrays took %f seconds
Merge update set ... 
Merge %lu terms
Merge Canceled
Copied term data: %f seconds
Computed ranges: %f seconds
Flush yielded %d times.
Merged terms: %f seconds
%s:%d: compressPostings err:%d
Compressed postings data: %f seconds %f total
flush positions %f
Done
Merge took %f seconds
Total merge time: %f seconds
Mapped Bases: %luKB (%f%% used)
Sparse Bases: %luKB for %d entries (%f%% used)
String arrays: %lluKB %f%% used
resolve trie terms bc:%u : %f seconds
Index %lu starting at %lu ending at %lu
node count: %d, compare count %d
resolve flat page terms: %f seconds
Applier %lu starting at %lu ending at %lu
bucket[%ld] %x %d %s
%s:%d: can't resolve flat store page
%s:%d: bad flat store page size: %x
String: %s
%s:%d: invalid flat store page (0x%llx)
### trie processing - %d ###
%s:%d: Setting index emergency state
%s:%d: Clearing index emergency state
%s:%d: Reaffirming index emergency state to true
%s:%d: fstatfs err:%d
No meta info
invalid meta info, cleanGeneration:%ld, shadowedGeneraton:%ld
no data in index - rebuilding, result:%d
%s:%d: Failed creating %s/%s, result:%d
%s invalid head (%d), will rebuild
%s invalid head (%d), will try to recover
preflight index %s base:%ld count:%d
Unclean shutdown of %s/%s; needs recovery
Could not open %s/%s; needs recovery
recovery not allowed for %s/%s
index %s base:%ld count:%d
could not recover %s/%s
could not open %s/%s
recover needed, scan date: %s, last valid doc id:%ld, %s
deleting index %s/%s
limbo counts live:%ld scan:%ld recover:%ld internal:%d priority:%d setAttr:%d migrate:%d
*warn* time stamp should be more recent new:%ld, old:%ld
%s:%d: Got error %d passing sync counts to journals
%s:%d: index invalid
%s:%d: preShadow error:%d %d
%s:%d: postShadow error:%d %d
%s:%d: indexCommitShadow error: %d, %s
%s:%d: indexShadowAndCommitBulk error:%d
%s:%d: indexShadowFiles error: %d, %s
%s:%d: pre-error %d
%s:%d: pre-error:%d
%s:%d: postSync error:%d %d
%s:%d: indexCommitSync error %d %s
%s:%d: indexCommitSyncBulk error:%d
%s:%d: indexPrepareForSyncBulk error:%d
%s:%d: preSync error:%d %d
%s:%d: indexPerformSyncBulk error:%d
%s:%d: indexPerformSync error: %d, %s
%s:%d: indexPrepareForSync error: %d, %s
%s:%d: corrupt index (%s), will rebuild
_CIUpdateContent oid: %lld
delete docId: %d oid: %lld
reassign docId: %d
dup oid (%lld)
%s:%d: mmap err: %d
%s:%d: expandUnsafeMap errno: %d err: %d
%s:%d: invalid offset %d, %p (%d)
%s:%d: invalid offset %d, %p (%d) s:%d l1:%d l2:%d
%s:%d: %s marking invalid
%s:%d: Got exception on %s addr:%p  sres:%d file_size:%lld dev:%d ino:%lld
%s:%d: Got exception on %s addr:%p start:%p map end:%p sres:%d file_size:%lld dev:%d ino:%lld
%s:%d: error %d updating sync count
Force split for large index %lld (%lld) %lld (%lld)
Force split for high delete count; %d %d
should exit
should cancel
resolve term ids: %f seconds
creating index at %s
%s:%d: stat err: %d
%s:%d: bad file size: %lldd, min size %lldd, %s
%s:%d: open file error: %d, %s
%s:%d: bad file size: %d, min size %d, %s
%s:%d: map error: %d, size: %lld, %s
%s:%d: ftruncate error: %d, size: %lld, %s
%s:%d: invalid file (%d, %d, %d)
%s:%d: Invalid version (%d) expected (%d)
open index at %s
%s:%d: Failed to open path index
%s:%d: Ignoring failed dirstore open for corespotlight
last term offset: %llu
%s:%d: unexpected sync count %lld %lld %lld %lld, expected %lld
head:%lld fat:%lld compact:%lld flat:%lld dir:%lld
%s:%d: index shadow err:%d at %s
%s:%d: index commit shadow err:%d at %s
%s:%d: can't shadow %s
shadowIndexTermIds took %f seconds
shadowIndexPositionTable took %f seconds
%s:%d: write err: %d
%s:%d: write(%d) %d err: %d, %s
shadowIndexGroups took %f seconds
Validating %s
%s:%d: %s invalid type %d
%s:%d: %s error walking terms
%s:%d: %s error walking directoyr store
Validate %s complete %d
oid: %lld depth: %d offset: %llu 
%s:%d: invalid term length: %d
%s:%d: invalid termID: %d, max: %d
%s:%d: invalid posting offset: %lld, max: %lld
%s:%d: invalid position offset: %lld, max: %lld
%s:%d: trying to modify read only index %s
%s:%d: indexMarkDirty failed - state:%x closing:%d
index updates disabled for %p (%d) from %d
skip index updates disable for %p (%d) from %d; flags 0x%x, disabled at %d
%s:%d: %s marking invalid at %d
%s:%d: can't commit %s
%s:%d: error copying (%s)
%s:%d: Unrecoverable error: Missing postings file (%s)
%s:%d: Unrecoverable error: Missing positions file (%s)
*warn* Unrecoverable error: Missing shadow head file (%s) %d
*warn* Unrecoverable error: Missing data in index head file (%s) %d %d
%s:%d: Unrecoverable error: Malformed index head file (%s)
%s:%d: Unrecoverable error: could not open update file (%s)
%s:%d: Unrecoverable error: could not recover term id file (%s)
%s:%d: Unrecoverable error: could not recover term id file (positions) (%s)
%s:%d: Unrecoverable error: could not recover term index (%s)
%s:%d: Unrecoverable error: could not recover path index (%s)
%s:%d: Unrecoverable error: could not recover groups file (%s)
%s:%d: Unrecoverable error: could not open shadow head file (%s) %d
%s:%d: Unrecoverable error: could not read shadow head file (%s) %d, %d
%s:%d: Unrecoverable error: could not open index head file (%s) %d
%s:%d: Unrecoverable error: could not write index head file (%s) %d
*warn* recover canceled (%s)
close requested
%s:%d: trying to set to invalid index %s/%s
setDocumentAttributes canceled
%s:%d: trying to add to read only index
%s:%d: TermUpdateSetCreate failed
%s:%d: indexMarkDirty failed
%s:%d: %s setDocumentAttributes error - bad oid/docid mapping oid: %lld, docId: %d, old oid: %lld 
%s:%d: Got error %d
Out of space growing payloads.
Mark index needs flush
should flush, tc:%d, limit:%d
positions exceed limit, used:%ld, limit:%ld
Terms + Postings exceed limit, used:%ld, limit:%ld
Terms + Postings + Positions exceed limit, terms used:%ld, postings used:%ld positions used:%ld limit:%ld
should flush term info (global), used:%ld, max:%ld
should flush, all:%ld, used:%ld, limit:%ld, total limit:%ld
Flush for high delete count
Got unexpected 0 payloadCount. Attempting repair.
%s:%d: trying to delete from invalid index %s/%s
%s:%d: trying to modify read only index %s/%s
%s:%d: deleteDocument error - docId (%lld) >= max (%d) 
*warn* deleteDocument error: mismatch oid: %ld docId: %ld idxOid: %ld
%s:%d: reassignDocument error - docId (%lld) >= max (%d) 
*warn* reassignDocument error: oid mismatch oldOid: %ld newOid: %ld docId: %ld idxOid: %ld
%s:%d: invalid index: %s
%s:%d: index alreay compact: %s
%s:%d: cant compact writable index: %s
computer new term ids time (%f)
%s:%d: Failed compacting postings
%s:%d: Failed compacting positions
compact_trie time (%f)
index_compact time (%f) - %d
%s:%d: fstatfs(%d) err:%d
%s:%d: not enought space to compact index - needed: %lld, avail: %lld, device: %lld
%s:%d: NULL payload fd obj
Payloads: %lluKB %f
%s:%d: ftruncate(%s, %lld) err: %d
%s:%d: open error; %ld
%s:%d: pwrite error; %ld
resolve term id offsets: %f
make hot: %f
write postings: %f
update term id offsets: %f
Memsize: %luKB %f%% used
%s:%d: ftruncate err: %d
%s:%d: Indexing watchdog fired, thread:%p delta:%llus, startTime:%.3f, itemCount:%lu, perItemCost:%lu resumeTime:%.3f endtime:%lld
%s watchdog for %s
Starting the indexing watchdog, timer:%p, delta:%llus, startTime:%.3f, itemCount:%lu, bundleID:%@
Stopping the indexing watchdog, timer:%p, delta:%llus, time:%.3f, itemCount:%lu, bundleId:%@
mmap (%p) %llx-%llx
%s:%d: mmap(%p, offset: %llx, size: %ld) error:%d, fSize:%lld
Memsize: %uKB %f%% used
%s:%d: offset(%lld) < freeRegion (%lld)
%s:%d: invalid storage data
%s:%d: storageInit - inFdPtr == NULL
%s:%d: mmap(%d, %lld) err:%d, %s
sync pages (%d, %d) took %f seconds
%s:%d: _storageExpand %s error:%d
%s:%d: ftruncate(%lld) error:%d
%s:%d: ftruncate error %d
mmap (%p) %llx-%llx (%llx-%llx)
%s:%d: head:0x%llx, freeRegion: 0x%llx
Store Update Set (t %d, d %d, p %d) took %f seconds
%s:%d: invalid store version %d, expected %d or %d
%s:%d: Failed restoring update set for paths
%s:%d: invalid termLen %d
%s:%d: invalid  postingCount %u > %u
%s:%d: storeStream error %d
%s:%d: lastPosting==0
%s:%d: invalid doc id %d exceeded (%d, %d)
%s:%d: value==0
%s:%d: posPtrStart==0
%s:%d: read error %d
%s:%d: posStart[len-1]: %d
%s:%d: invalid position data %p %p %d
%s:%d: invalid position data %d %d
Restore Update Set (t %d, d %d, p %d) took %f seconds
invalid type for %d %@
%s:%d: invalid content token (%d) for %s
%s:%d: init storage failed %s
%s:%d: init storage failed %s; could not read header got %ld bytes
%s:%d: no path for shadow
leafPageOffset: %llx
_directoryStoreGetParent failed. leafPageOffset: %llx
Move %llx/%llx
Set %llx/%llx
%s:%d: Move would loop in reverse directory store, skipping %lld to %lld
Tree page:%p level: %d depth: %d origin: %d size: %d
page:%p depth: %d idx: %d offset: 0x%8.8llx
Flat page:%p level: %d depth: %d origin: %d size: %d
*warn* Flat page: 0x%8.8llx depth: %d exected: %d
oid: %lld parent: %lld
completed flushReverseStore
skip flushReverseStore; %x
completed commitSyncReverseStore: %d
shadowReverseStore: %d
%s:%d: copyFile error, src: reverseDirectoryStore, dst: reverseDirectoryStore.shadow
Completed shadowReverseStore
%s:%d: Failed shadowReverseStore
Completed commitShadowReverseStore
Shadowing reverse store on open
%s:%d: Copy file failed for %s
%s:%d: Failed to read reverse store file %s
%s:%d: Failed to write reverse store header %s
%s:%d: Unexpected state in shadow header %x
Successfully recovered from %s to %s with state %x
%s:%d: update state (%d) failed err:%d at %d
skip dbo:%lld %@
Passing up deletion
Attempt to append to queue failed. Releasing result batch
live query nodes: %@
*warn* failed to parse %s
Computed time (%lld) %s from %s
Passing up out of results
Index Should Merge id:%d %p %s
at qos 0x%x
bitCount: %ld
Preiterate
QueryId=%{signpost.description:attribute}lld CurrentQoS=%{signpost.description:attribute}x
Get doc set!
alloc %ld cinodes for %ld noded
Got NULL context from processNodes
Computed doc set in %f seconds!
Couldn't create raw sdb iterator
1 - count: %ld
2 - count: %ld
QueryGatherIndexInfo
Looping %ld
Query detected merge is needed
Result count: %lld
readSDBForOids %p item count: %ld
readSDBForOids early exit %p
Created iterator for %ld oids in %f seconds!
Read/evaluated %ld dbos in %f seconds!
QueryReadSDB
readSDBForOids exit %p
Iterator out of results after %ld items
duplicate oid 0x%llx
Query was canceled
%s:%d: Assertion caught during query
Pack match bits %llx
Clipping at 1.0 for %llx
Clipping at 0.0 for %llx
No last opened date for %llx
No data for uniqued array %u
No data for uniqued string %u
No useful date for %llx
Set match bits %llx
Encoded results in %f seconds!
%p found %ld results
Enter %s
Batch size: %ld. Timeout: %f. Waiting: %d
Refilled oids. More: %s
Scope checked %ld dbos in %f seconds!
Permission for %ld items!
query %p group %d done
Not enough data yet. Max per group: %ld Unfilled: %ld
query %p skip checkGroupDone
Not enough data yet. More to collect: %s
Read/evaluated %ld oids in %f seconds!
Count: %ld
Real Count: %ld
Available at %ld loops: %ld (%ld) keep going:%d
Canceled; leaving with: %ld
QueryRefillOids
Available after %ld loops: %ld (%ld)
(Full)Available: %ld
(Full)Count: %ld
Timeout
keepGoing: %d
Couldn't gather.
Abandoning %ld %ld %d
(Loop end)Available: %ld
Extracted %ld oids in %f seconds! bad batch:%s batch full:%s timeout:%s keepgoing:%s iterator:%s check needed:%s
Early exit %llx%016llx %d
No permission for %llx
enqueue at qos 0x%x
%d start, end:%d
Skip at %ld
extracting field id %d: '%s'
%s:%d: Expected valid doc set type for %p. Got %d
Updates from %d to %d
1 Disk from %d to %d
Got (%d to %d) Squashed (%d to %d)
Range %d to %d
Finished iterator. Squashed (0 to %d) end:  %d
OID Range %d to %d
Found: %ld
%s:%d: Caught assertion for iterator %p %s
Bad skip doc set %p gen %u %u %u
from %d to %d
Read range: %d %d
%d items
Enumerator for range: %d %d
Yes: %d
No: %d
!!! No: %d (size %d)
Cleaning up %d for child %d
from %d to %d, with old end at %d
actually from %d to %d
Computed age stamp %d
Dropping older item %d > %d
%s %d %llx%016llx %d
adding oid %ld (%ld %ld)
%s:%d: Date too distant while restting sentinel
%s:%d: Failed reseting sentinel date
%s:%d: Beyond max entries in counts or tried adding out of order
%s:%d: Beyond max entries in counts or tried adding out of order in adding new
%s:%d: Incorrect data size in counts code
%s:%d: Failed updating render/engagement data
%s:%d: unexpected db signature %x
%s:%d: Error unliking store.updates: %d
%s:%d: Error opening store.updates: %d
%s:%d: Error storing dirty sdb pages: %d
%s:%d: err:%d num_chunks:%d > max_chunks:%d
*warn* Restore error: %d, recovering from shadow
*warn* Got exception on %s addr:%p start:%p map end:%p sres:%d file_size:%lld dev:%d ino:%lld
crash data for %s in volume %s?!?
%s:%d: Detected recurring crashes 3 hour window
%s:%d: Detected recurring crashes during compacting
%s:%d: crash count: %d
%s:%d: invalid crash state file (%s) deleting
%s:%d: Retry operation on address %p (%p) %f
%s:%d: Repeated error on address %p
%s:%d: thread_get_state: %s
%s:%d: thread_set_state: %s
%s:%d: state)Got some sort of signal!
%s:%d: state identity)Got some sort of signal!
Allocating threads and such.
Adding handler slot:%u port:%d
Creating exception handler thread
Done allocating threads and such.
starting exc_thread loop in task %d
Dropped handler slot:%u port:%d
Creating CI mergeIndexDataTrampoline thread
%s:%d: Exception on new index merging
Merging started (%s) count:%d
%s:%d: open termIdFile error: %d
%s:%d: ftruncate error: %d
start %lld
%p offset %lld
%s:%d: Nextlink out of bounds %lld %lld
%s:%d: %lld %lld %lld
%s:%d: Element outside legal range %lld>=%lld type %d (starting max %lld)
merging lat id %d
%s:%d: pwrite error: %d
No packing found for %u at index %lu
%s:%d: Exception raised during merging
Merging %s (%s)
Merge statistics: %s
%s:%d: missing positions for doc:%d %d %d
Impossibly large position added (%x) at %ld from %d
Had to take slow path, items out of order. Inserted at index %d of %d
Mismatched changed count for %lld, count %d
Bad doc id %lld (adjusted:%d) encountered during checking (current = %lld)
Mismatched changed count %d for %lld
%s:%d: Caught exception  on merge positions %p
%s:%d: Caught exception  on merge postings %p
fsync time %f - %s
%s:%d: fd_open failed, path:%s, name:%s, flags:0x%x, parent_fd:%d, errno:%d
fd_open failed, path:%s, name:%s, flags:0x%x, parent_fd:%d, errno:%d
%s:%d: fd_ptr instance was invalidated
open error NULL obj
%s:%d: faccurate_realpath() failed, parent_fd:%d, path:%s, flags:0x%lx, errno:%d
%s:%d: Invalid parent path, currentPath:%s, path:%s, flags:0x%lx
*warn* too many open files, err: %d, closing inactive and trying again
*warn* (%d) - %@
fd limit %d
%s:%d: Open failed on %s child of %d with error %d
%s:%d: ftruncate(%d %s, %lld) error:%d
%s:%d: lseek(%d %s, o:%lx, w:%d) err:%d
%s:%d: pread(%d %s, o:%lx, s:%d) err:%d
%s:%d: pwrite(%d %s, o:%lx, s:%d) err:%d
%s:%d: write(%d %s, s:%d) err:%d
%s:%d: rename(%s, %s) err:%d
msync(%p) err %d
%s:%d: Read depth %d in serialized value set
%s:%d: Read duplicate child entry at %d(%d) in serialized value set
%s:%d: missing end marker
FindTermIDsContext %p empty bucket %x %s %s
FindTermIDsContext %p invalidate fuzzy pages bc: %u fc: %u gc: %u mc:%u lc: %u s: %f took %f seconds
FindTermIDsContext %p fuzzy sort %d flat pages: %f seconds
FindTermIDsContext %p, fuzzy bc: %d -> %d
sort %d flat pages: %f seconds
FUZZY MATCH BEGIN, string:"%s", pattern:"%s", word_match:%s, word_start:%s, threshold:%d
%s match!
no match!
string: %s 
prefix matched against pattern 
normally matched against pattern 
FUZZY MATCH END: %s
%s:%d: Unexpected index base id order, recycling
%s:%d: Overlapping doc ids (%lld>%lld) between indexes %d and %d out of %d. Recycling
new loc field(%d): %@
ctx:%p idx:%p
ctx:%p %s
repair%s oid: %lld orphan parent: %lld
%s:%d: error: %d oid: %lld parent: %lld
repair oid: %lld parent oid: %lld skipped
repair oid: %lld skipped
%s:%d: db_cas_obj error: %d oid: %lld
%s:%d: SIPersistentIDStoreGetOidPathForOid error:%d at:%d oid:%lld parent:%lld
%s:%d: si_directoryStoreEnsurePath error:%d at:%d oid:%lld parent:%lld
%s:%d: db_add_field(_kMDItemGroupId) failed, oid:0x%llx(%lld), rc:%d
%s:%d: computePath error: %d oid: %lld parent: %lld
limit:%llu, used:%lld - using live index
DocID<->OID mapping out of sync. Had to do a brute force search. (Expected docId:%lld. Actual docID: %lld. OID: %llu
DocID<->OID mapping out of sync. Orphaned oid. (DocId:%lld. OID: %llu
sdb only update docId: %lld oid: %llu
%s:%d: ContentIndexUpdateContent failed (%d)
%s:%d: ContentIndexUpdateContent failed  (%d)
Update date to %d for oid %llx docId %llx
bad data in dbo (%lld) reimporting flags:%d
%s:%d: si_writeBackDBO failed, oid:0x%llx(%lld), rc:%d
lookupPathByDBO enter oid: %lld  parent: %lld
lookupPathByDBO oid: %lld p1: %lld p2: %lld
lookupPathByDBO error: %d oid: %lld  parent: %lld
%s:%d: SIPersistentIDStoreGetParentForOid error:%d at:%d oid:%lld parent:%lld
*warn* forceToOrphanParent oid: %lld dbo parent oid: %lld parent: %lld original: %d
Skipped swapindex
Skipped compacting
Starting flush
Failed index flush
Completed index flush
Merge slow at %f
force split; index size %lld
force split used:%lld, count:%d, deletes: %ld
force split; disk free %lld less than index size %lld
Flush took %f seconds, split state %d
Finished flush
Skipped flush
Skipped flushing
Transfer from %lld to %lld
Origin exists for %lld to %lld
Target does not exists for %lld to %lld
Reassigning %lld to %lld
Target exists for %lld to %lld
Target needs reimport for %lld to %lld
Target good for %lld to %lld
Store failed during attribute transfer on safe-save (%d)
No origin dbo exists for %lld to %lld
No target dbo exists for %lld to %lld
Target dbo exists for %lld to %lld
New last used date was not in the previous array: %f != %f
Failed adding used date
Failed adding ranking used date
New last used date was not in the previous array: %f
Failed adding used dates
Failed adding use count
skipping CFNull
no type conversion for %@
Waited %f seconds on scheduler
Running query at priority %d
Current QOS %d
*warn* Invalid query paramater version %d
%p: Query completely done
initWithQuery %@ for fields:%@ and scopes:%@
*warn* queryFromCFString faled for %@
*warn* grouping queryFromCFString faled for %@
*warn* Error creating ranking query for %@
*warn* rulecount %ld bitCount: %ld error
*warn* lifting queryFromCFString faled for %@
*warn* dboFilter queryFromCFString faled for %@
*warn* no fields for CoalescingCollectingQuery
*warn* No primary query
Query is limited to one group; turn off server side grouping
*warn* no or false _completeQuery
Failed creating query
Couldn't make query string
Sanitized to %@
Adding filter %@ to query %@
%s:%d: Failed adding filter: %@
Passing up change to not match
_queryGoCracked
Started normal (grouping) query threadlet.
Started normal query threadlet.
Could not execute query for %p (!)
Segmenting %s %s
check (%s) %@ old:%@ new:%@
%s:%d: scopePath: %@ / %s %f
siquery_addactivetime %g seconds to %g seconds 0x%x
Factor.
Metadata or Content.
Range query.
Range query: %@ < %s < %@
Found *=* query
Matched factor %s = %@ in %f seconds
No string for %s = %s
And.
*warn* Too few arguments for query: %d
%d: %s
Found custom proximity query
This node is always false.
This node is always true.
%s:%d: error (%d) opening %s
%s:%d: fstat error (%d)
%s:%d: ftruncate error (%d)
%s:%d: read error (%d)
canceled
%s:%d: write error (%d)
%s:%d: write failed
%s:%d: write failed - expected:%lld, actual: %lld
copy file %s to %s
%s:%d: copy file error(%d) (%s) (%s)
%s:%d: copy file error(%d) resolving dest fd %d for %s
%s:%d: copy file error(%d) resolving source fd %d for %s
%s:%d: sync err: %d
%s:%d: Tried to create index when index already existed %s
%s:%d: flock err: %d,  %s
*warn* flock err: %d,  %s
*warn* Couldn't statfs the CIMetaInfo. errno:%d
*warn* Failed to acquire lock on SMB CIMetaInfo; it might already be open by another machine's mds_stores.
*warn* Trying to acquire lock on CIMetaInfo again fd:%d
*warn* Failed to acquire lock on CIMetaInfo object: errno=%d
%s:%d: invalid generation file, resetting %s
*warn* Failed writing remapping data. Will cause index corruption if remapping fails.
%s(%d) [%d] %s
%s(%d) %s%s
%s(%d) %s
%s%s%s%s
%s:%d: EFAULT: Retry operation
%s fd:%d, error:%d
Couldn't make psid store at %s
%s:%d: getattrlist returned parent id %lld for %lld (%s)
%s:%d: stat succeeded, getattrlist returned error  %d for %lld (%s = %s)
%s:%d: Error (%d) for 0x%llx
DocID: %d
Next: %d
Delta: %d
Start pos: %d
range: %llu %u (%p)
rMin:%lld, uMax:%lld, 
rMax:%lld, uMin:%lld, 
Add %lld to change holder %p
Squashed update %u (%llu) from %p
*warn* Failed to map postings: %d
*warn* Failed to allocated guard page error %d
Split point: %llx
Rogue nil position at docID %d
%d rogue nil positions were found (use debug log level to see them all)
%s:%d: %llx==%llx
Expected start: %d Actual start: %d
Splitting at: %llx
Pushback; def we are done with a generation %p %p %llu
Pushback; pq we are done with a generation %p %p %llu
Out of postings, but we have more positions data(1); readers out of synch.
Out of postings, but we have more positions data(2); readers out of synch.
Out of postings, but we have more positions data (3); readers out of synch.
Pullback! %ld + %d
Un-split
Iterator for %p might be done with generation %d. Read docs: %x
Out of postings while we still have more position data (%u)
Iterator for %p done with generation %d
%p %llu %p
la_transition_context_for_state, current_state: %llu %d: transition_ctx.default_state: %llu %d
### %supdateset processing - %d ###
LL: Read %u
LL: Skip %u
MA: Read %u
MA: Skip %u
DU: Skip %u
DU: Read %u
Compacting %s payloads 
%s:%d: open err: %d
Reorder time (%f)
Merge (offset: %llx, length: %llx) (offset: %llx, length: %llx)
Merge (offset: %llx, length: %llx)
Merge time (%f)
Compact iteration:%d, src cnt:%d, new cnt:%d, time (%f) ERROR: %d
Compact iteration:%d, src cnt:%d, new cnt:%d, time (%f)
%s:%d: expected term id 1%d
Compacted payloads (%f)
%s:%d: Error %d compacting, retrying
%s:%d: openat err:%d %d %s
%s:%d: open err:%d %s
%s:%d: ftruncate err:%d size:%d
%s:%d: mmap err:%d size:%d
%s:%d: open err:%d
%s:%d: pwrite err:%d
%s:%d: Invalid string_id:%lu, map_count:%lu, index:%d, dst->name:%s, dst->flags:0x%x
%s:%d: marking invalid %s flags:0x%x
%s:%d: Failed to fetch the field for index:%d, nameId:%lu, dst->flags:0x%lx, dst->name:%s
%s:%d: Failed to fetch the field for index:%d, nameId:%lu, type:%d, flags:0x%lx(0x%lx), dst->flags:0x%lx, dst->name:%s
Got the wrong field for index:%d, nameId:%lu, type:%d, flags:0x%lx(0x%lx), dst->flags:0x%lx, dst->name:%s
DB Raw
*warn* could not resolve unique dbf value for field %d
DB_FIELD_LOCALIZED_STR: %@. 
DB_VAL: %@. 
db_utf8str: %@. 
Scalar array: %@. 
String: %@
C String: %s
%s:%d: Lock failed with error %d
%s:%d: %s : read_query: page at offset 0x%llx not valid (skipping %d)! (0x%x %d %d 0x%x)
%s:%d: %s:%d : %s : %s
%s:%d: db2_page_uncompress_swap: invalid page size, flags:0x%lx, used_bytes:%lu, size:%ld
%s:%d: db2_page_uncompress_swap: invalid page, flags:0x%lx, used_bytes:%lu, uncompressed_used_bytes:%ld
%s:%d: db2_page_uncompress_swap: invalid page, flags:0x%lx, uncompressed_size:%lu, uncompressed_used_bytes:%lu, compression_size_estimate:%lu
%s:%d: db2_page_uncompress_swap: invalid page, flags:0x%lx, uncompressed_size:%lu, uncompressed_used_bytes:%lu
%s:%d: db2_page_uncompress_swap: uncompress(%s) failed, status:%d, flags:0x%lx, src_size:%lu, out_size:%lu, uncompressed_used_bytes:%lu
%s:%d: db2_page_uncompress_swap: uncompressed size mismatch (%lu/%lu)
%s:%d: db2_create_datastore: ERR: Can't create file (%s : %s)
%s:%d: %s : ERR: can't init the map! (%s)
%s:%d: %s : ERR: can't init the string table! (%s)
%s:%d: %s : ERR: Can't write DST header (%s)
%s:%d: %s : ERR: Can't write shadow DST header (%s)
%s:%d: %s : db2_sync_datastore: !WARNING! prior write-errors invalidate sync.
Push dirty string page %d to disk (%d)
%s:%d: %s : db2_sync_datastore: !WARNING! write-errors flushing cache/writing map.
%s:%d: %s : db2_sync_datastore: ERR: Can't write DST header (%s)
%s:%d: %s : ERR: Can't write DST header (2: %s)
%s:%d: Failed syncing db
%s:%d: ERR: page compression error %d with page %d used_bytes %d disk page size %d
%s:%d: Failed splitting compressed page %d
%s:%d: should not need to split a compressed cache
%s:%d: Failed page_alloc for %d
%s:%d: Failed page_fetch for %d
Nothing found on page %d used_bytes %d
%s:%d: ERR: page_resize error %d with page %d used_bytes %d new page size %d
Forced to split page %d used_bytes %d into pieces of size %d
%s:%d: Page compress failed with error %d at %d/%d
Failure to split page %d used_bytes %d into pieces of size %d
splitting map page %x, new page %x max_oid %llx type %d
%s : ERR: map_split_page: BAD NEWS! pgnum 0x%x not found
%s:%d: Failed releasing dity compressed cache page %d with error %d
%s:%d: ERR: db_split_page error %d with page %d used_bytes %d disk page size %d
%s:%d: Failed compressing/splitting page %d error %d
%s:%d: Failed compressing/splitting page %d
%s:%d: Failed releasing null page
%s:%d: Failed writing page
%s:%d: %s : map_check_size: ERR FATAL: too many entries! %d / %ld
Moving string page from %d to %d
Change dirty string page %d to %d (%d)
Change first string page %d to %d (%d)
Push chain string page %d to disk (%d)
%s:%d: Failed reading pgnum %d
%s:%d: db2_page_uncompress_swap failed, error:%d, pgnum:%lu, pgoff:0x%llx, signature:0x%x, size:%d, used_bytes:%d, flags:0x%x, name:%s
%s:%d: page_fetch found an invalid page, pgnum:%lu, pgoff:0x%llx, signature:0x%x, size:%d, used_bytes:%d, flags:0x%x, name:%s
%s:%d: page_fetch marking the dst as corrupted, pgnum:%lu, pgoff:0x%llx, flags:0x%x, name:%s
page_compress: ERR: page is already compressed!
Zip Compression ratio: %f good:%lld bad:%lld
Zip Failed compressing %ld bytes
Compression ratio: %f good:%lld bad:%lld
Failed compressing %d bytes
%s:%d: Bad sdb in db_updateset_iterate (delete) at page %d
%s:%d: Failed delete, oid:0x%llx(%lld), type:%ld, pgnum:%ld, rc:%d
%s:%d: page_delete_obj: ERR: tried to read attr name table data! pgnum 0x%x
%s:%d: page_delete_obj: ERR: page is still compressed! pgnum 0x%x
%s : ERR: map_delete: trying to delete non-existent oid %.16llx (%.16llx), (%d, %d)
%s:%d: %s : ERR: map_update: page offset doesn't match! 0x%x != 0x%x
%s : ERR: map_update: did not find old oid %.16llx (%.16llx)
%s:%d: Bad sdb in db_updateset_iterate at page %d
%s:%d: Failed page allocation, oid:0x%llx(%lld), type:%ld, pgnum:%ld, rc:%d
%s:%d: Failed insert, oid:0x%llx(%lld), type:%ld, pgnum:%ld, rc:%d
%s:%d: Failed update, oid:0x%llx(%lld), type:%ld, pgnum:%ld, rc:%d
%s:%d: Failed %s, oid:0x%llx(%lld), type:%ld, pgnum:%ld, rc:%d
Failed delete after failed add/update, oid:0x%llx(%lld), type:%ld, pgnum:%ld, pageRc:%d updateSetRc:%d
%s:%d: page_insert_obj: ERR: tried to read attr name table data! pgnum 0x%x
%s:%d: page_insert_obj: ERR: page is still compressed! pgnum 0x%x
%s : ERR: map_insert: key already present! idx %d %.16llx
%s:%d: %s: page_insert_obj, page pgnum:%ld has a bad object at offset:%p
%s:%d: page_split: ERR: tried to read attr name table data! pgnum 0x%x
dbo %p beyond end of page at %p. Resetting to last at %p
%s : ERR: try_push_insert_obj: BAD NEWS! pgnum 0x%x not found
try_push_left: issshhhn't dat strange? decrease %d num_bytes %d
try_push_right: weird! end %p first %p but num_bytes %d
test_compress_obj: ERR %d: compressing %d dbo with oid 0x%llx (%d bytes)
%s:%d: page_find_oid: ERR: tried to read attr name table data! pgnum 0x%x
%s:%d: page_find_oid: ERR: page is still compressed! pgnum 0x%x
%s:%d: page_update_obj: ERR: tried to read attr name table data! pgnum 0x%x
%s:%d: page_update_obj: ERR: page is still compressed! pgnum 0x%x
%s : page_update_obj: ERR: page at num 0x%x has a bad object pgnum %p
%s:%d: sync_dirty_chunks: ERR: Can't determine the shadow file size! (%s)
sync_dirty_chunks: ERR: Failed to map shadow
sync_dirty_chunks: ERR: Failed to map master
sync_dirty_chunks: ERR: Failed to truncate master fd to %lld
%s:%d: sync_dirty_chunks: ERR:%d count:%d expected %d! (%s)
%s:%d: sync_dirty_chunks: ERR: Can't determine the master file size! (%s)
%s:%d: Failed writing map
%s:%d: %s : free: ERR: Danger! page num 0x%x looks bad (signature 0x%x expected 0x%x flags 0x%x)
%s:%d: ftruncate(%s, %lld) error: %d
%s:%d: page_free: ERR: error moving page from %lld to %d (%s)
Skipping string page move from %d to %d -- no string table!
Object page move from %d to %d
%s : ERR: map_change_pgnum: BAD NEWS! pgnum 0x%x not found
%s:%d: page_free: ERR: failed to change the map page offset from %lld to %d
page_free: ERR: tried to free the first & only page of the file (pgnum 0x%x).
%s:%d: page_release: ERROR: page_fetch caller responsible for making sure compressed page fits after changes.
%s:%d: page_release: page %d used_bytes %d disk page size %d
%s:%d: page_release: ERR: compress error %d with page %d used_bytes %d disk page size %d
%s:%d: Failed writing pgnum %d
*warn* sdb not page-size aligned. Extending.
%s:%d: Failed allocating page
%s:%d: pwrite(%s, %d, %lld) error: %d
%s:%d: %s: ERR: Can't add dirty chunks to a read-only db!
%s:%d: dirty callback returned non-zero
%s:%d: %s : db2_dirty_datastore: ERR: Can't write DST header (%s)
%s:%d: update state (%d) failed err:%d
%s:%d: update_db_header failed err:%d at %d for %s
%s:%d: db2_check_datastore: ERR: could not get parent fdp
%s:%d: db2_check_datastore: ERR: could not open parent
%s:%d: db2_check_datastore: ERR:%d could not get shadow fdp
%s:%d: db2_check_datastore: ERR:%d could not open shadow
%s:%d: %s : db2_check_datastore: ERR: could not read %d bytes
%s:%d: %s : db2_check_datastore: ERR: signature 0x%x != 0x%x. bailing out.
%s:%d: %s : db2_check_datastore: ERR: DST_BUSY
%s : db2_check_datastore:%d (s_flags:%x m_flags:%x)
%s:%d: %s : db2_get_datastore: ERR: could not read %d bytes
%s:%d: %s : db2_get_datastore: ERR: signature 0x%x != 0x%x. bailing out.
%s:%d: %s : db2_get_datastore: ERR: bad page size %d bailing out.
%s:%d: bad shadow. recover.
%s : db2_get_datastore opening - clean needs shadow
%s:%d: %s : db2_get_datastore: ERR: requested recovery from dirty master.
%s : db2_get_datastore open - shadow to master
%s:%d: %s : db2_get_datastore copyFile or copy_datastore err:%d
%s:%d: %s : db2_get_datastore: ERR: both shadow and master are dirty!  no recovery possible.
%s:%d: Invalid header ms: %u hs: %u fs: %lld
%s:%d: %s : load_map, invalid entry at %ld, oid:0x%.16llx/0x%.16llx, type:0x%lx/0x%lx, pgnum:%ld/%ld
%s:%d: load_string_table: circular string table (pgnum %d)
sdb: load_string_table: ERR: failed to load page @ 0x%x
%s:%d: load_string_table: unexpected page flags (%x %x)
%s:%d: load_string_table: read past bound: dstr (%tx) str_index (%d)
%s:%d: load_string_table: hash_insert: string %s (id %d) already exists (id %p)!
Loaded %d strings for %d
%s:%d: load_string_table: string id mismatch: dstr (%tx) str_id (%d) str_index (%d)
%s:%d: Invalid string table version (%d %d)
%s:%d: Upgrading string flag data
%s:%d: sdb: ERR: copy_datastore: source is not a valid fd
%s:%d: sdb: ERR: copy_datastore: destination is not a valid fd
sdb: validate_datastore: ERR: invalid datastore header size
sdb: validate_datastore: ERR: invalid datastore signature: 0x%x
sdb: validate_datastore: ERR: invalid page @ 0x%llx (sig: 0x%x)
sdb: validate_datastore: ERR: failed to read %d bytes at offset 0x%llx
%s:%d: sdb: ERR: invalid master datastore! (%s)
%s:%d: %s : ERR: 2: failed to write %d bytes at %lld to to_fdp
%s:%d: %s : copy_datastore: ERR: failed to update the header! (%s)
%s:%d: %s : copy_datastore: ERR: failed to update the header 2! (%s)
%s:%d: sdb: copy_datastore: ERR: %d %s: error restoring from master datastore.
%s:%d: %s : copy_datastore:2: ERR: failed to update the header! (%s)
Flush starting at %f
%s:%d: %s : db2_sync_datastore: !WARNING! write-errors flushing cache. %d
%s:%d: %s : db2_sync_datastore: !WARNING! write-errors writing map. %d
Push dirty string page %d to disk (%d). %d strings
No dirty string page for %d. %d strings
Flush ending at %f
Defragging index...
%s:%d: copyFile: ERR:%d (%s)
%s:%d: sync_dirty_chunks: ERR:%d (%s)
%s:%d: %s : ERR: Can't write DST master header (2: %d)
%s:%d: %s : ERR: Can't write DST header (2: %d)
Read page:%d sig:0x%4.4x sz:0x%4.4x used:0x%4.4x flags:0x%4.4x
%s:%d: invalid entry oid: 0x%llx type:%d, map[%d] oid:0x%llx type:%d, map[%d] oid:0x%llx type:%d
Grow string table %p (%d)
Inserted field name %s with id %d for %d
Push empty string page %d to disk (%d)
Push old dirty string page %p %d to disk (%d)
New string table page %d (%d)
%s:%d: Error %d, oid %llx
%s:%d: Previous write error
%s:%d: Error %d from db_updateset_insert_object
%s:%d: Error %d from flush_updateset_locked
%s:%d: dbo too small
%s:%d: Delete failed
Prefetch pages from %ld
1) Skipping oid %lld at index %ld
2) Skipping oid %lld at index %ld
Already found %lld
Wrong next page in db iterator 0x%x %p
%s:%d: obj_iter_fetch_page: ERR: page came back compressed! pgnum 0x%x
Prefetch page %d(%d) for db %p
(nextpage)Skip prefetch of %ld cache:%d
Failed to find %lld (%d) (offset:%lu first:%llu)
Expected %ld, found %ld
Missing item: %lld page:%d type %d. Lookup %ld of %ld. startIndex %ld of %ld
%s:%d: Field type %d out of bounds
%s:%d: Field name %s out of bounds
%s:%d: types don't match dbf_flags:%x dfb_type:%d flags:%x type:%d
original field not an array, dbf_flags:%x dfb_type:%d flags:%x type:%d
db_add_field: ERR: dbf is not valid! (dbf %p dbo %p size 0x%x)
%s : ERR: XXXdbg - whoa dude... can't get name ptr for name id %d
%s:%d: invalid sdb page in cache %d
convert_value_to_type: unknown data type 0x%x (val 0x%p)
%s:%d: nil str_val converted_bits: 0x%x field: %s string: %s
%s : node @ 0x%p looks trashed
%s : failed to convert qp @ 0x%p to type 0x%x
NULL
-------------------------------------------------------------------------------------------
Growing hash table %d %d
Cache remove %p %d
%s:%d: Large page cache fetch fail for pgnum:%ld, ret:%d
Cache add %d %p %d %p %d %d %d
Forcing Cache Cleanup
%s:%d: Unexpected EOF in page cache preload; got %ld bytes at offset %lld
%s:%d: Missing signature in page cache preload %llx
Pre-loaded %ld cache pages
Scopes: %@
Entering
File without dbo
File with dbo: ignored for search during initial indexing
File failed primary query
File failed secondary query
QOS enqueueCmd: qos: 0x%x
QOS executeSearchCtx_Start: %d
Search canceled while waiting on scheduler
Search waited %f seconds on the scheduler at qos 0x%x
Search was active (preIterate) for %f seconds on the scheduler at qos 0x%x
Search was active for %f seconds on the scheduler at qos 0x%x
QOS executeSearchContextCracked_2: %d
Search was active (performSearch) for %f seconds on the scheduler at qos 0x%x
Got %ld results
Coalesced result set: %p
Result queue overflowed. Not rescheduling
Query canceled
QOS executeSearchCtx2: %d
Low water routine triggered
Encoding %ld items from set %p for field %s
Appended datum: %@
Setting locale to %s
PQR had nothing!
Passing up %ld results for mode %d (removed %ld duplicates)
QOS executeSearchCtx: %d
Stalling query because the task has data in the set queue
Suspending root query scheduler(%d)
%s:%d: _data_map_rdlock error %d
%s:%d: _data_map_unlock error %d
*warn* Got exception on %s addr:%p start:%p map end:%p file end:%d sres:%d dev:%d ino:%lld
%s:%d: param error
%s:%d: offsets fd_mmap error
%s:%d: fd_truncate error
%s:%d: hash fd_mmap error
%s:%d: invalid header size %ld
%s:%d: header fd_mmap error
%s:%d: storage fd_mmap error
%s:%d: offset fd_mmap error
%s:%d: invalid offset size 1 - %ld %d
%s:%d: invalid offset signature
%s:%d: invalid version %d
%s:%d: invalid extra_size %d %d
%s:%d: invalid header size
%s:%d: invalid storage size 1
%s:%d: re-build hash error
%s:%d: invalid hash size 1
%s:%d: exception processing %s
rehash %p max id: %d deletes: %d count: %d hash_size: %d
%s:%d: invalid data offset 0x%lx 0x%lx %p %s
%s:%d: re-build hash error %p
%s:%d: invalid data id %d max %d %p %s
%s:%d: _data_map_wrlock error %d
added %d to %p
*warn* Delete strings canceled
rename %p %s to %s
Deleted %d items from %p[%d]
Delete data for id:%d size:%d %s from %p
Delete data for id:%d size:%d from %p
Found %d deleted strings from %p
Deleting id: %d size: %d %s from %p
Deleting id: %d size: %d from %p
Q(P'
333?
333?
333?
333?
333?
333?
333?
333?
333?
fff?
?333333
@@5^
